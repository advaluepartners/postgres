Here is the txt file that represents the folders and files in my github hub repo to build an ec2 and build and run a postrgesSQL db. The set id done by using github actions with nix, ansible and docker Each file in this repo separated by the sequence '''--- , followed by the file path, ending with ---. Each file's content begins immediately after its file path and extends until the next sequence of '''---

*Folder: /postgres/ansible*
ansible/
    manifest-playbook.yml
    vars.yml
    playbook.yml
tasks/
    setup-envoy.yml
    fix_ipv6_ndisc.yml
    setup-pgbouncer.yml
    setup-docker.yml
    setup-fail2ban.yml
    setup-kong.yml
    setup-extensions.yml
    setup-system.yml
    setup-vector.yml
    setup-postgrest.yml
    setup-migrations.yml
    setup-nginx.yml
    test-image.yml
    finalize-ami.yml
    setup-wal-g.yml
    stage2-setup-postgres.yml
    setup-gotrue.yml
    setup-supabase-internal.yml
    setup-postgres.yml
    clean-build-dependencies.yml
    internal/
        supautils.yml
        postgresql-prestart.yml
        optimizations.yml
        admin-api.yml
        install-salt.yml
        pg_egress_collect.yml
        admin-mgr.yml
        collect-pg-binaries.yml
        setup-ansible-pull.yml
        postgres-exporter.yml
        setup-nftables.yml
files/
    envoy.service
    postgres_prestart.sh.j2
    adminapi.service.j2
    supabase_facts.ini
    vector.service.j2
    stat_extension.sql
    journald.conf
    postgres_exporter.service.j2
    cron.deny
    default.sysstat
    ansible-pull.service
    ufw.service.conf
    gotrue-optimizations.service.j2
    pgsodium_getkey_readonly.sh.j2
    start-envoy.sh
    permission_check.py
    systemd-resolved.conf
    commence-backup.service.j2
    postgrest-optimizations.service.j2
    database-optimizations.service.j2
    logind.conf
    adminapi.sudoers.conf
    gotrue.service.j2
    manifest.json
    nginx.service.j2
    ansible-pull.timer
    services.slice.j2
    sysstat.sysstat
    postgrest.service.j2
    pgsodium_getkey_urandom.sh.j2
    pg_egress_collect.service.j2
    sodium_extension.sql
    apt_periodic
    fail2ban_config/
        fail2ban.service.conf
        jail.local
        jail-ssh.conf
        jail-postgresql.conf.j2
        jail-pgbouncer.conf.j2
        filter-postgresql.conf.j2
        filter-pgbouncer.conf.j2
    postgresql_extension_custom_scripts/
        before-create.sql
        postgres_fdw/
            after-create.sql
        pg_repack/
            after-create.sql
        pg_tle/
            after-create.sql
        pg_cron/
            after-create.sql
        dblink/
            after-create.sql
        pgsodium/
            after-create.sql
            before-create.sql
        pgmq/
            after-create.sql
        postgis_tiger_geocoder/
            after-create.sql
    postgresql_config/
        postgresql.service.j2
        tmpfiles.postgresql.conf
        supautils.conf.j2
        postgresql.conf.j2
        pg_hba.conf.j2
        custom_read_replica.conf.j2
        postgresql-csvlog.conf
        pg_ident.conf.j2
        postgresql-stdout-log.conf
        custom_walg.conf.j2
    pgbouncer_config/
        tmpfiles.d-pgbouncer.conf.j2
        pgbouncer_auth_schema.sql
        pgbouncer.service.j2
        pgbouncer.ini.j2
    admin_api_scripts/
        grow_fs.sh
        manage_readonly_mode.sh
        pg_egress_collect.pl
        pg_upgrade_scripts/
            initiate.sh
            common.sh
            complete.sh
            prepare.sh
            check.sh
            pgsodium_getkey.sh
    envoy_config/
        envoy.yaml
        cds.yaml
        lds.yaml
        lds.supabase.yaml
    systemd-networkd/
        systemd-networkd-check-and-fix.service
        systemd-networkd-check-and-fix.sh
        systemd-networkd-check-and-fix.timer
    walg_helper_scripts/
        wal_change_ownership.sh
        wal_fetch.sh
    kong_config/
        kong.conf.j2
        kong.env.j2
        kong.service.j2
    logrotate_config/
        logrotate-postgres-csv.conf
        logrotate-postgres-auth.conf
        logrotate-postgres.conf
        logrotate-walg.conf

*Folder: /postgres/ebssurrogate*
ebssurrogate/
    USAGE.md
scripts/
    chroot-bootstrap-nix.sh
    qemu-bootstrap.nex.sh
    surrogate-bootstrap-nix.sh
files/
    70-ec2-nvme-devices.rules
    sources.cfg
    vector.timer
    cloud.cfg
    sources-arm64.cfg
    ebsnvme-id
    unit-tests/
        unit-test-01.sql
    apparmor_profiles/
        opt.postgrest
        usr.bin.vector
        opt.gotrue.gotrue
        usr.local.bin.pgbouncer
        usr.lib.postgresql.bin.postgres

*Folder: /postgres/amazon-arm64-nix.pkr.hcl*

*Folder: /postgres/amazon-arm64-nix.pkr.hcl*

*Folder: /postgres/flake.nix*


File contents:

'''--- /postgres/ansible/manifest-playbook.yml ---
- hosts: localhost
  gather_facts: no

  vars_files:
    - ./vars.yml

  tasks:
    - name: Write out image manifest
      action: template src=files/manifest.json dest=./image-manifest-{{ ami_release_version }}.json

    - name: Upload image manifest
      shell: |
        aws s3 cp ./image-manifest-{{ ami_release_version }}.json s3://{{ internal_artifacts_bucket }}/manifests/postgres-{{ ami_release_version }}/software-manifest.json

    # upload software artifacts of interest
    # Generally - download, extract, repack as xz archive, upload
    # currently, we upload gotrue, adminapi, postgrest
    - name: gotrue - download commit archive
      get_url:
        url: "https://github.com/supabase/gotrue/releases/download/v{{ gotrue_release }}/auth-v{{ gotrue_release }}-arm64.tar.gz"
        dest: /tmp/auth-v{{ gotrue_release }}-arm64.tar.gz
        checksum: "{{ gotrue_release_checksum }}"
        timeout: 60

    - name: PostgREST - download ubuntu binary archive (arm)
      get_url:
        url: "https://github.com/PostgREST/postgrest/releases/download/v{{ postgrest_release }}/postgrest-v{{ postgrest_release }}-ubuntu-aarch64.tar.xz"
        dest: /tmp/postgrest-{{ postgrest_release }}-arm64.tar.xz
        checksum: "{{ postgrest_arm_release_checksum }}"
        timeout: 60

    - name: Download adminapi archive
      get_url:
        url: "https://supabase-public-artifacts-bucket.s3.amazonaws.com/supabase-admin-api/v{{ adminapi_release }}/supabase-admin-api_{{ adminapi_release }}_linux_arm64.tar.gz"
        dest: "/tmp/adminapi.tar.gz"
        timeout: 90

    - name: adminapi - unpack archive in /tmp
      unarchive:
        remote_src: yes
        src: /tmp/adminapi.tar.gz
        dest: /tmp

    - name: adminapi - pack archive
      shell: |
        cd /tmp && tar -cJf supabase-admin-api-{{ adminapi_release }}-arm64.tar.xz supabase-admin-api

    - name: Download admin-mgr archive
      get_url:
        url: "https://supabase-public-artifacts-bucket.s3.amazonaws.com/admin-mgr/v{{ adminmgr_release }}/admin-mgr_{{ adminmgr_release }}_linux_arm64.tar.gz"
        dest: "/tmp/admin-mgr.tar.gz"
        timeout: 90

    - name: admin-mgr - unpack archive in /tmp
      unarchive:
        remote_src: yes
        src: /tmp/admin-mgr.tar.gz
        dest: /tmp

    - name: admin-mgr - pack archive
      shell: |
        cd /tmp && tar -cJf admin-mgr-{{ adminmgr_release }}-arm64.tar.xz admin-mgr

    - name: upload archives
      shell: |
        aws s3 cp /tmp/{{ item.file }} s3://{{ internal_artifacts_bucket }}/upgrades/{{ item.service }}/{{ item.file }}
      with_items:
        - service: gotrue
          file: auth-v{{ gotrue_release }}-arm64.tar.gz
        - service: postgrest
          file: postgrest-{{ postgrest_release }}-arm64.tar.xz
        - service: supabase-admin-api
          file: supabase-admin-api-{{ adminapi_release }}-arm64.tar.xz
        - service: admin-mgr
          file: admin-mgr-{{ adminmgr_release }}-arm64.tar.xz

'''
'''--- /postgres/ansible/vars.yml ---
supabase_internal: true
ebssurrogate_mode: true
async_mode: true

postgres_major:
  - "15"
  - "orioledb-17"

# Full version strings for each major version
postgres_release:
  postgresorioledb-17: "17.0.1.31-orioledb"
  postgres15: "15.8.1.035"

# Non Postgres Extensions
pgbouncer_release: "1.19.0"
pgbouncer_release_checksum: sha256:af0b05e97d0e1fd9ad45fe00ea6d2a934c63075f67f7e2ccef2ca59e3d8ce682

# to get these use
# wget https://github.com/PostgREST/postgrest/releases/download/v12.2.3/postgrest-v12.2.3-ubuntu-aarch64.tar.xz -q -O- | sha1sum
# wget https://github.com/PostgREST/postgrest/releases/download/v12.2.3/postgrest-v12.2.3-linux-static-x64.tar.xz -q -O- | sha1sum
postgrest_release: "12.2.3"
postgrest_arm_release_checksum: sha1:fbfd6613d711ce1afa25c42d5df8f1b017f396f9
postgrest_x86_release_checksum: sha1:61c513f91a8931be4062587b9d4a18b42acf5c05

gotrue_release: 2.169.0
gotrue_release_checksum: sha1:1419b94683aac7ddc30355408b8e8b79e61146c4

aws_cli_release: "2.2.7"

salt_minion_version: 3007

golang_version: "1.19.3"
golang_version_checksum:
  arm64: sha256:99de2fe112a52ab748fb175edea64b313a0c8d51d6157dba683a6be163fd5eab
  amd64: sha256:74b9640724fd4e6bb0ed2a1bc44ae813a03f1e72a4c76253e2d5c015494430ba

envoy_release: 1.28.0
envoy_release_checksum: sha1:b0a06e9cfb170f1993f369beaa5aa9d7ec679ce5
envoy_hot_restarter_release_checksum: sha1:6d43b89d266fb2427a4b51756b649883b0617eda

kong_release_target: focal # if it works, it works
kong_deb: kong_2.8.1_arm64.deb
kong_deb_checksum: sha1:2086f6ccf8454fe64435252fea4d29d736d7ec61

nginx_release: 1.22.0
nginx_release_checksum: sha1:419efb77b80f165666e2ee406ad8ae9b845aba93

wal_g_release: "2.0.1"

postgres_exporter_release: "0.15.0"
postgres_exporter_release_checksum:
  arm64: sha256:29ba62d538b92d39952afe12ee2e1f4401250d678ff4b354ff2752f4321c87a0
  amd64: sha256:cb89fc5bf4485fb554e0d640d9684fae143a4b2d5fa443009bd29c59f9129e84

adminapi_release: 0.74.0
adminmgr_release: 0.24.1

vector_x86_deb: "https://packages.timber.io/vector/0.22.3/vector_0.22.3-1_amd64.deb"
vector_arm_deb: "https://packages.timber.io/vector/0.22.3/vector_0.22.3-1_arm64.deb"

supautils_release: "2.6.0"
supautils_release_checksum: "sha256:b1cf964d1c56f45120d4724bfaf258cc7c0caccb30d8bde20bcda088a5990718"

'''
'''--- /postgres/ansible/playbook.yml ---
- hosts: all
  become: yes
  gather_facts: yes  # Add this line

  pre_tasks:
    - import_tasks: tasks/setup-system.yml
  vars_files:
    - ./vars.yml

  vars:
    sql_files:
      - {
          source: "pgbouncer_config/pgbouncer_auth_schema.sql",
          dest: "00-schema.sql",
        }
      - { source: "stat_extension.sql", dest: "01-extension.sql" }
    
  environment:
    PATH: /usr/lib/postgresql/bin:{{ ansible_env.PATH }}
    
  tasks:
    - set_fact:
        supabase_internal: true
      tags:
        - install-supabase-internal

    - set_fact:
        parallel_jobs: 16
    - name: Set system state for user management
      block:
        - name: Ensure nscd is installed (if using glibc)
          apt:
            name: nscd
            state: present
          when: ansible_os_family == "Debian"
          ignore_errors: yes

        - name: Clear system user/group cache
          shell: |
            if command -v nscd >/dev/null 2>&1; then
              nscd -i group
              nscd -i passwd
            fi
            systemctl daemon-reload
          ignore_errors: yes

    - name: Install Postgres from source
      import_tasks: tasks/setup-postgres.yml

    - name: Install PgBouncer
      import_tasks: tasks/setup-pgbouncer.yml
      tags:
        - install-pgbouncer
        - install-supabase-internal
      when:  debpkg_mode or nixpkg_mode or stage2_nix

    - name: Install WAL-G
      import_tasks: tasks/setup-wal-g.yml
      when: debpkg_mode or nixpkg_mode or stage2_nix

    - name: Install Gotrue
      import_tasks: tasks/setup-gotrue.yml
      tags:
        - install-gotrue
        - install-supabase-internal
      when: debpkg_mode or nixpkg_mode or stage2_nix  # Add stage2_nix
      
    - name: Install PostgREST
      import_tasks: tasks/setup-postgrest.yml
      vars:
        postgresql_major: "{{ postgresql_major_version }}"
      tags:
        - install-postgrest
        - install-supabase-internal
      when: debpkg_mode or nixpkg_mode or stage2_nix

    - name: Install Envoy
      import_tasks: tasks/setup-envoy.yml
      tags:
        - install-supabase-internal
      when: debpkg_mode or nixpkg_mode or stage2_nix  # Add stage2_nix

    - name: Install Kong
      import_tasks: tasks/setup-kong.yml
      tags:
        - install-supabase-internal
      when: debpkg_mode or nixpkg_mode or stage2_nix

    - name: Install nginx
      import_tasks: tasks/setup-nginx.yml
      tags:
        - install-supabase-internal
      when: debpkg_mode or nixpkg_mode or stage2_nix

    - name: Install Vector
      import_tasks: tasks/setup-vector.yml
      tags:
        - install-supabase-internal
      when: debpkg_mode or nixpkg_mode or stage2_nix

    - name: Install Supabase specific content
      import_tasks: tasks/setup-supabase-internal.yml
      tags:
        - install-supabase-internal
      when: debpkg_mode or nixpkg_mode

    - name: Fix IPv6 NDisc issues
      import_tasks: tasks/fix_ipv6_ndisc.yml
      tags:
        - install-supabase-internal
      when: debpkg_mode or nixpkg_mode

    - name: Start Postgres Database without Systemd
      become: yes
      become_user: postgres
      shell:
        cmd: /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data start
      when: debpkg_mode

    - name: Adjust APT update intervals
      copy:
        src: files/apt_periodic
        dest: /etc/apt/apt.conf.d/10periodic
      when: debpkg_mode or nixpkg_mode
      
    - name: Transfer init SQL files
      copy:
        src: files/{{ item.source }}
        dest: /tmp/{{ item.dest }}
      loop: "{{ sql_files }}"
      when: debpkg_mode or stage2_nix

    - name: Create postgres role
      become: yes
      become_user: postgres
      shell:
        cmd: /usr/lib/postgresql/bin/psql --username=supabase_admin -d postgres -c "create role postgres superuser login; alter database postgres owner to postgres;"
      when: debpkg_mode or stage2_nix

    - name: Execute init SQL files
      become: yes
      become_user: postgres
      shell:
        cmd: /usr/lib/postgresql/bin/psql -f /tmp/{{ item.dest }}
      loop: "{{ sql_files }}"
      when: debpkg_mode or stage2_nix

    - name: Delete SQL scripts
      file:
        path: /tmp/{{ item.dest }}
        state: absent
      loop: "{{ sql_files }}"
      when: debpkg_mode or stage2_nix

    - name: First boot optimizations
      import_tasks: tasks/internal/optimizations.yml
      tags:
        - install-supabase-internal
      when: debpkg_mode or stage2_nix
      
    - name: Finalize AMI
      import_tasks: tasks/finalize-ami.yml
      tags:
        - install-supabase-internal
      when: debpkg_mode or nixpkg_mode
      
    - name: Enhance fail2ban
      import_tasks: tasks/setup-fail2ban.yml
      when: debpkg_mode or nixpkg_mode

    - name: Install Admin API
      import_tasks: tasks/internal/admin-api.yml
      tags:
        - install-supabase-internal
      when: debpkg_mode or nixpkg_mode or stage2_nix

    - name: Internal tasks setup
      block:
        - name: Install supautils
          import_tasks: tasks/internal/supautils.yml
        - name: Setup postgresql-prestart
          import_tasks: tasks/internal/postgresql-prestart.yml
        # - name: Setup optimizations
        #   import_tasks: tasks/internal/optimizations.yml
        # - name: Setup admin-api
        #   import_tasks: tasks/internal/admin-api.yml
        - name: Install salt
          import_tasks: tasks/internal/install-salt.yml
        - name: Setup pg_egress_collect
          import_tasks: tasks/internal/pg_egress_collect.yml
        - name: Setup admin-mgr
          import_tasks: tasks/internal/admin-mgr.yml
        - name: Setup postgres-exporter
          import_tasks: tasks/internal/postgres-exporter.yml
        - name: Setup nftables
          import_tasks: tasks/internal/setup-nftables.yml
      when: debpkg_mode or nixpkg_mode or stage2_nix
      tags:
        - install-supabase-internal

    # Install EC2 instance connect
    # Only for AWS images
    - name: install EC2 instance connect
      become: yes
      apt:
        pkg:
          - ec2-instance-connect
      tags:
        - aws-only

    # Install this at the end to prevent it from kicking in during the apt process, causing conflicts
    - name: Install security tools
      become: yes
      apt:
        pkg:
          - unattended-upgrades
        update_cache: yes
        cache_valid_time: 3600

    - name: Clean out build dependencies
      import_tasks: tasks/clean-build-dependencies.yml

    - name: Ensure /run/postgresql exists for lock file creation
      become: yes
      file:
        path: /run/postgresql
        state: directory
        owner: postgres
        group: postgres
        mode: '2775'

      when: stage2_nix

    - name: Check if PostgreSQL is running
      become: yes
      become_user: postgres
      shell: /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data status
      args:
        executable: /bin/bash
      register: pg_status
      ignore_errors: yes
      when: stage2_nix

    - name: Force kill PostgreSQL process if running and remove stale PID file
      become: yes
      become_user: postgres
      shell: |
        if [ -f /var/lib/postgresql/data/postmaster.pid ]; then
          PID=$(head -n 1 /var/lib/postgresql/data/postmaster.pid)
          if ps -p $PID > /dev/null 2>&1; then
            echo "PostgreSQL process $PID is still running. Force killing..."
            kill -9 $PID
            # Give the OS a moment to reap the process
            sleep 2
          fi
          echo "Removing stale PID file"
          rm -f /var/lib/postgresql/data/postmaster.pid
        fi
      args:
        executable: /bin/bash
      when: stage2_nix

    - name: Ensure PostgreSQL is not running (double-check)
      become: yes
      become_user: postgres
      shell: /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data status
      args:
        executable: /bin/bash
      register: pg_status_after
      ignore_errors: yes
      when: stage2_nix

    - name: Fail if PostgreSQL is still running
      fail:
        msg: "PostgreSQL is still running after force kill; cannot start a new instance."
      when: stage2_nix and (pg_status_after.rc == 0)

    - name: Restart PostgreSQL without Systemd
      become: yes
      become_user: postgres
      ansible.builtin.shell: |
        # Export environment variables inline
        export LANG=en_US.UTF-8
        export LANGUAGE=en_US:en
        export LC_ALL=en_US.UTF-8
        export LC_CTYPE=en_US.UTF-8
        export LOCALE_ARCHIVE=/usr/lib/locale/locale-archive
        # Use the POSIX "." operator instead of "source"
        . /var/lib/postgresql/.bashrc
        /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data start
      args:
        executable: /bin/bash
      when: stage2_nix

    - name: Run migrations
      import_tasks: tasks/setup-migrations.yml
      tags:
        - migrations
      when: debpkg_mode or stage2_nix

    # - name: Stop Postgres Database without Systemd
    #   become: yes
    #   become_user: postgres
    #   shell:
    #     cmd: /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data stop
    #   when: debpkg_mode    

    - name: Check if PostgreSQL PID file exists
      stat:
        path: /var/lib/postgresql/data/postmaster.pid
      register: pg_pid_file
      when: stage2_nix

    - name: Stop Postgres Database without Systemd (force shutdown)
      become: yes
      become_user: postgres
      shell: /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data stop -m immediate
      args:
        executable: /bin/bash
      when: stage2_nix and pg_pid_file.stat.exists

    - name: Run unit tests
      import_tasks: tasks/test-image.yml
      tags:
        - unit-tests
      when: debpkg_mode or stage2_nix

    - name: Collect Postgres binaries
      import_tasks: tasks/internal/collect-pg-binaries.yml
      tags:
        - collect-binaries
      when: debpkg_mode

    - name: Install osquery from nixpkgs binary cache
      become: yes
      shell: |
        sudo -u ubuntu bash -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && nix profile install github:nixos/nixpkgs/f98ec4f73c762223d62bee706726138cb6ea27cc#osquery"
      when: stage2_nix

    - name: Pre-check before osquery - Verify system state  
      shell: |
        echo "=== Final System State Check ==="
        echo "User details:"
        id pgbouncer
        echo "\nGroup memberships:"
        for group in postgres ssl-cert pgbouncer; do
          echo "$group:" $(getent group $group)
        done
      register: final_system_check

    - name: Display final system state
      debug:
        var: final_system_check.stdout_lines

    - name: Ensure pgbouncer has correct group memberships
      fail:
        msg: "pgbouncer user is missing required group memberships"
      when: >
        final_system_check.stdout is not search('postgres') or
        final_system_check.stdout is not search('ssl-cert') or
        final_system_check.stdout is not search('pgbouncer')

    - name: Display final system state
      debug:
        var: final_system_check.stdout_lines

    - name: Ensure pgbouncer has correct group memberships
      fail:
        msg: "pgbouncer user is missing required group memberships"
      when: >
        final_system_check.stdout is not search('postgres') or
        final_system_check.stdout is not search('ssl-cert') or
        final_system_check.stdout is not search('pgbouncer')

    - name: Run osquery permission checks
      become: yes
      shell: |
        sudo -u ubuntu bash -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && /usr/bin/python3 /tmp/ansible-playbook/ansible/files/permission_check.py"
      when: stage2_nix

    - name: Remove osquery
      become: yes
      shell: |
        sudo -u ubuntu bash -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && nix profile remove osquery"
      when: stage2_nix

    - name: nix collect garbage
      become: yes
      shell: |
        sudo -u ubuntu bash -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && nix-collect-garbage -d"
      when: stage2_nix

'''
'''--- /postgres/ansible/tasks/setup-envoy.yml ---
# Group and user creation first
- name: Envoy - create group
  group:
    name: envoy
    state: present
    system: yes
  when: stage2_nix

- name: Envoy - system user
  user:
    name: envoy
    system: yes
    group: envoy
    shell: /bin/false
    create_home: no
  when: stage2_nix

# Verify user creation
- name: Verify envoy user creation
  shell: |
    getent passwd envoy || echo "User missing"
    getent group envoy || echo "Group missing"
  register: user_check
  changed_when: false
  when: stage2_nix

- name: Display user verification
  debug:
    var: user_check.stdout_lines
  when: stage2_nix

# - name: Verify envoy user and group setup
#   block:
#     - name: Check envoy user and group existence
#       shell: |
#         echo "=== Envoy User/Group Verification ==="
#         id envoy
#         echo "Group details:"
#         getent group envoy
#       register: envoy_verify
#       changed_when: false

#     - name: Display verification results
#       debug:
#         var: envoy_verify.stdout_lines
#   when: stage2_nix

# Then create directories
- name: Create envoy directories
  file:
    path: "{{ item }}"
    state: directory
    owner: envoy
    group: envoy
    mode: '0755'
    recurse: yes
  with_items:
    - /opt/envoy
    - /etc/envoy
  when: stage2_nix

- name: Verify directory permissions
  shell: |
    ls -la /opt/envoy
    ls -la /etc/envoy
  register: dir_check
  changed_when: false
  when: stage2_nix

- name: Display directory verification
  debug:
    var: dir_check.stdout_lines
  when: stage2_nix


# Download and setup binaries
- name: Envoy - download binary
  get_url:
    checksum: "{{ envoy_release_checksum }}"
    dest: /opt/envoy
    group: envoy
    mode: '0755'
    owner: envoy
    url: "https://github.com/envoyproxy/envoy/releases/download/v{{ envoy_release }}/envoy-{{ envoy_release }}-linux-aarch_64"
  when: stage2_nix

- name: Envoy - download hot restarter script
  get_url:
    checksum: "{{ envoy_hot_restarter_release_checksum }}"
    dest: /opt/envoy-hot-restarter.py
    group: envoy
    mode: '0755'
    owner: envoy
    url: "https://raw.githubusercontent.com/envoyproxy/envoy/v{{ envoy_release }}/restarter/hot-restarter.py"
  when: stage2_nix

# System configurations
- name: Envoy - bump up ulimit
  community.general.pam_limits:
    domain: envoy
    limit_item: nofile
    limit_type: soft
    value: 4096
  when: stage2_nix

# Configuration files
- name: Envoy - create script to start envoy
  copy:
    dest: /opt/start-envoy.sh
    group: envoy
    mode: '0755'
    owner: envoy
    src: files/start-envoy.sh
  when: stage2_nix

- name: Envoy - create configuration files
  copy:
    dest: /etc/envoy/
    directory_mode: '0775'
    group: envoy
    mode: '0664'
    owner: envoy
    src: files/envoy_config/
  when: stage2_nix

# Service setup
- name: Envoy - create service file
  copy:
    dest: /etc/systemd/system/envoy.service
    mode: '0644'
    src: files/envoy.service
  when: stage2_nix

- name: Envoy - configure systemd
  systemd:
    daemon_reload: true
    enabled: false
    name: envoy
    state: stopped
  when: stage2_nix

- name: Verify envoy final setup
  block:
    - name: Check envoy installation
      shell: |
        echo "=== Final Envoy Setup Verification ==="
        # Check executable permissions
        if [ ! -x /opt/envoy ]; then
          echo "Envoy binary not executable"
          exit 1
        fi
        if [ ! -x /opt/envoy-hot-restarter.py ]; then
          echo "Hot restarter not executable"
          exit 1
        fi
        # Check directory existence
        if [ ! -d /etc/envoy ]; then
          echo "Config directory missing"
          exit 1
        fi
        # Check user and group
        if ! getent passwd envoy >/dev/null; then
          echo "User missing"
          exit 1
        fi
        if ! getent group envoy >/dev/null; then
          echo "Group missing"
          exit 1
        fi
        if ! id envoy | grep -q "envoy"; then
          echo "Group membership incorrect"
          exit 1
        fi
      register: install_check
      changed_when: false

    - name: Debug verification results
      debug:
        var: install_check.stdout_lines
  when: stage2_nix

- name: Verify envoy configuration
  shell: |
    echo "Checking configuration files..."
    ls -la /etc/envoy/
    echo "Checking binary permissions..."
    ls -la /opt/envoy*
  register: config_check
  changed_when: false
  when: stage2_nix

- name: Show configuration check results
  debug:
    var: config_check.stdout_lines
  when: stage2_nix

# - name: Envoy - system user
#   ansible.builtin.user:
#     name: envoy

# - name: Envoy - download binary
#   ansible.builtin.get_url:
#     checksum: "{{ envoy_release_checksum }}"
#     dest: /opt/envoy
#     group: envoy
#     mode: u+x
#     owner: envoy
#     # yamllint disable-line rule:line-length
#     url: "https://github.com/envoyproxy/envoy/releases/download/v{{ envoy_release }}/envoy-{{ envoy_release }}-linux-aarch_64"

# - name: Envoy - download hot restarter script
#   ansible.builtin.get_url:
#     checksum: "{{ envoy_hot_restarter_release_checksum }}"
#     dest: /opt/envoy-hot-restarter.py
#     group: envoy
#     mode: u+x
#     owner: envoy
#     # yamllint disable-line rule:line-length
#     url: https://raw.githubusercontent.com/envoyproxy/envoy/v{{ envoy_release }}/restarter/hot-restarter.py

# - name: Envoy - bump up ulimit
#   community.general.pam_limits:
#     domain: envoy
#     limit_item: nofile
#     limit_type: soft
#     value: 4096

# - name: Envoy - create script to start envoy
#   ansible.builtin.copy:
#     dest: /opt/start-envoy.sh
#     group: envoy
#     mode: u+x
#     owner: envoy
#     src: files/start-envoy.sh

# - name: Envoy - create configuration files
#   ansible.builtin.copy:
#     dest: /etc/envoy/
#     directory_mode: u=rwx,g=rwx,o=rx
#     group: envoy
#     mode: u=rw,g=rw,o=r
#     owner: envoy
#     src: files/envoy_config/

# - name: Envoy - create service file
#   ansible.builtin.copy:
#     dest: /etc/systemd/system/envoy.service
#     mode: u=rw,g=r,o=r
#     src: files/envoy.service

# - name: Envoy - disable service
#   ansible.builtin.systemd:
#     daemon_reload: true
#     enabled: false
#     name: envoy
#     state: stopped

'''
'''--- /postgres/ansible/tasks/fix_ipv6_ndisc.yml ---
---
- name: fix Network - systemd timer file
  copy:
    dest: /etc/systemd/system/systemd-networkd-check-and-fix.timer
    src: "files/systemd-networkd/systemd-networkd-check-and-fix.timer"
    owner: root
    group: root
    mode: 0644

- name: fix Network - systemd service file
  copy:
    dest: /etc/systemd/system/systemd-networkd-check-and-fix.service
    src: "files/systemd-networkd/systemd-networkd-check-and-fix.service"
    owner: root
    group: root
    mode: 0644

- name: fix Network - detect script
  copy:
    dest: /usr/local/bin/systemd-networkd-check-and-fix.sh
    src: "files/systemd-networkd/systemd-networkd-check-and-fix.sh"
    owner: root
    group: root
    mode: 0700

- name: fix Network - reload systemd
  systemd:
    daemon_reload: yes

- name: fix Network - enable systemd timer
  systemd:
    name: systemd-networkd-check-and-fix.timer
    enabled: true

'''
'''--- /postgres/ansible/tasks/setup-pgbouncer.yml ---
# Keep the installation tasks but add conditions
- name: PgBouncer - download & install dependencies
  apt:
    pkg:
      - build-essential
      - libssl-dev
      - pkg-config
      - libevent-dev
      - libsystemd-dev
    update_cache: yes
    cache_valid_time: 3600
  when: debpkg_mode or stage2_nix

# Add when conditions to download, unpack, configure, build, install
- name: PgBouncer - download latest release
  get_url:
    url: "https://www.pgbouncer.org/downloads/files/{{ pgbouncer_release }}/pgbouncer-{{ pgbouncer_release }}.tar.gz"
    dest: /tmp/pgbouncer-{{ pgbouncer_release }}.tar.gz
    checksum: "{{ pgbouncer_release_checksum }}"
    timeout: 60
  when: debpkg_mode or stage2_nix

- name: PgBouncer - unpack archive
  unarchive:
    remote_src: yes
    src: /tmp/pgbouncer-{{ pgbouncer_release }}.tar.gz
    dest: /tmp
  become: yes

- name: PgBouncer - configure
  shell:
    cmd: "./configure --prefix=/usr/local --with-systemd"
    chdir: /tmp/pgbouncer-{{ pgbouncer_release }}
  become: yes

- name: PgBouncer - build
  make:
    chdir: /tmp/pgbouncer-{{ pgbouncer_release }}
  become: yes

- name: PgBouncer - install
  make:
    chdir: /tmp/pgbouncer-{{ pgbouncer_release }}
    target: install
  become: yes

- name: Debug - Show execution mode
  debug:
    msg: 
      - "Running in: {{ 'nixpkg_mode' if nixpkg_mode | default(false) else 'debpkg_mode' if debpkg_mode | default(false) else 'stage2_nix' if stage2_nix | default(false) else 'unknown' }} mode"

- name: Debug - Initial group status
  shell: |
    echo "=== Initial Group Status ==="
    for group in postgres ssl-cert pgbouncer; do
      echo "[$group]"
      getent group $group || echo "not found"
    done
  register: initial_groups
  changed_when: false

# Group and user management - consolidated version
- name: Ensure required groups exist with specific GIDs
  group:
    name: "{{ item.name }}"
    gid: "{{ item.gid }}"
    state: present
    system: yes
  loop:
    - { name: 'postgres', gid: 1002 }
    - { name: 'ssl-cert', gid: 1001 }
    - { name: 'pgbouncer', gid: 101 }
  when: stage2_nix

- name: Create pgbouncer user
  user:
    name: pgbouncer
    uid: 101
    shell: /bin/false
    system: yes
    comment: PgBouncer user
    group: pgbouncer
  when: stage2_nix

- name: Add pgbouncer to groups and verify
  block:
    - name: Add pgbouncer to groups
      shell: |
        usermod -a -G postgres,ssl-cert pgbouncer
        systemctl daemon-reload
        if command -v nscd >/dev/null 2>&1; then
          nscd -i group
          nscd -i passwd
        fi
        sleep 2
        # Verify membership
        id pgbouncer | grep -q "postgres" && \
        id pgbouncer | grep -q "ssl-cert" && \
        id pgbouncer | grep -q "pgbouncer"
      register: group_add_result
      failed_when: group_add_result.rc != 0

    - name: Verify final group memberships
      shell: |
        echo "=== Final Group Memberships ==="
        id pgbouncer
        echo "Group details:"
        getent group postgres
        getent group ssl-cert
        getent group pgbouncer
      register: final_verify
  when: stage2_nix

# Directory and file setup
- name: Create PgBouncer directories
  file:
    path: "{{ item.path }}"
    state: directory
    owner: pgbouncer
    group: pgbouncer
    mode: "{{ item.mode }}"
  loop:
    - { path: '/etc/pgbouncer', mode: '0700' }
    - { path: '/etc/pgbouncer-custom', mode: '0775' }
  when: stage2_nix

- name: Create config files
  file:
    path: "/etc/pgbouncer-custom/{{ item }}"
    state: touch
    owner: pgbouncer
    group: pgbouncer
    mode: '0664'
  loop:
    - 'generated-optimizations.ini'
    - 'custom-overrides.ini'
    - 'ssl-config.ini'
  when: stage2_nix

# Configuration files
- name: Configure PgBouncer
  block:
    - name: Copy pgbouncer.ini
      copy:
        src: files/pgbouncer_config/pgbouncer.ini.j2
        dest: /etc/pgbouncer/pgbouncer.ini
        owner: pgbouncer
        mode: '0700'

    - name: Create userlist.txt
      file:
        path: /etc/pgbouncer/userlist.txt
        state: touch
        owner: pgbouncer
        mode: '0700'

    - name: Configure tmpfiles.d
      template:
        src: files/pgbouncer_config/tmpfiles.d-pgbouncer.conf.j2
        dest: /etc/tmpfiles.d/pgbouncer.conf

    - name: Configure SSL
      copy:
        dest: /etc/pgbouncer-custom/ssl-config.ini
        content: |
          client_tls_sslmode = allow
        owner: pgbouncer
        group: pgbouncer
        mode: '0664'
  when: stage2_nix

# Permissions and fail2ban
- name: Configure permissions and security
  block:
    - name: Set file permissions
      shell: |
        chmod g+w /etc/postgresql/pg_hba.conf
        chmod g+w /etc/pgbouncer-custom/ssl-config.ini

    - name: Configure fail2ban
      template:
        src: "files/fail2ban_config/{{ item.src }}"
        dest: "/etc/fail2ban/{{ item.dest }}"
      loop:
        - { src: 'jail-pgbouncer.conf.j2', dest: 'jail.d/pgbouncer.conf' }
        - { src: 'filter-pgbouncer.conf.j2', dest: 'filter.d/pgbouncer.conf' }
  when: stage2_nix

# Systemd setup
- name: Configure systemd
  block:
    - name: Install service file
      template:
        src: files/pgbouncer_config/pgbouncer.service.j2
        dest: /etc/systemd/system/pgbouncer.service

    - name: Reload systemd
      systemd:
        daemon_reload: yes
  when: stage2_nix
'''
'''--- /postgres/ansible/tasks/setup-docker.yml ---
- name: Copy extension packages
  copy:
    src: files/extensions/
    dest: /tmp/extensions/
  when: debpkg_mode

# Builtin apt module does not support wildcard for deb paths
- name: Install extensions
  shell: |
    set -e
    apt-get update
    apt-get install -y --no-install-recommends /tmp/extensions/*.deb
  when: debpkg_mode

- name: pgsodium - determine postgres bin directory
  shell: pg_config --bindir
  register: pg_bindir_output
  when: debpkg_mode
  
- set_fact:
    pg_bindir: "{{ pg_bindir_output.stdout }}"
  when: debpkg_mode 

- name: pgsodium - set pgsodium.getkey_script
  become: yes
  lineinfile:
    path: /etc/postgresql/postgresql.conf
    state: present
    # script is expected to be placed by finalization tasks for different target platforms
    line: pgsodium.getkey_script= '{{ pg_bindir }}/pgsodium_getkey.sh'
  when: debpkg_mode

# supautils
- name: supautils - add supautils to session_preload_libraries
  become: yes
  replace:
    path: /etc/postgresql/postgresql.conf
    regexp: "#session_preload_libraries = ''"
    replace: session_preload_libraries = 'supautils'
  when: debpkg_mode or stage2_nix

- name: supautils - write custom supautils.conf
  template:
    src: "files/postgresql_config/supautils.conf.j2"
    dest: /etc/postgresql-custom/supautils.conf
    mode: 0664
    owner: postgres
    group: postgres
  when: debpkg_mode or stage2_nix

- name: supautils - copy extension custom scripts
  copy:
    src: files/postgresql_extension_custom_scripts/
    dest: /etc/postgresql-custom/extension-custom-scripts
  become: yes
  when: debpkg_mode or stage2_nix

- name: supautils - chown extension custom scripts
  file:
    mode: 0775
    owner: postgres
    group: postgres
    path: /etc/postgresql-custom/extension-custom-scripts
    recurse: yes
  become: yes
  when: debpkg_mode or stage2_nix

- name: supautils - include /etc/postgresql-custom/supautils.conf in postgresql.conf
  become: yes
  replace:
    path: /etc/postgresql/postgresql.conf
    regexp: "#include = '/etc/postgresql-custom/supautils.conf'"
    replace: "include = '/etc/postgresql-custom/supautils.conf'"
  when: debpkg_mode or stage2_nix

- name: Cleanup - extension packages
  file:
    path: /tmp/extensions
    state: absent
  when: debpkg_mode

'''
'''--- /postgres/ansible/tasks/setup-fail2ban.yml ---
# set default bantime to 1 hour
- name: extend bantime
  become: yes
  replace:
    path: /etc/fail2ban/jail.conf
    regexp: bantime  = 10m
    replace: bantime  = 3600
  when: debpkg_mode or nixpkg_mode

- name: Configure journald
  copy:
    src: files/fail2ban_config/jail-ssh.conf
    dest: /etc/fail2ban/jail.d/sshd.local
  when: debpkg_mode or nixpkg_mode

- name: configure fail2ban to use nftables
  copy:
    src: files/fail2ban_config/jail.local
    dest: /etc/fail2ban/jail.local
  when: debpkg_mode or nixpkg_mode

# postgresql
- name: import jail.d/postgresql.conf
  template:
    src: files/fail2ban_config/jail-postgresql.conf.j2
    dest: /etc/fail2ban/jail.d/postgresql.conf
  become: yes
  when: debpkg_mode or nixpkg_mode

- name: import filter.d/postgresql.conf
  template:
    src: files/fail2ban_config/filter-postgresql.conf.j2
    dest: /etc/fail2ban/filter.d/postgresql.conf
  become: yes
  when: debpkg_mode or nixpkg_mode

- name: create overrides dir
  file:
    state: directory
    owner: root
    group: root
    path: /etc/systemd/system/fail2ban.service.d
    mode: '0700'
  when: debpkg_mode or nixpkg_mode

- name: Custom systemd overrides
  copy:
    src: files/fail2ban_config/fail2ban.service.conf
    dest: /etc/systemd/system/fail2ban.service.d/overrides.conf
  when: debpkg_mode or nixpkg_mode

- name: add in supabase specific ignore filters
  lineinfile:
    path: /etc/fail2ban/filter.d/postgresql.conf
    state: present
    line: "{{ item.line }}"
  loop:
    - { line: '              ^.*,.*,.*,.*,"<HOST>:.*password authentication failed for user ""supabase_admin".*$' }
    - { line: '              ^.*,.*,.*,.*,"<HOST>:.*password authentication failed for user ""supabase_auth_admin".*$' }
    - { line: '              ^.*,.*,.*,.*,"<HOST>:.*password authentication failed for user ""supabase_storage_admin".*$' }
    - { line: '              ^.*,.*,.*,.*,"<HOST>:.*password authentication failed for user ""authenticator".*$' }
    - { line: '              ^.*,.*,.*,.*,"<HOST>:.*password authentication failed for user ""pgbouncer".*$' }
  become: yes
  tags:
    - install-supabase-internal
  when: debpkg_mode or nixpkg_mode

# Restart
- name: fail2ban - restart
  systemd:
    name: fail2ban
    state: restarted
  when: debpkg_mode or nixpkg_mode

- name: fail2ban - disable service
  systemd:
    name: fail2ban
    enabled: no
    daemon_reload: yes
  when: debpkg_mode or nixpkg_mode
'''
'''--- /postgres/ansible/tasks/setup-kong.yml ---
# User and group setup first
- name: Kong - create group
  group:
    name: kong
    state: present
    system: yes
  when: stage2_nix

- name: Kong - system user
  user:
    name: kong
    system: yes
    group: kong
    shell: /bin/false
    create_home: no
  when: stage2_nix

- name: Verify Kong user and group setup
  block:
    - name: Check Kong user and group existence
      shell: |
        echo "=== Kong User/Group Verification ==="
        id kong || echo "User kong not found"
        getent group kong || echo "Group kong not found"
      register: kong_verify
      changed_when: false

    - name: Display verification results
      debug:
        var: kong_verify.stdout_lines
  when: stage2_nix

# Rest of Kong installation
- name: Kong - system dependencies
  apt:
    pkg:
      - openssl
      - libpcre3
      - procps
      - perl
    state: present
  when: stage2_nix

- name: Kong - download deb package
  get_url:
    url: "https://packages.konghq.com/public/gateway-28/deb/ubuntu/pool/{{ kong_release_target }}/main/k/ko/kong_2.8.1/{{ kong_deb }}"
    dest: /tmp/kong.deb
    checksum: "{{ kong_deb_checksum }}"
  when: stage2_nix

- name: Kong - deb installation
  apt: 
    deb: file:///tmp/kong.deb
  when: stage2_nix

- name: Kong - ensure it is NOT autoremoved
  shell: |
    set -e
    apt-mark manual kong zlib1g*
  when: stage2_nix

- name: Kong - configuration
  template:
    src: files/kong_config/kong.conf.j2
    dest: /etc/kong/kong.conf
    owner: kong
    group: kong
    mode: '0644'
  when: stage2_nix

- name: Kong - hand over ownership of /usr/local/kong to user kong
  file:
    path: /usr/local/kong
    recurse: yes
    owner: kong
    group: kong
    mode: '0755'
  when: stage2_nix

- name: Kong - bump up ulimit
  pam_limits:
    limit_item: nofile
    limit_type: soft
    domain: kong
    value: "4096"
  when: stage2_nix

- name: Kong - create env file
  template:
    src: files/kong_config/kong.env.j2
    dest: /etc/kong/kong.env
    owner: kong
    group: kong
    mode: '0644'
  when: stage2_nix

- name: Kong - create service file
  template:
    src: files/kong_config/kong.service.j2
    dest: /etc/systemd/system/kong.service
    mode: '0644'
  when: stage2_nix

- name: Kong - final verification
  block:
    - name: Verify Kong installation
      shell: |
        echo "=== Kong Installation Verification ==="
        id kong
        ls -la /usr/local/kong
        ls -la /etc/kong
        getent group kong
      register: final_verify
      changed_when: false

    - name: Display final verification
      debug:
        var: final_verify.stdout_lines
  when: stage2_nix

- name: Kong - disable service
  systemd:
    enabled: no
    name: kong
    state: stopped
    daemon_reload: yes
  when: stage2_nix

# - name: Kong - system user
#   user: name=kong

# # Kong installation steps from http://archive.vn/3HRQx
# - name: Kong - system dependencies
#   apt:
#     pkg:
#       - openssl
#       - libpcre3
#       - procps
#       - perl

# - name: Kong - download deb package
#   get_url:
#     url: "https://packages.konghq.com/public/gateway-28/deb/ubuntu/pool/{{ kong_release_target }}/main/k/ko/kong_2.8.1/{{ kong_deb }}"
#     dest: /tmp/kong.deb
#     checksum: "{{ kong_deb_checksum }}"

# - name: Kong - deb installation
#   apt: deb=file:///tmp/kong.deb

# - name: Kong - ensure it is NOT autoremoved
#   shell: |
#     set -e
#     apt-mark manual kong zlib1g*

# - name: Kong - configuration
#   template:
#     src: files/kong_config/kong.conf.j2
#     dest: /etc/kong/kong.conf

# - name: Kong - hand over ownership of /usr/local/kong to user kong
#   file:
#     path: /usr/local/kong
#     recurse: yes
#     owner: kong

# # [warn] ulimit is currently set to "1024". For better performance set it to at least
# # "4096" using "ulimit -n"
# - name: Kong - bump up ulimit
#   pam_limits:
#     limit_item: nofile
#     limit_type: soft
#     domain: kong
#     value: "4096"

# - name: Kong - create env file
#   template:
#     src: files/kong_config/kong.env.j2
#     dest: /etc/kong/kong.env

# - name: Kong - create service file
#   template:
#     src: files/kong_config/kong.service.j2
#     dest: /etc/systemd/system/kong.service

# - name: Kong - disable service
#   systemd:
#     enabled: no
#     name: kong
#     state: stopped
#     daemon_reload: yes

'''
'''--- /postgres/ansible/tasks/setup-extensions.yml ---
- name: Install plv8
  import_tasks: tasks/postgres-extensions/13-plv8.yml

- name: Install pg_jsonschema
  import_tasks: tasks/postgres-extensions/22-pg_jsonschema.yml

- name: Install postgis
  import_tasks: tasks/postgres-extensions/01-postgis.yml

- name: Install pgrouting
  import_tasks: tasks/postgres-extensions/02-pgrouting.yml

- name: Install pgtap
  import_tasks: tasks/postgres-extensions/03-pgtap.yml

- name: Install pg_cron
  import_tasks: tasks/postgres-extensions/04-pg_cron.yml

- name: Install pgaudit
  import_tasks: tasks/postgres-extensions/05-pgaudit.yml

- name: Install pgjwt
  import_tasks: tasks/postgres-extensions/06-pgjwt.yml

- name: Install pgsql-http
  import_tasks: tasks/postgres-extensions/07-pgsql-http.yml

- name: Install plpgsql_check
  import_tasks: tasks/postgres-extensions/08-plpgsql_check.yml

- name: Install pg-safeupdate
  import_tasks: tasks/postgres-extensions/09-pg-safeupdate.yml

- name: Install timescaledb
  import_tasks: tasks/postgres-extensions/10-timescaledb.yml

- name: Install wal2json
  import_tasks: tasks/postgres-extensions/11-wal2json.yml

- name: Install pljava
  import_tasks: tasks/postgres-extensions/12-pljava.yml
  tags:
    - legacy-incompatible

- name: Install pg_plan_filter
  import_tasks: tasks/postgres-extensions/14-pg_plan_filter.yml

- name: Install pg_net
  import_tasks: tasks/postgres-extensions/15-pg_net.yml

- name: Install rum
  import_tasks: tasks/postgres-extensions/16-rum.yml

- name: Install pg_hashids
  import_tasks: tasks/postgres-extensions/17-pg_hashids.yml

- name: Install pgsodium
  import_tasks: tasks/postgres-extensions/18-pgsodium.yml

- name: Install pg_graphql
  import_tasks: tasks/postgres-extensions/19-pg_graphql.yml
  tags:
    - legacy-incompatible

- name: Install pg_stat_monitor
  import_tasks: tasks/postgres-extensions/20-pg_stat_monitor.yml

- name: Install vault
  import_tasks: tasks/postgres-extensions/23-vault.yml

- name: Install PGroonga
  import_tasks: tasks/postgres-extensions/24-pgroonga.yml

- name: Install wrappers
  import_tasks: tasks/postgres-extensions/25-wrappers.yml

- name: Install hypopg
  import_tasks: tasks/postgres-extensions/26-hypopg.yml

 - name: Install pg_repack
  import_tasks: tasks/postgres-extensions/27-pg_repack.yml
  
- name: Install pgvector
  import_tasks: tasks/postgres-extensions/28-pgvector.yml

- name: Install Trusted Language Extensions
  import_tasks: tasks/postgres-extensions/29-pg_tle.yml

- name: Verify async task status
  import_tasks: tasks/postgres-extensions/99-finish_async_tasks.yml
  when: async_mode
 
'''
'''--- /postgres/ansible/tasks/setup-system.yml ---
- name: System - apt update and apt upgrade
  apt: update_cache=yes upgrade=yes
  when: debpkg_mode or nixpkg_mode
  # SEE http://archive.vn/DKJjs#parameter-upgrade

- name: Install required security updates
  apt:
    pkg:
      - tzdata
      - linux-libc-dev
  when: debpkg_mode or nixpkg_mode
# SEE https://github.com/georchestra/ansible/issues/55#issuecomment-588313638
# Without this, a similar error is faced
- name: Install Ansible dependencies
  apt:
    pkg:
      - acl
  when: debpkg_mode or nixpkg_mode

- name: Install security tools
  apt:
    pkg:
      - nftables
      - fail2ban
    update_cache: yes
    cache_valid_time: 3600
  when: debpkg_mode or nixpkg_mode

- name: Use nftables backend
  shell: |
    update-alternatives --set iptables /usr/sbin/iptables-nft
    update-alternatives --set ip6tables /usr/sbin/ip6tables-nft
    update-alternatives --set arptables /usr/sbin/arptables-nft
    update-alternatives --set ebtables /usr/sbin/ebtables-nft
    systemctl restart ufw
  when: debpkg_mode or nixpkg_mode

- name: Create Sysstat log directory
  file:
    path: /var/log/sysstat
    state: directory
  when: debpkg_mode or nixpkg_mode
    
- name: Install other useful tools
  apt:
    pkg:
      - bwm-ng
      - htop
      - net-tools
      - ngrep
      - sysstat
      - vim-tiny
    update_cache: yes
  when: debpkg_mode or nixpkg_mode

- name: Configure sysstat
  copy:
    src: files/sysstat.sysstat
    dest: /etc/sysstat/sysstat
  when: debpkg_mode or nixpkg_mode

- name: Configure default sysstat
  copy:
    src: files/default.sysstat
    dest: /etc/default/sysstat
  when: debpkg_mode or nixpkg_mode


- name: Adjust APT update intervals
  copy:
    src: files/apt_periodic
    dest: /etc/apt/apt.conf.d/10periodic
  when: debpkg_mode or nixpkg_mode

# Find platform architecture and set as a variable
- name: finding platform architecture
  shell: if [ $(uname -m) = "aarch64" ]; then echo "arm64";  else echo "amd64"; fi
  register: platform_output
  tags:
    - update
    - update-only
- set_fact:
    platform: "{{ platform_output.stdout }}"
  tags:
    - update
    - update-only
  when: debpkg_mode or nixpkg_mode or stage2_nix

- name: create overrides dir
  file:
    state: directory
    owner: root
    group: root
    path: /etc/systemd/system/systemd-resolved.service.d
    mode: '0700'
  when: debpkg_mode or nixpkg_mode

- name: Custom systemd overrides for resolved
  copy:
    src: files/systemd-resolved.conf
    dest: /etc/systemd/system/systemd-resolved.service.d/override.conf
  when: debpkg_mode or nixpkg_mode

- name: System - Create services.slice
  template:
    src: files/services.slice.j2
    dest: /etc/systemd/system/services.slice
  when: debpkg_mode or nixpkg_mode


- name: System - systemd reload
  systemd: daemon_reload=yes
  when: debpkg_mode or nixpkg_mode

- name: Configure journald
  copy:
    src: files/journald.conf
    dest: /etc/systemd/journald.conf
  when: debpkg_mode or nixpkg_mode

- name: reload systemd-journald
  systemd:
   name: systemd-journald
   state: restarted
  when: debpkg_mode or nixpkg_mode

- name: Configure logind
  copy:
    src: files/logind.conf
    dest: /etc/systemd/logind.conf
  when: debpkg_mode or nixpkg_mode

- name: reload systemd-logind
  systemd:
   name: systemd-logind
   state: restarted
  when: debpkg_mode or nixpkg_mode

- name: enable timestamps for shell history
  copy:
    content: |
      export HISTTIMEFORMAT='%d/%m/%y %T '
    dest: /etc/profile.d/09-history-timestamps.sh
    mode: 0644
    owner: root
    group: root
  when: debpkg_mode or nixpkg_mode

- name: set hosts file
  copy:
    content: |
      127.0.0.1   localhost
      ::1         localhost
    dest: /etc/hosts
    mode: 0644
    owner: root
    group: root
  when: debpkg_mode or stage2_nix

#Set Sysctl params for restarting the OS on oom after 10
- name: Set vm.panic_on_oom=1
  ansible.builtin.sysctl:
    name: vm.panic_on_oom
    value: '1'
    state: present
    reload: yes
  when: debpkg_mode or nixpkg_mode

- name: Set kernel.panic=10
  ansible.builtin.sysctl:
    name: kernel.panic
    value: '10'
    state: present
    reload: yes
  when: debpkg_mode or nixpkg_mode

- name: configure system
  ansible.posix.sysctl:
    name: 'net.core.somaxconn'
    value: 16834

- name: configure system
  ansible.posix.sysctl:
    name: 'net.ipv4.ip_local_port_range'
    value: '1025 65000'

#Set Sysctl params specific to keepalives
- name: Set net.ipv4.tcp_keepalive_time=1800
  ansible.builtin.sysctl:
    name: net.ipv4.tcp_keepalive_time
    value: 1800
    state: present
  when: debpkg_mode or nixpkg_mode
- name: Set net.ipv4.tcp_keepalive_intvl=60
  ansible.builtin.sysctl:
    name: net.ipv4.tcp_keepalive_intvl
    value: 60
    state: present
  when: debpkg_mode or nixpkg_mode

'''
'''--- /postgres/ansible/tasks/setup-vector.yml ---
# First create vector group and user
- name: Vector - create group
  group:
    name: vector
    state: present
    system: yes
  when: stage2_nix

- name: Vector - system user
  user:
    name: vector
    system: yes
    group: vector
    shell: /bin/false
    create_home: no
  when: stage2_nix

- name: Add vector to required groups
  user:
    name: vector
    groups: vector,adm,systemd-journal,postgres
    append: yes
  when: stage2_nix

- name: Vector - install dependencies
  apt:
    pkg:
      - curl
      - ca-certificates
    state: present
  when: stage2_nix

- name: Vector - download deb package
  get_url:
    url: "{{ vector_arm_deb if platform == 'arm64' else vector_x86_deb }}"
    dest: /tmp/vector.deb
  when: stage2_nix

- name: Vector - install package
  apt:
    deb: /tmp/vector.deb
  when: stage2_nix

- name: Create vector directories
  file:
    path: "{{ item }}"
    state: directory
    owner: vector
    group: vector
    mode: '0755'
  loop:
    - /etc/vector
    - /var/lib/vector
    - /var/log/vector
  when: stage2_nix

- name: Verify vector setup
  block:
    - name: Check vector installation
      shell: |
        echo "=== Vector Installation Verification ==="
        id vector
        echo "Group memberships:"
        for group in vector adm systemd-journal postgres; do
          echo "Checking $group:"
          getent group $group | grep vector || echo "Not in $group"
        done
        which vector || echo "Vector binary not found"
        ls -la /etc/vector
      register: verify_result
      changed_when: false

    - name: Show verification results
      debug:
        var: verify_result.stdout_lines
  when: stage2_nix

- name: Vector - create service file
  template:
    src: files/vector.service.j2
    dest: /etc/systemd/system/vector.service
    mode: '0644'
  when: stage2_nix

- name: Vector - reload systemd
  systemd:
    daemon_reload: yes
  when: stage2_nix
'''
'''--- /postgres/ansible/tasks/setup-postgrest.yml ---
- name: PostgREST - create group
  group:
    name: postgrest
    state: present
    system: yes
  when: stage2_nix

# - name: PostgREST - system user
#   user: name=postgrest

- name: PostgREST - add Postgres PPA gpg key
  apt_key:
    url: https://www.postgresql.org/media/keys/ACCC4CF8.asc
    state: present

- name: PostgREST - add Postgres PPA
  apt_repository:
    repo: "deb http://apt.postgresql.org/pub/repos/apt/ focal-pgdg main"
    state: present
  when: stage2_nix

- name: PostgREST - system user
  user:
    name: postgrest
    system: yes
    group: postgrest
    shell: /bin/false
    create_home: no
  when: stage2_nix

- name: PostgREST - update apt cache
  apt:
    update_cache: yes
  when: stage2_nix

# libpq is a C library that enables user programs to communicate with
# the PostgreSQL database server.

- name: PostgREST - system dependencies
  apt:
    pkg:
      - libpq5
      - libnuma-dev
  when: stage2_nix


- name: PostgREST - remove Postgres PPA gpg key
  apt_key:
    url: https://www.postgresql.org/media/keys/ACCC4CF8.asc
    state: absent

- name: PostgREST - remove Postgres PPA
  apt_repository:
    repo: "deb http://apt.postgresql.org/pub/repos/apt/ focal-pgdg {{ postgresql_major }}"
    state: absent

- name: postgis - ensure dependencies do not get autoremoved
  shell: |
    set -e
    apt-mark manual libpq5*
    apt-mark manual libnuma*
    apt-mark auto libnuma*-dev

- name: PostgREST - download ubuntu binary archive (arm)
  get_url:
    url: "https://github.com/PostgREST/postgrest/releases/download/v{{ postgrest_release }}/postgrest-v{{ postgrest_release }}-ubuntu-aarch64.tar.xz"
    dest: /tmp/postgrest.tar.xz
    checksum: "{{ postgrest_arm_release_checksum }}"
    timeout: 60
  when: platform == "arm64"

- name: PostgREST - download ubuntu binary archive (x86)
  get_url:
    url: "https://github.com/PostgREST/postgrest/releases/download/v{{ postgrest_release }}/postgrest-v{{ postgrest_release }}-linux-static-x64.tar.xz"
    dest: /tmp/postgrest.tar.xz
    checksum: "{{ postgrest_x86_release_checksum }}"
    timeout: 60    
  when: platform == "amd64"

- name: PostgREST - unpack archive in /opt
  unarchive:
    remote_src: yes
    src: /tmp/postgrest.tar.xz
    dest: /opt
    owner: postgrest
    mode: '0755'

- name: create directories
  file:
    state: directory
    owner: postgrest
    group: postgrest
    mode: '0775'
    path: /etc/postgrest

- name: empty files
  file:
    state: touch
    owner: postgrest
    group: postgrest
    path: /etc/postgrest/{{ item }}
  with_items:
    - base.conf
    - generated.conf

- name: create conf merging script
  copy:
    content: |
      #! /usr/bin/env bash
      set -euo pipefail
      set -x

      cd "$(dirname "$0")"
      cat $@ > merged.conf
    dest: /etc/postgrest/merge.sh
    mode: 0750
    owner: postgrest
    group: postgrest

- name: PostgREST - create service files
  template:
    src: files/{{ item }}.j2
    dest: /etc/systemd/system/{{ item }}
  with_items:
    - postgrest.service
    - postgrest-optimizations.service

- name: PostgREST - reload systemd
  systemd:
    daemon_reload: yes

'''
'''--- /postgres/ansible/tasks/setup-migrations.yml ---
- name: Run migrate.sh script
  shell: ./migrate.sh
  register: retval
  when: debpkg_mode or stage2_nix
  args:
    chdir: /tmp/migrations/db
  failed_when: retval.rc != 0

- name: Create /root/MIGRATION-AMI file
  file:
    path: "/root/MIGRATION-AMI"
    state: touch
  when: debpkg_mode or stage2_nix

'''
'''--- /postgres/ansible/tasks/setup-nginx.yml ---
# First create nginx group and user
- name: Nginx - create group
  group:
    name: nginx
    state: present
    system: yes
  when: stage2_nix

- name: Nginx - system user
  user:
    name: nginx
    system: yes
    group: nginx
    shell: /bin/false
    create_home: no
  when: stage2_nix

- name: Verify nginx user setup
  block:
    - name: Check nginx user and group existence
      shell: |
        echo "=== Nginx User/Group Verification ==="
        id nginx || echo "User nginx not found"
        getent group nginx || echo "Group nginx not found"
      register: nginx_verify
      changed_when: false

    - name: Display verification results
      debug:
        var: nginx_verify.stdout_lines
  when: stage2_nix

# Installation steps
- name: Nginx - system dependencies
  apt:
    pkg:
      - openssl
      - libpcre3-dev
      - libssl-dev
      - zlib1g-dev
    state: present
  when: stage2_nix

- name: Nginx - download source
  get_url:
    url: "https://nginx.org/download/nginx-{{ nginx_release }}.tar.gz"
    dest: /tmp/nginx-{{ nginx_release }}.tar.gz
    checksum: "{{ nginx_release_checksum }}"
  when: stage2_nix

- name: Nginx - unpack archive
  unarchive:
    remote_src: yes
    src: /tmp/nginx-{{ nginx_release }}.tar.gz
    dest: /tmp
  when: stage2_nix

- name: Nginx - configure
  shell:
    chdir: /tmp/nginx-{{ nginx_release }}
    cmd: |
      set -e
      ./configure \
      --prefix=/usr/local/nginx \
      --conf-path=/etc/nginx/nginx.conf \
      --with-http_ssl_module \
      --with-http_realip_module \
      --with-threads \
      --user=nginx \
      --group=nginx
  become: yes
  when: stage2_nix

- name: Nginx - build
  community.general.make:
    target: build
    chdir: /tmp/nginx-{{ nginx_release }}
    jobs: "{{ parallel_jobs | default(omit) }}"
  become: yes
  when: stage2_nix

- name: Nginx - install
  make:
    chdir: /tmp/nginx-{{ nginx_release }}
    target: install
  become: yes
  when: stage2_nix

- name: Create required nginx directories
  file:
    path: "{{ item }}"
    state: directory
    owner: nginx
    group: nginx
    mode: '0755'
  loop:
    - /usr/local/nginx
    - /etc/nginx
    - /var/log/nginx
    - /var/cache/nginx
  when: stage2_nix

- name: Nginx - set ownership and permissions
  block:
    - name: Set ownership of nginx directories
      file:
        path: "{{ item }}"
        state: directory
        owner: nginx
        group: nginx
        mode: '0755'
        recurse: yes
      loop:
        - /usr/local/nginx
        - /etc/nginx

    - name: Verify permissions
      shell: |
        echo "=== Nginx Permissions Check ==="
        ls -la /usr/local/nginx
        ls -la /etc/nginx
      register: perm_check
      changed_when: false

    - name: Show permissions check
      debug:
        var: perm_check.stdout_lines
  when: stage2_nix

- name: Nginx - bump up ulimit
  pam_limits:
    limit_item: nofile
    limit_type: soft
    domain: nginx
    value: "4096"
  when: stage2_nix

- name: Nginx - create service file
  template:
    src: files/nginx.service.j2
    dest: /etc/systemd/system/nginx.service
    owner: root
    group: root
    mode: '0644'
  when: stage2_nix

- name: Nginx - reload systemd
  systemd:
    daemon_reload: yes
  when: stage2_nix

- name: Final nginx verification
  block:
    - name: Verify nginx setup
      shell: |
        echo "=== Final Nginx Verification ==="
        id nginx
        getent group nginx
        ls -la /usr/local/nginx
        ls -la /etc/nginx
        test -f /etc/systemd/system/nginx.service || echo "Service file missing"
      register: final_verify
      changed_when: false

    - name: Show final verification
      debug:
        var: final_verify.stdout_lines
  when: stage2_nix

# - name: nginx - system user
#   user: name=nginx

# # Kong installation steps from http://archive.vn/3HRQx
# - name: nginx - system dependencies
#   apt:
#     pkg:
#       - openssl
#       - libpcre3-dev
#       - libssl-dev
#       - zlib1g-dev

# - name: nginx - download source
#   get_url:
#     url: "https://nginx.org/download/nginx-{{ nginx_release }}.tar.gz"
#     dest: /tmp/nginx-{{ nginx_release }}.tar.gz
#     checksum: "{{ nginx_release_checksum }}"

# - name: nginx - unpack archive
#   unarchive:
#     remote_src: yes
#     src: /tmp/nginx-{{ nginx_release }}.tar.gz
#     dest: /tmp

# - name: nginx - configure
#   shell:
#     chdir: /tmp/nginx-{{ nginx_release }}
#     cmd: |
#       set -e

#       ./configure \
#       --prefix=/usr/local/nginx \
#       --conf-path=/etc/nginx/nginx.conf \
#       --with-http_ssl_module \
#       --with-http_realip_module \
#       --with-threads
#   become: yes

# - name: nginx - build
#   community.general.make:
#     target: build
#     chdir: /tmp/nginx-{{ nginx_release }}
#     jobs: "{{ parallel_jobs | default(omit) }}"
#   become: yes

# - name: nginx - install
#   make:
#     chdir: /tmp/nginx-{{ nginx_release }}
#     target: install
#   become: yes

# - name: nginx - hand over ownership of /usr/local/nginx to user nginx
#   file:
#     path: /usr/local/nginx
#     recurse: yes
#     owner: nginx

# - name: nginx - hand over ownership of /etc/nginx to user nginx
#   file:
#     path: /etc/nginx
#     recurse: yes
#     owner: nginx

# # [warn] ulimit is currently set to "1024". For better performance set it to at least
# # "4096" using "ulimit -n"
# - name: nginx - bump up ulimit
#   pam_limits:
#     limit_item: nofile
#     limit_type: soft
#     domain: nginx
#     value: "4096"

# - name: nginx - create service file
#   template:
#     src: files/nginx.service.j2
#     dest: /etc/systemd/system/nginx.service

# # Keep it dormant for the timebeing

# # - name: nginx - reload systemd
# #   systemd:
# #     daemon_reload: yes

'''
'''--- /postgres/ansible/tasks/test-image.yml ---
- name: install pg_prove
  apt:
    pkg:
      - libtap-parser-sourcehandler-pgtap-perl
  when: debpkg_mode

# - name: Temporarily disable PG Sodium references in config
#   become: yes
#   become_user: postgres
#   shell:
#     cmd: sed -i.bak -e "s/pg_net,\ pgsodium,\ timescaledb/pg_net,\ timescaledb/g" -e "s/pgsodium.getkey_script=/#pgsodium.getkey_script=/g" /etc/postgresql/postgresql.conf
#   when: debpkg_mode or stage2_nix

- name: Temporarily disable PG Sodium references in config
  become: yes
  become_user: postgres
  shell:
    cmd: >
      sed -i.bak
      -e 's/\(shared_preload_libraries = '\''.*\)pgsodium,\(.*'\''\)/\1\2/'
      -e 's/pgsodium.getkey_script=/#pgsodium.getkey_script=/'
      /etc/postgresql/postgresql.conf
  when: debpkg_mode or stage2_nix

- name: Start Postgres Database to load all extensions.
  become: yes
  become_user: postgres
  shell:
    cmd: /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data start "-o -c config_file=/etc/postgresql/postgresql.conf"
  when: debpkg_mode

- name: Check if PostgreSQL PID file exists
  stat:
    path: /var/lib/postgresql/data/postmaster.pid
  register: pg_pid_file
  when: stage2_nix

- name: Stop Postgres Database in stage 2
  become: yes
  become_user: postgres
  shell: |
    source /var/lib/postgresql/.bashrc && /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data stop
  args:
    executable: /bin/bash
  environment:
    LANG: en_US.UTF-8
    LANGUAGE: en_US:en
    LC_ALL: en_US.UTF-8
    LC_CTYPE: en_US.UTF-8
    LOCALE_ARCHIVE: /usr/lib/locale/locale-archive
  when: stage2_nix and pg_pid_file.stat.exists

- name: Check logging.conf existence
  stat:
    path: /etc/postgresql/logging.conf
  register: logging_conf
- debug:
    var: logging_conf.stat.exists

- name: Ensure logging configuration file exists at /etc/postgresql/logging.conf
  copy:
    src: files/postgresql_config/postgresql-csvlog.conf
    dest: /etc/postgresql/logging.conf
    owner: postgres
    group: postgres
    mode: '0644'
  when: debpkg_mode or stage2_nix


- name: Start Postgres Database to load all extensions.
  become: yes
  become_user: postgres
  shell: source /var/lib/postgresql/.bashrc &&  /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data start "-o -c config_file=/etc/postgresql/postgresql.conf"
  args:
    executable: /bin/bash
  environment:
    LANG: en_US.UTF-8
    LANGUAGE: en_US.UTF-8
    LC_ALL: en_US.UTF-8
    LC_CTYPE: en_US.UTF-8
    LOCALE_ARCHIVE: /usr/lib/locale/locale-archive
  when: stage2_nix


- name: Check psql_version and modify migrations if oriole-xx
  block:
    - name: Check if psql_version is psql_orioledb-xx
      set_fact:
        is_psql_oriole: "{{ psql_version in ['psql_orioledb-16', 'psql_orioledb-17'] }}"

    - name: Remove specified extensions from SQL file
      ansible.builtin.command:
        cmd: >
          sed -i '/\\ir.*\(timescaledb\|postgis\|pgrouting\|plv8\).*\.sql/d' /tmp/migrations/tests/extensions/test.sql
      when: is_psql_oriole
      become: yes

    - name: Remove specified extension files from extensions directory
      ansible.builtin.find:
        paths: /tmp/migrations/tests/extensions
        patterns: 
          - '*timescaledb*.sql'
          - '*plv8*.sql'
          - '*postgis*.sql'
          - '*pgrouting*.sql'
      register: files_to_remove
      when: is_psql_oriole

    - name: Delete matched extension files
      ansible.builtin.file:
        path: "{{ item.path }}"
        state: absent
      loop: "{{ files_to_remove.files }}"
      when: is_psql_oriole
      become: yes

- name: Run Unit tests (with filename unit-test-*) on Postgres Database
  shell: /usr/bin/pg_prove -U postgres -h localhost -d postgres -v /tmp/unit-tests/unit-test-*.sql
  register: retval
  failed_when: retval.rc != 0
  when: debpkg_mode or stage2_nix

- name: Run migrations tests
  shell: /usr/bin/pg_prove -U supabase_admin -h localhost -d postgres -v tests/test.sql
  register: retval
  failed_when: retval.rc != 0
  when: debpkg_mode or stage2_nix
  args:
    chdir: /tmp/migrations

- name: Re-enable PG Sodium references in config
  become: yes
  become_user: postgres
  shell:
    cmd: mv /etc/postgresql/postgresql.conf.bak /etc/postgresql/postgresql.conf
  when: debpkg_mode or stage2_nix

- name: Reset db stats
  shell: /usr/lib/postgresql/bin/psql --no-password --no-psqlrc -d postgres -h localhost -U supabase_admin -c 'SELECT pg_stat_statements_reset(); SELECT pg_stat_reset();'
  when: debpkg_mode or stage2_nix

- name: remove pg_prove
  apt:
    pkg:
      - libtap-parser-sourcehandler-pgtap-perl
    state: absent
    autoremove: yes
  when: debpkg_mode

- name: Stop Postgres Database
  become: yes
  become_user: postgres
  shell:
    cmd: /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data stop
  when: debpkg_mode or stage2_nix

'''
'''--- /postgres/ansible/tasks/finalize-ami.yml ---
- name: PG logging conf
  template:
    src: files/postgresql_config/postgresql-csvlog.conf
    dest: /etc/postgresql/logging.conf
    group: postgres

- name: UFW - Allow SSH connections
  ufw:
    rule: allow
    name: OpenSSH

- name: UFW - Allow connections to postgreSQL (5432)
  ufw:
    rule: allow
    port: "5432"

- name: UFW - Allow connections to postgreSQL (6543)
  ufw:
    rule: allow
    port: "6543"
  tags:
    - install-pgbouncer

- name: UFW - Allow connections to http (80)
  ufw:
    rule: allow
    port: http
  tags:
  - install-supabase-internal 

- name: UFW - Allow connections to https (443)
  ufw:
    rule: allow
    port: https
  tags:
  - install-supabase-internal 

- name: UFW - Deny all other incoming traffic by default
  ufw:
    state: enabled
    policy: deny
    direction: incoming

- name: Move logrotate files to /etc/logrotate.d/
  copy:
    src: "files/logrotate_config/{{ item.file }}"
    dest: "/etc/logrotate.d/{{ item.file }}"
    mode: "0700"
    owner: root
  loop:
    - { file: "logrotate-postgres-csv.conf" }
    - { file: "logrotate-postgres.conf" }
    - { file: "logrotate-walg.conf" }
    - { file: "logrotate-postgres-auth.conf" }

- name: Ensure default Postgres logrotate config is removed
  file:
    path: /etc/logrotate.d/postgresql-common
    state: absent

- name: Disable cron access
  copy:
    src: files/cron.deny
    dest: /etc/cron.deny

- name: Configure logrotation to run every hour
  shell:
    cmd: |
        cp  /usr/lib/systemd/system/logrotate.timer /etc/systemd/system/logrotate.timer
        sed -i -e 's;daily;*:0/5;' /etc/systemd/system/logrotate.timer
        systemctl reenable logrotate.timer
  become: yes

- name: import pgsodium_getkey script
  template:
    src: files/pgsodium_getkey_readonly.sh.j2
    dest: "{{ pg_bindir }}/pgsodium_getkey.sh"
    owner: postgres
    group: postgres
    mode: 0700
  when: debpkg_mode or stage2_nix

'''
'''--- /postgres/ansible/tasks/setup-wal-g.yml ---
# Dependencies
- name: Install git and WAL-G dependencies
  apt:
    pkg:
      - git
      - libbrotli-dev
      - liblzo2-dev
      - libsodium-dev
      - cmake
      - pkg-config  # Add this
      - build-essential
      - libsodium23  # Add this
    state: present
  when: stage2_nix

# Go installation
- name: Install Go for WAL-G
  block:
    - name: Download Go
      get_url:
        url: "https://golang.org/dl/go{{ golang_version }}.linux-{{ platform }}.tar.gz"
        dest: /tmp
        checksum: "{{ golang_version_checksum[platform] }}"
        timeout: 60

    - name: Unpack Go archive
      unarchive:
        remote_src: yes
        src: "/tmp/go{{ golang_version }}.linux-{{ platform }}.tar.gz"
        dest: /usr/local
  when: stage2_nix

- name: Verify Go installation
  shell: |
    export PATH=$PATH:/usr/local/go/bin
    go version
  register: go_check
  failed_when: go_check.rc != 0
  when: stage2_nix

- name: Show Go version
  debug:
    var: go_check.stdout_lines
  when: stage2_nix


- name: Build WAL-G
  block:
    - name: Clean build directory
      file:
        path: /tmp/wal-g
        state: absent

    - name: Clone WAL-G
      git:
        repo: https://github.com/wal-g/wal-g.git
        dest: /tmp/wal-g
        version: "v{{ wal_g_release }}"
        depth: 1

    - name: Debug directory structure
      shell: |
        echo "=== Directory Structure ==="
        ls -la /tmp/wal-g
        echo "=== Main Directory ==="
        ls -la /tmp/wal-g/main
        echo "=== CMD Directory ==="
        ls -la /tmp/wal-g/cmd
      register: dir_check
      ignore_errors: yes

    - name: Build WAL-G
      shell: |
        cd /tmp/wal-g
        export PKG_CONFIG_PATH="/usr/lib/pkgconfig"
        export CGO_ENABLED=1
        export USE_LIBSODIUM=true
        export PATH=$PATH:/usr/local/go/bin
        export GO111MODULE=on

        # Initialize modules
        go mod init github.com/wal-g/wal-g
        go mod tidy
        
        go build -tags postgresql \
          -ldflags "-X main.buildDate=`date -u +%Y.%m.%d_%H:%M:%S` -X main.gitRevision=`git rev-parse --short HEAD`" \
          -o /usr/local/bin/wal-g \
          ./cmd/pg

      environment:
        GOBIN: "/usr/local/bin"
        CGO_ENABLED: "1"
        USE_LIBSODIUM: "true"
        GO111MODULE: "on"
      register: build_result

    - name: Show build output
      debug:
        var: build_result

    - name: Verify WAL-G installation
      shell: |
        which wal-g || echo "wal-g not found"
        if [ -f /usr/local/bin/wal-g ]; then
          echo "wal-g binary exists"
          ls -l /usr/local/bin/wal-g
        fi
      register: verify_result
      changed_when: false

  when: stage2_nix
  rescue:
    - name: Show detailed error information
      debug:
        msg: 
          - "Build failed with following details:"
          - "Return code: {{ build_result.rc | default('unknown') }}"
          - "Stdout: {{ build_result.stdout | default('') }}"
          - "Stderr: {{ build_result.stderr | default('') }}"
      when: build_result is defined

    - name: Check Go environment
      shell: |
        echo "=== Go Environment ==="
        go env
        echo "=== Go Version ==="
        go version
      register: go_env
      ignore_errors: yes

    - name: Show Go environment
      debug:
        var: go_env
      when: go_env is defined

    - fail:
        msg: "WAL-G build failed. See above logs for details."


- name: Verify libsodium installation
  shell: |
    pkg-config --libs libsodium
    ldconfig -p | grep libsodium
  register: libsodium_check
  changed_when: false
  when: stage2_nix

- name: Show libsodium status
  debug:
    var: libsodium_check.stdout_lines
  when: stage2_nix

# User and group setup
- name: WAL-G user and group setup
  block:
    - name: Create WAL-G group
      group:
        name: wal-g
        state: present
        system: yes

    - name: Create WAL-G user
      user:
        name: wal-g
        shell: /bin/false
        comment: WAL-G user
        group: wal-g
        system: yes

    - name: Add WAL-G to postgres group
      user:
        name: wal-g
        groups: postgres
        append: yes

# Configuration setup
- name: WAL-G configuration
  block:
    - name: Create WAL-G directories
      file:
        path: "{{ item.path }}"
        state: "{{ item.state }}"
        owner: "{{ item.owner }}"
        group: "{{ item.group }}"
        mode: "{{ item.mode }}"
      loop:
        - { path: '/etc/wal-g', state: 'directory', owner: 'wal-g', group: 'wal-g', mode: '0770' }
        - { path: '/etc/wal-g/config.json', state: 'touch', owner: 'wal-g', group: 'wal-g', mode: '0664' }

    - name: Configure WAL-G
      template:
        src: "{{ item.src }}"
        dest: "{{ item.dest }}"
        mode: "{{ item.mode }}"
        owner: "{{ item.owner }}"
        group: "{{ item.group }}"
      loop:
        - { src: 'files/postgresql_config/custom_walg.conf.j2', dest: '/etc/postgresql-custom/wal-g.conf', mode: '0664', owner: 'postgres', group: 'postgres' }
        - { src: 'files/walg_helper_scripts/wal_fetch.sh', dest: '/home/postgres/wal_fetch.sh', mode: '0500', owner: 'postgres', group: 'postgres' }
        - { src: 'files/walg_helper_scripts/wal_change_ownership.sh', dest: '/root/wal_change_ownership.sh', mode: '0700', owner: 'root', group: 'root' }
  when: stage2_nix

# Configuration updates
- name: Update PostgreSQL configuration
  replace:
    path: /etc/postgresql/postgresql.conf
    regexp: "#include = '/etc/postgresql-custom/wal-g.conf'"
    replace: "include = '/etc/postgresql-custom/wal-g.conf'"
  when: stage2_nix

# Cleanup
- name: Cleanup Go installation
  file:
    path: /usr/local/go
    state: absent
  when: stage2_nix

# # Downloading dependencies
# - name: wal-g dependencies
#   become: yes
#   apt:
#     pkg:
#       - libbrotli-dev
#       - liblzo2-dev
#       - libsodium-dev
#       - cmake

# # install go dependency for WAL-G
# - name: wal-g go dependency
#   get_url:
#     url: "https://golang.org/dl/go{{ golang_version }}.linux-{{ platform }}.tar.gz"
#     dest: /tmp
#     checksum: "{{ golang_version_checksum[platform] }}"
#     timeout: 60

# - name: unpack go archive
#   unarchive:
#     remote_src: yes
#     src: "/tmp/go{{ golang_version }}.linux-{{ platform }}.tar.gz"
#     dest: /usr/local

# # Download WAL-G
# - name: wal-g - download latest version
#   git:
#     repo: https://github.com/wal-g/wal-g.git
#     dest: /tmp/wal-g
#     version: "v{{ wal_g_release }}"
#   become: yes

# - name: wal-g - pg_clean
#   make:
#     chdir: /tmp/wal-g
#     target: pg_clean
#     params:
#       GOBIN: "/usr/local/bin"
#       PATH: "{{ ansible_env.PATH }}:/usr/local/go/bin"
#       USE_LIBSODIUM: true
#   become: yes
#   ignore_errors: yes

# - name: wal-g - deps
#   make:
#     chdir: /tmp/wal-g
#     target: deps
#     params:
#       GOBIN: "/usr/local/bin"
#       PATH: "{{ ansible_env.PATH }}:/usr/local/go/bin"
#       USE_LIBSODIUM: true
#   become: yes
#   ignore_errors: yes

# - name: wal-g - build and install
#   community.general.make:
#     chdir: /tmp/wal-g
#     target: pg_install
#     jobs: "{{ parallel_jobs | default(omit) }}"
#     params:
#       GOBIN: "/usr/local/bin"
#       PATH: "{{ ansible_env.PATH }}:/usr/local/go/bin"
#       USE_LIBSODIUM: true
#   become: yes

# - name: Create wal-g group
#   group:
#     name: wal-g
#     state: present

# - name: Create wal-g user
#   user:
#     name: wal-g
#     shell: /bin/false
#     comment: WAL-G user
#     group: wal-g
#     groups: wal-g, postgres

# - name: Create a config directory owned by wal-g
#   file:
#     path: /etc/wal-g
#     state: directory
#     owner: wal-g
#     group: wal-g
#     mode: '0770'

# - name: Create /etc/wal-g/config.json
#   file:
#     path: /etc/wal-g/config.json
#     state: touch
#     owner: wal-g
#     group: wal-g
#     mode: '0664'

# - name: Move custom wal-g.conf file to /etc/postgresql-custom/wal-g.conf
#   template:
#     src: "files/postgresql_config/custom_walg.conf.j2"
#     dest: /etc/postgresql-custom/wal-g.conf
#     mode: 0664
#     owner: postgres
#     group: postgres

# - name: Add script to be run for restore_command
#   template:
#     src: "files/walg_helper_scripts/wal_fetch.sh"
#     dest: /home/postgres/wal_fetch.sh
#     mode: 0500
#     owner: postgres
#     group: postgres

# - name: Add helper script for wal_fetch.sh
#   template:
#     src: "files/walg_helper_scripts/wal_change_ownership.sh"
#     dest: /root/wal_change_ownership.sh
#     mode: 0700
#     owner: root

# - name: Include /etc/postgresql-custom/wal-g.conf in postgresql.conf
#   become: yes
#   replace:
#     path: /etc/postgresql/postgresql.conf
#     regexp: "#include = '/etc/postgresql-custom/wal-g.conf'"
#     replace: "include = '/etc/postgresql-custom/wal-g.conf'"

# # Clean up Go
# - name: Uninstall Go
#   become: yes
#   file:
#     path: /usr/local/go
#     state: absent

'''
'''--- /postgres/ansible/tasks/stage2-setup-postgres.yml ---
# - name: Install openjdk11 for pljava from nix binary cache
#   become: yes
#   shell: |
#     sudo -u postgres bash -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && nix profile install nixpkgs#openjdk11"
# It was decided to leave pljava disabled at https://github.com/supabase/postgres/pull/690 therefore removing this task

- name: Check psql_version and modify supautils.conf and postgresql.conf if necessary
  block:
    - name: Check if psql_version is psql_orioledb-16
      set_fact:
        is_psql_oriole: "{{ psql_version in ['psql_orioledb-16', 'psql_orioledb-17'] }}"

    - name: Remove specified extensions from postgresql.conf if oriole-16 build
      ansible.builtin.command:
        cmd: >
          sed -i 's/ timescaledb,//g' 
          /etc/postgresql/postgresql.conf
      when: is_psql_oriole and stage2_nix
      become: yes

    - name: Remove specified extensions from supautils.conf if oriole-16 build
      ansible.builtin.command:
        cmd: >
          sed -i 's/ timescaledb,//g; s/ vector,//g; s/ plv8,//g; s/ postgis,//g; s/ pgrouting,//g' 
          /etc/postgresql-custom/supautils.conf
      when: is_psql_oriole and stage2_nix
      become: yes

    - name: Remove db_user_namespace from postgresql.conf if oriole-xx build
      ansible.builtin.command:
        cmd: >
          sed -i 's/db_user_namespace = off/#db_user_namespace = off/g;' 
          /etc/postgresql/postgresql.conf
      when: is_psql_oriole and stage2_nix
      become: yes

    - name: Append orioledb to shared_preload_libraries append within closing quote
      ansible.builtin.command:
        cmd: >
          sed -i 's/\(shared_preload_libraries.*\)'\''\(.*\)$/\1, orioledb'\''\2/'
          /etc/postgresql/postgresql.conf
      when: is_psql_oriole and stage2_nix
      become: yes

    - name: Add default_table_access_method setting
      ansible.builtin.lineinfile:
        path: /etc/postgresql/postgresql.conf
        line: "default_table_access_method = 'orioledb'"
        state: present
      when: is_psql_oriole and stage2_nix
      become: yes
    
    - name: Add ORIOLEDB_ENABLED environment variable
      ansible.builtin.lineinfile:
        path: /etc/environment
        line: 'ORIOLEDB_ENABLED=true'
      when: is_psql_oriole and stage2_nix
      become: yes

- name: Install Postgres from nix binary cache
  become: yes
  shell: |
    sudo -u postgres bash -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && nix profile install github:supabase/postgres/{{ git_commit_sha }}#{{psql_version}}/bin"
  when: stage2_nix

- name: Install pg_prove from nix binary cache
  become: yes
  shell: |
    sudo -u postgres bash -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && nix profile install github:supabase/postgres/{{ git_commit_sha }}#pg_prove"
  when: stage2_nix

- name: Install supabase-groonga from nix binary cache
  become: yes
  shell: |
    sudo -u postgres bash -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && nix profile install github:supabase/postgres/{{ git_commit_sha }}#supabase-groonga"
  when: stage2_nix

- name: Install debug symbols for postgres version
  become: yes
  shell: |
    sudo -u postgres bash -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && nix profile install github:supabase/postgres/{{ git_commit_sha }}#{{postgresql_version}}_debug"
  when: stage2_nix

- name: Install source files for postgresql version
  become: yes
  shell: |
    sudo -u postgres bash -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && nix profile install github:supabase/postgres/{{ git_commit_sha }}#{{postgresql_version}}_src"
  when: stage2_nix
  
- name: Set ownership and permissions for /etc/ssl/private
  become: yes
  file:
    path: /etc/ssl/private
    owner: root
    group: postgres
    mode: '0750'
  when: stage2_nix

- name: Set permissions for postgresql.env
  become: yes
  file:
    path: /etc/environment.d/postgresql.env
    owner: postgres
    group: postgres
    mode: '0644'
  when: stage2_nix

- name: Ensure /usr/lib/postgresql/bin directory exists
  file:
    path: /usr/lib/postgresql/bin
    state: directory
    owner: postgres
    group: postgres
  when: stage2_nix

- name: Ensure /usr/lib/postgresql/share directory exists
  file:
    path: /usr/lib/postgresql/share/postgresql
    state: directory
    owner: postgres
    group: postgres
  when: stage2_nix

- name: Ensure /usr/lib/postgresql/share/contrib directory exists
  file:
    path: /usr/lib/postgresql/share/postgresql/contrib
    state: directory
    owner: postgres
    group: postgres
  when: stage2_nix

- name: Ensure /usr/lib/postgresql/share/timezonesets directory exists
  file:
    path: /usr/lib/postgresql/share/postgresql/timezonesets
    state: directory
    owner: postgres
    group: postgres
  when: stage2_nix

- name: Ensure /usr/lib/postgresql/share/tsearch_data directory exists
  file:
    path: /usr/lib/postgresql/share/postgresql/tsearch_data
    state: directory
    owner: postgres
    group: postgres
  when: stage2_nix

- name: Ensure /usr/lib/postgresql/share/extension directory exists
  file:
    path: /usr/lib/postgresql/share/postgresql/extension
    state: directory
    owner: postgres
    group: postgres
  when: stage2_nix

# - name: Ensure /usr/lib/postgresql/share/postgresql/pljava directory exists
#   file:
#     path: /usr/lib/postgresql/share/postgresql/pljava
#     state: directory
#     owner: postgres
#     group: postgres
#   when: stage2_nix
# It was decided to leave pljava disabled at https://github.com/supabase/postgres/pull/690 therefore removing this task

- name: import pgsodium_getkey script
  template:
    src: /tmp/ansible-playbook/ansible/files/pgsodium_getkey_readonly.sh.j2
    dest: "/usr/lib/postgresql/bin/pgsodium_getkey.sh"
    owner: postgres
    group: postgres
    mode: 0700
  when: stage2_nix

- name: Create symbolic links from /var/lib/postgresql/.nix-profile/bin to /usr/lib/postgresql/bin
  file:
    src: "{{ item }}"
    dest: "/usr/lib/postgresql/bin/{{ item | basename }}"
    state: link
  with_fileglob:
    - "/var/lib/postgresql/.nix-profile/bin/*"
  become: yes
  when: stage2_nix

- name: Check if /usr/bin/pg_config exists
  stat:
    path: /usr/bin/pg_config
  register: pg_config_stat
  when: stage2_nix

- name: Remove existing /usr/bin/pg_config if it is not a symlink
  file:
    path: /usr/bin/pg_config
    state: absent
  when: pg_config_stat.stat.exists and not pg_config_stat.stat.islnk and stage2_nix
  become: yes

- name: Create symbolic links from /var/lib/postgresql/.nix-profile/bin to /usr/bin
  file:
    src: "{{ item }}"
    dest: "/usr/bin/{{ item | basename }}"
    state: link
  with_fileglob:
    - "/var/lib/postgresql/.nix-profile/bin/*"
  become: yes
  when: stage2_nix

- name: Ensure postgres user has ownership of symlink
  file:
    path: "/usr/bin/{{ item | basename }}"
    owner: postgres
    group: postgres
  with_fileglob:
    - "/var/lib/postgresql/.nix-profile/bin/*"
  become: yes
  when: stage2_nix

# - name: Create symbolic links from /var/lib/postgresql/.nix-profile/share/pljava to /usr/lib/postgresql/share/postgresql/pljava
#   file:
#     src: "{{ item }}"
#     dest: "/usr/lib/postgresql/share/postgresql/pljava/{{ item | basename }}"
#     state: link
#   with_fileglob:
#     - "/var/lib/postgresql/.nix-profile/share/pljava/*"
#   become: yes
# It was decided to leave pljava disabled at https://github.com/supabase/postgres/pull/690 therefore removing this task

- name: Create symbolic links from /var/lib/postgresql/.nix-profile/share/postgresql to /usr/lib/postgresql/share/postgresql
  file:
    src: "{{ item }}"
    dest: "/usr/lib/postgresql/share/postgresql/{{ item | basename }}"
    state: link
  with_fileglob:
    - "/var/lib/postgresql/.nix-profile/share/postgresql/*"
  become: yes
  when: stage2_nix

- name: Create symbolic links from /var/lib/postgresql/.nix-profile/share/postgresql/extension to /usr/lib/postgresql/share/postgresql/extension
  file:
    src: "{{ item }}"
    dest: "/usr/lib/postgresql/share/postgresql/extension/{{ item | basename }}"
    state: link
  with_fileglob:
    - "/var/lib/postgresql/.nix-profile/share/postgresql/extension/*"
  become: yes
  when: stage2_nix

- name: create destination directory
  file:
    path: /usr/lib/postgresql/share/postgresql/contrib/
    state: directory
    recurse: yes
  when: stage2_nix

- name: Check psql_version and run postgis linking if not oriole-xx
  block:
    - name: Check if psql_version is psql_orioledb-17
      set_fact:
        is_psql_oriole: "{{ psql_version == 'psql_orioledb-17' }}"

    - name: Recursively create symbolic links and set permissions for the contrib/postgis-* dir
      shell: >
        sudo mkdir -p /usr/lib/postgresql/share/postgresql/contrib && \
        sudo find /var/lib/postgresql/.nix-profile/share/postgresql/contrib/ -mindepth 1 -type d -exec sh -c 'for dir do sudo ln -s "$dir" "/usr/lib/postgresql/share/postgresql/contrib/$(basename "$dir")"; done' sh {} + \
        && chown -R postgres:postgres "/usr/lib/postgresql/share/postgresql/contrib/"
      become: yes
      when: stage2_nix and not is_psql_oriole

- name: Create symbolic links from /var/lib/postgresql/.nix-profile/share/postgresql/timezonesets to /usr/lib/postgresql/share/postgresql/timeszonesets
  file:
    src: "{{ item }}"
    dest: "/usr/lib/postgresql/share/postgresql/timezonesets/{{ item | basename }}"
    state: link
  with_fileglob:
    - "/var/lib/postgresql/.nix-profile/share/postgresql/timezonesets/*"
  become: yes
  when: stage2_nix

- name: Create symbolic links from /var/lib/postgresql/.nix-profile/share/postgresql/tsearch_data to /usr/lib/postgresql/share/postgresql/tsearch_data
  file:
    src: "{{ item }}"
    dest: "/usr/lib/postgresql/share/postgresql/tsearch_data/{{ item | basename }}"
    state: link
  with_fileglob:
    - "/var/lib/postgresql/.nix-profile/share/postgresql/tsearch_data/*"
  become: yes
  when: stage2_nix

- set_fact:
    pg_bindir: "/usr/lib/postgresql/bin"
  when: stage2_nix

- name: pgsodium - set pgsodium.getkey_script
  become: yes
  lineinfile:
    path: /etc/postgresql/postgresql.conf
    state: present
    # script is expected to be placed by finalization tasks for different target platforms
    line: pgsodium.getkey_script= '{{ pg_bindir }}/pgsodium_getkey.sh'
  when: stage2_nix

- name: Create symbolic link for pgsodium_getkey script
  file:
    src: "/usr/lib/postgresql/bin/pgsodium_getkey.sh"
    dest: "/usr/lib/postgresql/share/postgresql/extension/pgsodium_getkey"
    state: link
  become: yes
  when: stage2_nix

- name: Append GRN_PLUGINS_DIR to /etc/environment.d/postgresql.env
  ansible.builtin.lineinfile:
    path: /etc/environment.d/postgresql.env
    line: 'GRN_PLUGINS_DIR=/var/lib/postgresql/.nix-profile/lib/groonga/plugins'
  become: yes

'''
'''--- /postgres/ansible/tasks/setup-gotrue.yml ---
# Group creation first
- name: Gotrue - create group
  group:
    name: gotrue
    state: present
    system: yes
  when: stage2_nix

# Then user creation with proper group
- name: Gotrue - system user
  user:
    name: gotrue
    system: yes
    group: gotrue
    shell: /bin/false
    create_home: no
  when: stage2_nix

- name: UFW - Allow connections to GoTrue metrics exporter
  ufw:
    rule: allow
    port: "9122"
  when: stage2_nix

- name: Setting arch (x86)
  set_fact:
    arch: "x86"
  when: platform == "amd64"

- name: Setting arch (arm)
  set_fact:
    arch: "arm64"
  when: platform == "arm64"

- name: gotrue - download commit archive
  get_url:
    url: "https://github.com/supabase/gotrue/releases/download/v{{ gotrue_release }}/auth-v{{ gotrue_release }}-{{ arch }}.tar.gz"
    dest: /tmp/gotrue.tar.gz
    checksum: "{{ gotrue_release_checksum }}"
  when: stage2_nix

- name: gotrue - create /opt/gotrue
  file:
    path: /opt/gotrue
    state: directory
    owner: gotrue
    group: gotrue
    mode: 0775
  when: stage2_nix

- name: gotrue - unpack archive in /opt/gotrue
  unarchive:
    remote_src: yes
    src: /tmp/gotrue.tar.gz
    dest: /opt/gotrue
    owner: gotrue
    group: gotrue
  when: stage2_nix

- name: Verify gotrue user and group setup
  block:
    - name: Check gotrue user and group existence
      shell: |
        echo "=== GoTrue User/Group Verification ==="
        id gotrue
        echo "Group details:"
        getent group gotrue
      register: gotrue_verify
      changed_when: false

    - name: Display verification results
      debug:
        var: gotrue_verify.stdout_lines
  when: stage2_nix

- name: gotrue - create service file
  template:
    src: files/gotrue.service.j2
    dest: /etc/systemd/system/gotrue.service
  when: stage2_nix

- name: gotrue - create optimizations file
  template:
    src: files/gotrue-optimizations.service.j2
    dest: /etc/systemd/system/gotrue-optimizations.service
  when: stage2_nix

- name: gotrue - reload systemd
  systemd:
    daemon_reload: yes
  when: stage2_nix
'''
'''--- /postgres/ansible/tasks/setup-supabase-internal.yml ---
- name: AWS CLI dep
  apt:
    pkg:
      - unzip
      - jq
    install_recommends: no

- name: AWS CLI (arm)
  get_url:
    url: "https://awscli.amazonaws.com/awscli-exe-linux-aarch64-{{ aws_cli_release }}.zip"
    dest: "/tmp/awscliv2.zip"
    timeout: 60
  when: platform == "arm64"

- name: AWS CLI (x86)
  get_url:
    url: "https://awscli.amazonaws.com/awscli-exe-linux-x86_64-{{ aws_cli_release }}.zip"
    dest: "/tmp/awscliv2.zip"
    timeout: 60
  when: platform == "amd64"

- name: AWS CLI - expand
  unarchive:
    remote_src: yes
    src: "/tmp/awscliv2.zip"
    dest: "/tmp"

- name: AWS CLI - install
  shell: "/tmp/aws/install --update"
  become: true

- name: AWS CLI - configure ipv6 support for s3
  shell: |
    aws configure set default.s3.use_dualstack_endpoint true

- name: install Vector for logging
  become: yes
  apt:
    deb: "{{ vector_x86_deb }}"
  when: platform == "amd64"

- name: install Vector for logging
  become: yes
  apt:
    deb: "{{ vector_arm_deb }}"
  when: platform == "arm64"

- name: add Vector to postgres group
  become: yes
  shell:
    cmd: |
      usermod -a -G postgres vector

- name: create service files for Vector
  template:
    src: files/vector.service.j2
    dest: /etc/systemd/system/vector.service

- name: configure tmpfiles for postgres - overwrites upstream package
  template:
    src: files/postgresql_config/tmpfiles.postgresql.conf
    dest: /etc/tmpfiles.d/postgresql-common.conf

- name: fix permissions for vector config to be managed
  shell:
    cmd: |
      chown -R vector:vector /etc/vector
      chmod 0775 /etc/vector

- name: vector - reload systemd
  systemd:
    daemon_reload: yes

- name: Create checkpoints dir
  become: yes
  file:
    path: /var/lib/vector
    state: directory
    owner: vector

- name: Include file for generated optimizations in postgresql.conf
  become: yes
  replace:
    path: /etc/postgresql/postgresql.conf
    regexp: "#include = '/etc/postgresql-custom/generated-optimizations.conf'"
    replace: "include = '/etc/postgresql-custom/generated-optimizations.conf'"

- name: Include file for custom overrides in postgresql.conf
  become: yes
  replace:
    path: /etc/postgresql/postgresql.conf
    regexp: "#include = '/etc/postgresql-custom/custom-overrides.conf'"
    replace: "include = '/etc/postgresql-custom/custom-overrides.conf'"

- name: Install Postgres exporter
  import_tasks: internal/postgres-exporter.yml

- name: Install admin-mgr
  import_tasks: internal/admin-mgr.yml

- name: Install adminapi
  import_tasks: internal/admin-api.yml

- name: Init nftabless
  import_tasks: internal/setup-nftables.yml

- name: Install pg_egress_collect
  import_tasks: internal/pg_egress_collect.yml

- name: Install PostgreSQL prestart script
  import_tasks: internal/postgresql-prestart.yml

- name: Install salt minion
  import_tasks: internal/install-salt.yml
  tags:
    - aws-only

- name: Envoy - use lds.supabase.yaml for /etc/envoy/lds.yaml
  command: mv /etc/envoy/lds.supabase.yaml /etc/envoy/lds.yaml

'''
'''--- /postgres/ansible/tasks/setup-postgres.yml ---
- name: Debug - PostgreSQL pre-setup
  shell: |
    echo "=== System State ==="
    echo "Groups:"
    getent group postgres ssl-cert || echo "Groups not found"
    echo "====="
  register: pre_postgres_debug
  changed_when: false

- name: Show PostgreSQL pre-setup debug
  debug:
    var: pre_postgres_debug.stdout_lines

- name: Postgres - copy package
  copy:
    src: files/postgres/
    dest: /tmp/build/
  when: debpkg_mode

- name: Postgres - add PPA
  apt_repository:
    repo: "deb [ trusted=yes ] file:///tmp/build ./"
    state: present
  when: debpkg_mode

- name: Postgres - install commons
  apt:
    name: postgresql-common
    install_recommends: no
  when: debpkg_mode

- name: Do not create main cluster
  shell:
    cmd: sed -ri 's/#(create_main_cluster) .*$/\1 = false/' /etc/postgresql-common/createcluster.conf
  when: debpkg_mode

- name: Postgres - install server
  apt:
    name: postgresql-{{ postgresql_major }}={{ postgresql_release }}-1.pgdg20.04+1
    install_recommends: no
  when: debpkg_mode

- name: Postgres - remove PPA
  apt_repository:
    repo: "deb [ trusted=yes ] file:///tmp/build ./"
    state: absent
  when: debpkg_mode

- name: Postgres - cleanup package
  file:
    path: /tmp/build
    state: absent
  when: debpkg_mode

- name: install locales
  apt:
    name: locales
    state: present
  become: yes
  when: stage2_nix

- name: configure locales
  command: echo "C.UTF-8 UTF-8" > /etc/locale.gen && echo "en_US.UTF-8 UTF-8" >> /etc/locale.gen
  become: yes
  when: stage2_nix

- name: locale-gen
  command: sudo locale-gen
  when: stage2_nix

- name: update-locale
  command: sudo update-locale
  when: stage2_nix

- name: Create symlink to /usr/lib/postgresql/bin
  shell:
    cmd: ln -s /usr/lib/postgresql/{{ postgresql_major }}/bin /usr/lib/postgresql/bin
  when: debpkg_mode

# - name: create ssl-cert group
#   group:
#     name: ssl-cert
#     state: present
#   when: nixpkg_mode

# - name: create postgres group
#   group:
#     name: postgres
#     state: present
#   when: nixpkg_mode

# - name: create postgres user
#   shell: adduser --system  --home /var/lib/postgresql --no-create-home --shell /bin/bash --group --gecos "PostgreSQL administrator" postgres
#   args:
#     executable: /bin/bash
#   become: yes
#   when: nixpkg_mode

# - name: add postgres user to postgres group
#   shell: usermod -a -G ssl-cert postgres
#   args:
#     executable: /bin/bash
#   become: yes
#   when: nixpkg_mode

- name: create ssl-cert group
  group:
    name: ssl-cert
    state: present
    gid: 1001
  when: nixpkg_mode

- name: create postgres group
  group:
    name: postgres
    state: present
    gid: 1002
  when: nixpkg_mode

- name: Create postgres user and set primary group
  user:
    name: postgres
    system: yes
    home: /var/lib/postgresql
    shell: /bin/bash
    group: postgres
    groups: []
  when: nixpkg_mode

- name: Add postgres to additional groups
  user:
    name: postgres
    group: postgres
    groups: ssl-cert
    append: yes
  when: nixpkg_mode

- name: Verify postgres user groups
  shell: |
    echo "=== Verifying postgres user groups ==="
    id postgres
    echo "Group memberships:"
    getent group postgres
    getent group ssl-cert
  register: verify_postgres
  changed_when: false
  when: nixpkg_mode

- name: Show verification results
  debug:
    var: verify_postgres.stdout_lines
  when: nixpkg_mode

- name: Force system to recognize group changes
  shell: |
    # Reload system group cache
    systemctl daemon-reload
    # Force group membership update
    pkill -SIGHUP -u postgres || true
  changed_when: false
  when: nixpkg_mode

- name: Create relevant directories
  file:
    path: '{{ item }}'
    recurse: yes
    state: directory
    owner: postgres
    group: postgres
  with_items:
    - '/home/postgres'
    - '/var/log/postgresql'
    - '/var/lib/postgresql'
  when: debpkg_mode or nixpkg_mode

- name: Allow adminapi to write custom config
  file:
    path: '{{ item }}'
    recurse: yes
    state: directory
    owner: postgres
    group: postgres
    mode: 0775
  with_items:
    - '/etc/postgresql'
    - '/etc/postgresql-custom'
  when: debpkg_mode or nixpkg_mode

- name: create placeholder config files
  file:
    path: '/etc/postgresql-custom/{{ item }}'
    state: touch
    owner: postgres
    group: postgres
    mode: 0664
  with_items:
    - 'generated-optimizations.conf'
    - 'custom-overrides.conf'
  when: debpkg_mode or nixpkg_mode

# Move Postgres configuration files into /etc/postgresql
# Add postgresql.conf
- name: import postgresql.conf
  template:
    src: files/postgresql_config/postgresql.conf.j2
    dest: /etc/postgresql/postgresql.conf
    group: postgres
  when: debpkg_mode or nixpkg_mode

# Add pg_hba.conf
- name: import pg_hba.conf
  template:
    src: files/postgresql_config/pg_hba.conf.j2
    dest: /etc/postgresql/pg_hba.conf
    group: postgres
  when: debpkg_mode or nixpkg_mode

# Add pg_ident.conf
- name: import pg_ident.conf
  template:
    src: files/postgresql_config/pg_ident.conf.j2
    dest: /etc/postgresql/pg_ident.conf
    group: postgres
  when: debpkg_mode or nixpkg_mode

# Add custom config for read replicas set up
- name: Move custom read-replica.conf file to /etc/postgresql-custom/read-replica.conf
  template:
    src: "files/postgresql_config/custom_read_replica.conf.j2"
    dest: /etc/postgresql-custom/read-replica.conf
    mode: 0664
    owner: postgres
    group: postgres
  when: debpkg_mode or nixpkg_mode

# Install extensions before init
- name: Install Postgres extensions
  import_tasks: tasks/setup-docker.yml
  when: debpkg_mode or stage2_nix

#stage 2 postgres tasks
- name: stage2 postgres tasks
  import_tasks: tasks/stage2-setup-postgres.yml
  when: stage2_nix

# init DB
- name: Create directory on data volume
  file:
    path: '{{ item }}'
    recurse: yes
    state: directory
    owner: postgres
    group: postgres
    mode: 0750
  with_items:
    - "/data/pgdata"
  when: debpkg_mode or nixpkg_mode

- name: Link database data_dir to data volume directory
  file:
    src: "/data/pgdata"
    path: "/var/lib/postgresql/data"
    state: link
    force: yes
  when: debpkg_mode or nixpkg_mode

- name: Initialize the database
  become: yes
  become_user: postgres
  shell: /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data initdb -o "--allow-group-access" -o "--username=supabase_admin"
  vars:
    ansible_command_timeout: 60
  when: debpkg_mode

- name: Check psql_version and modify supautils.conf and postgresql.conf if necessary
  block:
    - name: Check if psql_version is psql_orioledb
      set_fact:
        is_psql_oriole: "{{ psql_version in ['psql_orioledb-16', 'psql_orioledb-17'] }}"

    - name: Initialize the database stage2_nix (non-orioledb)
      become: yes
      become_user: postgres
      shell: source /var/lib/postgresql/.bashrc && /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data initdb -o "--allow-group-access" -o "--username=supabase_admin"
      args:
        executable: /bin/bash
      environment:
        LANG: en_US.UTF-8
        LANGUAGE: en_US.UTF-8
        LC_ALL: en_US.UTF-8
        LC_CTYPE: en_US.UTF-8
        LOCALE_ARCHIVE: /usr/lib/locale/locale-archive
      vars:
        ansible_command_timeout: 60
      when: stage2_nix and not is_psql_oriole

    - name: Initialize the database stage2_nix (orioledb)
      become: yes
      become_user: postgres
      shell: >
        source /var/lib/postgresql/.bashrc && initdb -D /var/lib/postgresql/data 
        --allow-group-access 
        --username=supabase_admin 
        --locale-provider=icu 
        --encoding=UTF-8 
        --icu-locale=en_US.UTF-8 
      args:
        executable: /bin/bash
      environment:
        LANG: en_US.UTF-8
        LANGUAGE: en_US.UTF-8
        LC_ALL: en_US.UTF-8
        LC_CTYPE: en_US.UTF-8
        LOCALE_ARCHIVE: /usr/lib/locale/locale-archive
      vars:
        ansible_command_timeout: 60
      when: stage2_nix and is_psql_oriole

- name: copy PG systemd unit
  template:
    src: files/postgresql_config/postgresql.service.j2
    dest: /etc/systemd/system/postgresql.service
  when: debpkg_mode or stage2_nix

- name: copy optimizations systemd unit
  template:
    src: files/database-optimizations.service.j2
    dest: /etc/systemd/system/database-optimizations.service
  when: debpkg_mode or stage2_nix

- name: Ensure /run/postgresql exists for lock file creation
  become: yes
  file:
    path: /run/postgresql
    state: directory
    owner: postgres
    group: postgres
    mode: '2775'
  when: stage2_nix

- name: Check if PostgreSQL PID file exists
  stat:
    path: /var/lib/postgresql/data/postmaster.pid
  register: pg_pid_file
  when: stage2_nix

- name: Stop Postgres Database without Systemd (force shutdown)
  become: yes
  become_user: postgres
  shell: /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data stop -m immediate
  args:
    executable: /bin/bash
  when: stage2_nix and pg_pid_file.stat.exists

- name: Restart Postgres Database without Systemd
  become: yes
  become_user: postgres
  ansible.builtin.shell: |
    # Export environment variables inline
    export LANG=en_US.UTF-8
    export LANGUAGE=en_US:en
    export LC_ALL=en_US.UTF-8
    export LC_CTYPE=en_US.UTF-8
    export LOCALE_ARCHIVE=/usr/lib/locale/locale-archive
    # Use the POSIX . operator instead of source
    . /var/lib/postgresql/.bashrc
    /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data start
  args:
    executable: /bin/bash
  when: stage2_nix


# Reload
- name: System - systemd reload
  systemd:
    enabled: yes
    name: postgresql
    daemon_reload: yes
  when: debpkg_mode or stage2_nix

- name: Make sure .bashrc exists
  file: 
    path: /var/lib/postgresql/.bashrc 
    state: touch
    owner: postgres
    group: postgres
  when: nixpkg_mode 

- name: Add LOCALE_ARCHIVE to .bashrc
  lineinfile:
    dest: "/var/lib/postgresql/.bashrc"
    line: 'export LOCALE_ARCHIVE=/usr/lib/locale/locale-archive'
    create: yes
  become: yes
  when: nixpkg_mode

- name: Add LANG items to .bashrc
  lineinfile:
    dest: "/var/lib/postgresql/.bashrc"
    line: "{{ item }}"
  loop: 
    - 'export LANG="en_US.UTF-8"'
    - 'export LANGUAGE="en_US.UTF-8"'
    - 'export LC_ALL="en_US.UTF-8"'
    - 'export LANG="en_US.UTF-8"'
    - 'export LC_CTYPE="en_US.UTF-8"'
  become: yes
  when: nixpkg_mode

'''
'''--- /postgres/ansible/tasks/clean-build-dependencies.yml ---
- name: Remove build dependencies
  apt:
    pkg:
      - bison
      - build-essential
      - clang-11
      - cmake
      - cpp
      - flex
      - g++
      - g++-10
      - g++-9
      - gcc-10
      - make
      - manpages
      - manpages-dev
      - ninja-build
      - patch
      - python2
    state: absent
    autoremove: yes

'''
'''--- /postgres/ansible/tasks/internal/supautils.yml ---
# # supautils
# - name: supautils - download & install dependencies
#   apt:
#     pkg:
#       - build-essential
#       - clang-11
#     update_cache: yes
#     cache_valid_time: 3600
#   when: stage2_nix

# - name: supautils - download source
#   get_url:
#     url: "https://github.com/supabase/supautils/archive/refs/tags/v2.6.0.tar.gz"
#     dest: /tmp/supautils-2.6.0.tar.gz
#     timeout: 60
#   when: stage2_nix

# - name: supautils - unpack archive
#   unarchive:
#     remote_src: yes
#     src: /tmp/supautils-2.6.0.tar.gz
#     dest: /tmp
#   become: yes
#   when: stage2_nix

# - name: supautils - build
#   make:
#     chdir: /tmp/supautils-2.6.0
#   become: yes
#   when: stage2_nix

# - name: supautils - install
#   make:
#     chdir: /tmp/supautils-2.6.0
#     target: install
#   become: yes
#   when: stage2_nix

# - name: supautils - add supautils to session_preload_libraries
#   become: yes
#   replace:
#     path: /etc/postgresql/postgresql.conf
#     regexp: "#session_preload_libraries = ''"
#     replace: session_preload_libraries = 'supautils'
#   when: stage2_nix

# - name: supautils - write custom supautils.conf
#   template:
#     src: "files/postgresql_config/supautils.conf.j2"
#     dest: /etc/postgresql-custom/supautils.conf
#     mode: 0664
#     owner: postgres
#     group: postgres
#   when: stage2_nix

# - name: supautils - copy extension custom scripts
#   copy:
#     src: files/postgresql_extension_custom_scripts/
#     dest: /etc/postgresql-custom/extension-custom-scripts
#     owner: postgres
#     group: postgres
#     mode: '0775'
#   become: yes
#   when: stage2_nix

# - name: supautils - chown extension custom scripts
#   file:
#     mode: 0775
#     owner: postgres
#     group: postgres
#     path: /etc/postgresql-custom/extension-custom-scripts
#     recurse: yes
#   become: yes
#   when: stage2_nix

# - name: supautils - include /etc/postgresql-custom/supautils.conf in postgresql.conf
#   become: yes
#   replace:
#     path: /etc/postgresql/postgresql.conf
#     regexp: "#include = '/etc/postgresql-custom/supautils.conf'"
#     replace: "include = '/etc/postgresql-custom/supautils.conf'"
#   when: stage2_nix

# - name: Remove build artifacts
#   file:
#     path: "/tmp/supautils-2.6.0"
#     state: absent
#   when: stage2_nix

# supautils
- name: supautils - download & install dependencies
  apt:
    pkg:
      - build-essential
      - clang-11
    update_cache: yes
    cache_valid_time: 3600

- name: supautils - download latest release
  get_url:
    url: "https://github.com/supabase/supautils/archive/refs/tags/v{{ supautils_release }}.tar.gz"
    dest: /tmp/supautils-{{ supautils_release }}.tar.gz
    checksum: "{{ supautils_release_checksum }}"
    timeout: 60

- name: supautils - unpack archive
  unarchive:
    remote_src: yes
    src: /tmp/supautils-{{ supautils_release }}.tar.gz
    dest: /tmp
  become: yes

- name: supautils - build
  make:
    chdir: /tmp/supautils-{{ supautils_release }}
  become: yes

- name: supautils - install
  make:
    chdir: /tmp/supautils-{{ supautils_release }}
    target: install
  become: yes

- name: supautils - add supautils to session_preload_libraries
  become: yes
  replace:
    path: /etc/postgresql/postgresql.conf
    regexp: "#session_preload_libraries = ''"
    replace: session_preload_libraries = 'supautils'

- name: supautils - write custom supautils.conf
  template:
    src: "files/postgresql_config/supautils.conf.j2"
    dest: /etc/postgresql-custom/supautils.conf
    mode: 0664
    owner: postgres
    group: postgres

- name: supautils - copy extension custom scripts
  copy:
    src: files/postgresql_extension_custom_scripts/
    dest: /etc/postgresql-custom/extension-custom-scripts
  become: yes

- name: supautils - chown extension custom scripts
  file:
    mode: 0775
    owner: postgres
    group: postgres
    path: /etc/postgresql-custom/extension-custom-scripts
    recurse: yes
  become: yes

- name: supautils - include /etc/postgresql-custom/supautils.conf in postgresql.conf
  become: yes
  replace:
    path: /etc/postgresql/postgresql.conf
    regexp: "#include = '/etc/postgresql-custom/supautils.conf'"
    replace: "include = '/etc/postgresql-custom/supautils.conf'"

- name: supautils - remove build dependencies
  apt:
    pkg:
      - build-essential
      - clang-11
    state: absent

'''
'''--- /postgres/ansible/tasks/internal/postgresql-prestart.yml ---
- name: postgres_prestart - create service file
  template:
    src: files/postgres_prestart.sh.j2
    dest: /usr/local/bin/postgres_prestart.sh
    mode: a+x
    owner: root
    group: root

'''
'''--- /postgres/ansible/tasks/internal/optimizations.yml ---
- name: ensure services are stopped and disabled for first boot debian build
  systemd:
    enabled: no
    name: '{{ item }}'
    state: stopped
  with_items:
    - postgresql
    - pgbouncer
    - fail2ban
    - motd-news
    - vector
    - lvm2-monitor
    - salt-minion
  when: debpkg_mode 

# - name: ensure services are stopped and disabled for first boot nix build
#   systemd:
#     enabled: no
#     name: '{{ item }}'
#     state: stopped
#   loop:
#     - postgresql
#     - pgbouncer
#     - fail2ban
#     - motd-news
#     - vector
#     - salt-minion
#   when: stage2_nix
#   ignore_errors: yes

- name: ensure services are stopped and disabled for first boot nix build
  block:
    - name: Stop and disable services if they exist
      systemd:
        enabled: no
        name: '{{ item }}'
        state: stopped
      loop:
        - postgresql
        - pgbouncer
        - fail2ban
        - motd-news
        - vector
        - salt-minion
      register: service_result
      failed_when: 
        - service_result.failed is defined 
        - service_result.failed
        - '"Could not find the requested service" not in service_result.msg'
  when: stage2_nix


- name: disable man-db
  become: yes
  file:
    state: absent
    path: "/etc/cron.daily/{{ item }}"
  with_items:
    - man-db
    - popularity-contest
    - ubuntu-advantage-tools
  when: debpkg_mode or stage2_nix

'''
'''--- /postgres/ansible/tasks/internal/admin-api.yml ---
- name: adminapi - system user
  user:
    name: adminapi
    groups: root,admin,envoy,kong,pgbouncer,postgres,postgrest,systemd-journal,vector,wal-g
    append: yes
  when: stage2_nix

- name: Ensure all required groups exist
  block:
    - name: Create admin group if not exists
      group:
        name: admin
        state: present
        system: yes

    - name: Verify all groups exist
      shell: |
        for group in root admin envoy kong pgbouncer postgres postgrest systemd-journal vector wal-g; do
          getent group $group || echo "Missing group: $group"
        done
      register: group_check
      changed_when: false
  when: stage2_nix

- name: Move shell scripts to /root dir
  copy:
    src: "files/admin_api_scripts/{{ item.file }}"
    dest: "/root/{{ item.file }}"
    mode: "0700"
    owner: root
  loop:
    - { file: "grow_fs.sh" }
    - { file: "manage_readonly_mode.sh" }
    - { file: "pg_egress_collect.pl" }
  when: stage2_nix

- name: give adminapi user permissions
  copy:
    src: files/adminapi.sudoers.conf
    dest: /etc/sudoers.d/adminapi
    mode: "0644"

- name: perms for adminapi
  shell: |
    chmod g+w /etc

- name: Setting arch (x86)
  set_fact:
    arch: "x86"
  when: platform == "amd64"

- name: Setting arch (arm)
  set_fact:
    arch: "arm64"
  when: platform == "arm64"

- name: Download adminapi archive
  get_url:
    url: "https://supabase-public-artifacts-bucket.s3.amazonaws.com/supabase-admin-api/v{{ adminapi_release }}/supabase-admin-api_{{ adminapi_release }}_linux_{{ arch }}.tar.gz"
    dest: "/tmp/adminapi.tar.gz"
    timeout: 90

- name: adminapi - unpack archive in /opt
  unarchive:
    remote_src: yes
    src: /tmp/adminapi.tar.gz
    dest: /opt
    owner: adminapi

- name: adminapi - config dir
  file:
    path: /etc/adminapi
    owner: adminapi
    state: directory

- name: adminapi - pg_upgrade scripts dir
  file:
    path: /etc/adminapi/pg_upgrade_scripts
    owner: adminapi
    state: directory

- name: Move shell scripts to /etc/adminapi/pg_upgrade_scripts/
  copy:
    src: "files/admin_api_scripts/pg_upgrade_scripts/{{ item.file }}"
    dest: "/etc/adminapi/pg_upgrade_scripts/{{ item.file }}"
    mode: "0755"
    owner: adminapi
  loop:
    - { file: "check.sh" }
    - { file: "complete.sh" }
    - { file: "initiate.sh" }
    - { file: "prepare.sh" }
    - { file: "pgsodium_getkey.sh" }
    - { file: "common.sh" }

- name: adminapi - create service file
  template:
    src: files/adminapi.service.j2
    dest: /etc/systemd/system/adminapi.service

- name: adminapi - create service file for commence backup process
  template:
     src: files/commence-backup.service.j2
     dest: /etc/systemd/system/commence-backup.service

- name: UFW - Allow connections to adminapi ports
  ufw:
    rule: allow
    port: "8085"

- name: adminapi - reload systemd
  systemd:
    daemon_reload: yes

- name: adminapi - grant extra priviliges to user
  shell: chmod 775 /etc && chmod 775 /etc/kong

'''
'''--- /postgres/ansible/tasks/internal/install-salt.yml ---
- name: Add apt repository for Saltstack (arm)
  block:
    - name: Ensure /etc/apt/keyrings directory exists
      file:
        path: /etc/apt/keyrings
        state: directory
        mode: '0755'

    - name: salt gpg key
      ansible.builtin.get_url:
        url: https://packages.broadcom.com/artifactory/api/security/keypair/SaltProjectKey/public
        dest: /etc/apt/keyrings/salt-archive-keyring-2023.pgp
        mode: '0644'

    - name: salt apt repo
      ansible.builtin.apt_repository:
         repo: "deb [signed-by=/etc/apt/keyrings/salt-archive-keyring-2023.pgp arch=arm64] https://packages.broadcom.com/artifactory/saltproject-deb/ stable main"
         filename: 'salt.list'
         state: present
  when: platform == "arm64"

- name: Add apt repository for Saltstack (amd)
  block:
    - name: Ensure /etc/apt/keyrings directory exists
      file:
        path: /etc/apt/keyrings
        state: directory
        mode: '0755'

    - name: salt gpg key
      ansible.builtin.get_url:
        url: https://packages.broadcom.com/artifactory/api/security/keypair/SaltProjectKey/public
        dest: /etc/apt/keyrings/salt-archive-keyring-2023.pgp
        mode: '0644'

    - name: salt apt repo
      ansible.builtin.apt_repository:
         repo: "deb [signed-by=/etc/apt/keyrings/salt-archive-keyring-2023.pgp arch=amd64] https://packages.broadcom.com/artifactory/saltproject-deb/ stable main"
         filename: 'salt.list'
         state: present
  when: platform == "amd64"

- name: Salt minion install
  apt:
    name: salt-minion
    state: present
    update_cache: yes

'''
'''--- /postgres/ansible/tasks/internal/pg_egress_collect.yml ---
- name: pg_egress_collect - install tcpdump and perl async lib
  apt:
    pkg:
      - tcpdump
      - libio-async-perl


- name: pg_egress_collect - create service file
  template:
    src: files/pg_egress_collect.service.j2
    dest: /etc/systemd/system/pg_egress_collect.service

- name: pg_egress_collect - reload systemd
  systemd:
    daemon_reload: yes


'''
'''--- /postgres/ansible/tasks/internal/admin-mgr.yml ---
- name: Setting arch (x86)
  set_fact:
    arch: "amd64"
  when: platform == "amd64"

- name: Setting arch (arm)
  set_fact:
    arch: "arm64"
  when: platform == "arm64"

- name: Download admin-mgr archive
  get_url:
    url: "https://supabase-public-artifacts-bucket.s3.amazonaws.com/admin-mgr/v{{ adminmgr_release }}/admin-mgr_{{ adminmgr_release }}_linux_{{ arch }}.tar.gz"
    dest: "/tmp/admin-mgr.tar.gz"
    timeout: 90

- name: admin-mgr - unpack archive in /usr/bin/
  unarchive:
    remote_src: yes
    src: /tmp/admin-mgr.tar.gz
    dest: /usr/bin/
    owner: root

'''
'''--- /postgres/ansible/tasks/internal/collect-pg-binaries.yml ---
- name: Collect Postgres binaries - create collection directory
  file:
    path: /tmp/pg_binaries/{{ postgresql_major }}/
    state: directory

- name: Collect Postgres binaries - collect binaries and libraries
  copy:
    remote_src: yes
    src: /usr/lib/postgresql/{{ postgresql_major }}/{{ item }}/
    dest: /tmp/pg_binaries/{{ postgresql_major }}/{{ item }}/
  with_items:
    - bin
    - lib

- name: Collect Postgres libraries - collect libraries which are in /usr/lib/postgresql/lib/
  copy:
    remote_src: yes
    src: /usr/lib/postgresql/lib/
    dest: /tmp/pg_binaries/{{ postgresql_major }}/lib/

- name: Collect Postgres libraries - collect libraries which are in /var/lib/postgresql/extension/
  copy:
    remote_src: yes
    src: /var/lib/postgresql/extension/
    dest: /tmp/pg_binaries/{{ postgresql_major }}/lib/

- name: Collect Postgres libraries - collect latest libpq
  copy:
    remote_src: yes
    src: /usr/lib/aarch64-linux-gnu/libpq.so.5
    dest: /tmp/pg_binaries/{{ postgresql_major }}/lib/libpq.so.5

- name: Collect Postgres binaries - collect shared files
  copy:
    remote_src: yes
    src: /usr/share/postgresql/{{ postgresql_major }}/
    dest: /tmp/pg_binaries/{{ postgresql_major }}/share/

- name: Collect Postgres binaries - create tarfile
  archive:
    path: /tmp/pg_binaries/
    dest: /tmp/pg_binaries.tar.gz
    remove: yes

- name: Fetch tarfile to local
  fetch:
    src: /tmp/pg_binaries.tar.gz
    dest: /tmp/
    flat: true

'''
'''--- /postgres/ansible/tasks/internal/setup-ansible-pull.yml ---
- name: install ansible
  shell:
    cmd: |
      apt install -y software-properties-common
      add-apt-repository --yes --update ppa:ansible/ansible
      apt install -y ansible
      sed -i -e 's/#callback_whitelist.*/callback_whitelist = profile_tasks/' /etc/ansible/ansible.cfg

- name: ansible pull systemd units
  copy:
    src: files/{{ item }}
    dest: /etc/systemd/system/{{ item }}
  with_items:
    - ansible-pull.service
    - ansible-pull.timer

- name: create facts dir
  file:
    path: /etc/ansible/facts.d
    state: directory

- name: ansible facts
  copy:
    src: files/supabase_facts.ini
    dest: /etc/ansible/facts.d/supabase.fact

- name: reload systemd
  systemd:
    daemon_reload: yes

'''
'''--- /postgres/ansible/tasks/internal/postgres-exporter.yml ---
- name: UFW - Allow connections to exporter for prometheus
  ufw:
    rule: allow
    port: "9187"

- name: create directories - systemd unit
  file:
    state: directory
    path: /etc/systemd/system/postgres_exporter.service.d
    owner: root
    mode: '0700'
  become: yes

- name: create directories - service files
  file:
    state: directory
    path: /opt/postgres_exporter
    owner: postgres
    group: postgres
    mode: '0775'
  become: yes

- name: download postgres exporter
  get_url:
    url: "https://github.com/prometheus-community/postgres_exporter/releases/download/v{{ postgres_exporter_release }}/postgres_exporter-{{ postgres_exporter_release }}.linux-{{ platform }}.tar.gz"
    dest: /tmp/postgres_exporter.tar.gz
    checksum: "{{ postgres_exporter_release_checksum[platform] }}"
    timeout: 60

- name: expand postgres exporter
  unarchive:
    remote_src: yes
    src: /tmp/postgres_exporter.tar.gz
    dest: /opt/postgres_exporter
    extra_opts: [--strip-components=1]
  become: yes

- name: exporter create a service
  template:
    src: files/postgres_exporter.service.j2
    dest: /etc/systemd/system/postgres_exporter.service

- name: exporter ensure service is present
  systemd:
    enabled: no
    name: postgres_exporter
    daemon_reload: yes
    state: stopped

'''
'''--- /postgres/ansible/tasks/internal/setup-nftables.yml ---
- name: nftables overrides
  file:
    state: directory
    path: /etc/nftables
    owner: adminapi

- name: nftables empty config
  file:
    state: touch
    path: /etc/nftables/supabase_managed.conf
    owner: adminapi

- name: include managed config
  shell: |
    cat >> "/etc/nftables.conf" << EOF
    table inet supabase_managed { }
    include "/etc/nftables/supabase_managed.conf";

    EOF

- name: ufw overrides dir
  file:
    state: directory
    path: /etc/systemd/system/ufw.service.d
    owner: root

- name: Custom systemd overrides
  copy:
    src: files/ufw.service.conf
    dest: /etc/systemd/system/ufw.service.d/overrides.conf

- name: reload systemd
  systemd:
    daemon_reload: yes

'''
'''--- /postgres/ansible/files/envoy.service ---
[Unit]
Description=Envoy
After=postgrest.service gotrue.service adminapi.service
Wants=postgrest.service gotrue.service adminapi.service
Conflicts=kong.service

[Service]
Type=simple

ExecStartPre=sh -c 'if ss -lnt | grep -Eq ":(80|443) "; then echo "Port 80 or 443 already in use"; exit 1; fi'

# Need to run via a restarter script to support hot restart when using a process
# manager, see:
# https://www.envoyproxy.io/docs/envoy/latest/operations/hot_restarter
ExecStart=/opt/envoy-hot-restarter.py /opt/start-envoy.sh

ExecReload=/bin/kill -HUP $MAINPID
ExecStop=/bin/kill -TERM $MAINPID
User=envoy
Slice=services.slice
Restart=always
RestartSec=3
LimitNOFILE=100000

# The envoy user is unprivileged and thus not permitted to bind on ports < 1024
# Via systemd we grant the process a set of privileges to bind to 80/443
# See http://archive.vn/36zJU
AmbientCapabilities=CAP_NET_BIND_SERVICE

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/postgres_prestart.sh.j2 ---
#!/bin/bash

check_orioledb_enabled() {
   local pg_conf="/etc/postgresql/postgresql.conf"
   if [ ! -f "$pg_conf" ]; then
       return 0
    fi
   grep "^shared_preload_libraries" "$pg_conf" | grep -c "orioledb" || return 0
}

get_shared_buffers() {
    local opt_conf="/etc/postgresql-custom/generated-optimizations.conf"
    if [ ! -f "$opt_conf" ]; then
        return 0
    fi
    grep "^shared_buffers = " "$opt_conf" | cut -d "=" -f2 | tr -d ' ' || return 0
}

update_orioledb_buffers() {
   local pg_conf="/etc/postgresql/postgresql.conf"
   local value="$1"
   if grep -q "^orioledb.main_buffers = " "$pg_conf"; then
       sed -i "s/^orioledb.main_buffers = .*/orioledb.main_buffers = $value/" "$pg_conf"
   else
       echo "orioledb.main_buffers = $value" >> "$pg_conf"
   fi
}

main() {
   local has_orioledb=$(check_orioledb_enabled)
   if [ "$has_orioledb" -lt 1 ]; then
       return 0
   fi
   local shared_buffers_value=$(get_shared_buffers)
   if [ ! -z "$shared_buffers_value" ]; then
       update_orioledb_buffers "$shared_buffers_value"
   fi
}

# Initial locale setup
if [ $(cat /etc/locale.gen | grep -c en_US.UTF-8) -eq 0 ]; then
   echo "en_US.UTF-8 UTF-8" >> /etc/locale.gen
fi

if [ $(locale -a | grep -c en_US.utf8) -eq 0 ]; then
   locale-gen
fi

main

'''
'''--- /postgres/ansible/files/adminapi.service.j2 ---
[Unit]
Description=AdminAPI

[Service]
Type=simple
ExecStart=/opt/supabase-admin-api
User=adminapi
Restart=always
RestartSec=3
Environment="AWS_USE_DUALSTACK_ENDPOINT=true"

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/supabase_facts.ini ---
[general]
postgres_version=15

'''
'''--- /postgres/ansible/files/vector.service.j2 ---
[Unit]
Description=Vector
Documentation=https://vector.dev
After=network-online.target
Requires=network-online.target

[Service]
User=vector
Group=vector
ExecStartPre=/usr/bin/vector validate --config-yaml /etc/vector/vector.yaml
ExecStart=/usr/bin/vector --config-yaml /etc/vector/vector.yaml
ExecReload=/usr/bin/vector validate --config-yaml /etc/vector/vector.yaml
ExecReload=/bin/kill -HUP $MAINPID
Restart=always
RestartSec=3
AmbientCapabilities=CAP_NET_BIND_SERVICE
EnvironmentFile=-/etc/default/vector

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/stat_extension.sql ---
CREATE SCHEMA IF NOT exists extensions;
CREATE EXTENSION IF NOT EXISTS pg_stat_statements with schema extensions;

'''
'''--- /postgres/ansible/files/journald.conf ---
[Journal]
Storage=persistent
SystemMaxUse=3G
SystemKeepFree=3G
SystemMaxFileSize=200M
ForwardToSyslog=no

'''
'''--- /postgres/ansible/files/postgres_exporter.service.j2 ---
[Unit]
Description=Postgres Exporter

[Service]
Type=simple
ExecStart=/opt/postgres_exporter/postgres_exporter --disable-settings-metrics --extend.query-path="/opt/postgres_exporter/queries.yml" --disable-default-metrics --no-collector.locks --no-collector.replication --no-collector.replication_slot --no-collector.stat_bgwriter --no-collector.stat_database --no-collector.stat_user_tables --no-collector.statio_user_tables --no-collector.wal
User=postgres
Group=postgres
Restart=always
RestartSec=3
Environment="DATA_SOURCE_NAME=host=localhost dbname=postgres sslmode=disable user=supabase_admin pg_stat_statements.track=none application_name=postgres_exporter"

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/cron.deny ---
ubuntu
postgres

'''
'''--- /postgres/ansible/files/default.sysstat ---
#
# Default settings for /etc/init.d/sysstat, /etc/cron.d/sysstat
# and /etc/cron.daily/sysstat files
#

# Should sadc collect system activity informations? Valid values
# are "true" and "false". Please do not put other values, they
# will be overwritten by debconf!
ENABLED="true"

'''
'''--- /postgres/ansible/files/ansible-pull.service ---
[Unit]
Description=Ansible pull

[Service]
Type=simple
User=ubuntu

ExecStart=/usr/bin/ansible-pull --private-key "$SSH_READ_KEY_FILE" -U "$REPO" --accept-host-key -t "$REGION,db-all" -i localhost --clean --full "$PLAYBOOK" -v -o -C "$REPO_BRANCH"

# --verify-commit
# temporarily disable commit verification, while we figure out how we want to balance commit signatures
# and PR reviews; an --ff-only merge options would have allowed us to use this pretty nicely

MemoryAccounting=true
MemoryMax=30%

StandardOutput=append:/var/log/ansible-pull.stdout
StandardError=append:/var/log/ansible-pull.error

TimeoutStopSec=600

'''
'''--- /postgres/ansible/files/ufw.service.conf ---
[Unit]
After=nftables.service
Requires=nftables.service
PartOf=nftables.service

'''
'''--- /postgres/ansible/files/gotrue-optimizations.service.j2 ---
[Unit]
Description=GoTrue (Auth) optimizations

[Service]
Type=oneshot
# we don't want failures from this command to cause PG startup to fail
ExecStart=/bin/bash -c "/opt/supabase-admin-api optimize auth --destination-config-file-path /etc/gotrue/gotrue.generated.env ; exit 0"
User=postgrest

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/pgsodium_getkey_readonly.sh.j2 ---
#!/bin/bash

set -euo pipefail

KEY_FILE=/etc/postgresql-custom/pgsodium_root.key

# On the hosted platform, the root key is generated and managed for each project
# If for some reason the key is missing, we want to fail loudly,
# rather than generating a new one.
if [[ ! -f "${KEY_FILE}" ]]; then
    echo "Key file ${KEY_FILE} does not exist." >&2
    exit 1
fi
cat $KEY_FILE

'''
'''--- /postgres/ansible/files/start-envoy.sh ---
#!/usr/bin/env bash
set -eou pipefail

if [[ $(cat /sys/module/ipv6/parameters/disable) = 1 ]]; then
  sed -i -e "s/address: '::'/address: '0.0.0.0'/" -e 's/ipv4_compat: true/ipv4_compat: false/' /etc/envoy/lds.yaml
else
  sed -i -e "s/address: '0.0.0.0'/address: '::'/" -e 's/ipv4_compat: false/ipv4_compat: true/' /etc/envoy/lds.yaml
fi

# Workaround using `tee` to get `/dev/stdout` access logging to work, see:
# https://github.com/envoyproxy/envoy/issues/8297#issuecomment-620659781
exec /opt/envoy --config-path /etc/envoy/envoy.yaml --restart-epoch "${RESTART_EPOCH}" 2>&1 | tee

'''
'''--- /postgres/ansible/files/permission_check.py ---
import subprocess
import json
import sys

# Expected groups for each user
expected_results = {
    "postgres": [
        {"groupname": "postgres", "username": "postgres"},
        {"groupname": "ssl-cert", "username": "postgres"}
    ],
    "ubuntu": [
        {"groupname":"ubuntu","username":"ubuntu"},
        {"groupname":"adm","username":"ubuntu"},
        {"groupname":"dialout","username":"ubuntu"},
        {"groupname":"cdrom","username":"ubuntu"},
        {"groupname":"floppy","username":"ubuntu"},
        {"groupname":"sudo","username":"ubuntu"},
        {"groupname":"audio","username":"ubuntu"},
        {"groupname":"dip","username":"ubuntu"},
        {"groupname":"video","username":"ubuntu"},
        {"groupname":"plugdev","username":"ubuntu"},
        {"groupname":"lxd","username":"ubuntu"},
        {"groupname":"netdev","username":"ubuntu"}
    ],
    "root": [
        {"groupname":"root","username":"root"}
    ],
    "daemon": [
        {"groupname":"daemon","username":"daemon"}
    ],
    "bin": [
        {"groupname":"bin","username":"bin"}
    ],
    "sys": [
        {"groupname":"sys","username":"sys"}
    ],
    "sync": [
        {"groupname":"nogroup","username":"sync"}
    ],
    "games": [
        {"groupname":"games","username":"games"}
    ],
    "man": [
        {"groupname":"man","username":"man"}
    ],
    "lp": [
        {"groupname":"lp","username":"lp"}
    ],
    "mail": [
        {"groupname":"mail","username":"mail"}
    ],
    "news": [
        {"groupname":"news","username":"news"}
    ],
    "uucp": [
        {"groupname":"uucp","username":"uucp"}
    ],
    "proxy": [
        {"groupname":"proxy","username":"proxy"}
    ],
    "www-data": [
        {"groupname":"www-data","username":"www-data"}
    ],
    "backup": [
        {"groupname":"backup","username":"backup"}
    ],
    "list": [
        {"groupname":"list","username":"list"}
    ],
    "irc": [
        {"groupname":"irc","username":"irc"}
    ],
    "gnats": [
        {"groupname":"gnats","username":"gnats"}
    ],
    "nobody": [
        {"groupname":"nogroup","username":"nobody"}
    ],
    "systemd-network": [
        {"groupname":"systemd-network","username":"systemd-network"}
    ],
    "systemd-resolve": [
        {"groupname":"systemd-resolve","username":"systemd-resolve"}
    ],
    "systemd-timesync": [
        {"groupname":"systemd-timesync","username":"systemd-timesync"}
    ],
    "messagebus": [
        {"groupname":"messagebus","username":"messagebus"}
    ],
    "ec2-instance-connect": [
        {"groupname":"nogroup","username":"ec2-instance-connect"}
    ],
    "sshd": [
        {"groupname":"nogroup","username":"sshd"}
    ],
    "wal-g": [
        {"groupname":"wal-g","username":"wal-g"},
        {"groupname":"postgres","username":"wal-g"}
    ],
    "pgbouncer": [
        {"groupname":"pgbouncer","username":"pgbouncer"},
        {"groupname":"ssl-cert","username":"pgbouncer"},
        {"groupname":"postgres","username":"pgbouncer"}
    ],
    "gotrue": [
        {"groupname":"gotrue","username":"gotrue"}
    ],
    "envoy": [
        {"groupname":"envoy","username":"envoy"}
    ],
    "kong": [
        {"groupname":"kong","username":"kong"}
    ],
    "nginx": [
        {"groupname":"nginx","username":"nginx"}
    ],
    "vector": [
        {"groupname":"vector","username":"vector"},
        {"groupname":"adm","username":"vector"},
        {"groupname":"systemd-journal","username":"vector"},
        {"groupname":"postgres","username":"vector"}
    ],
    "adminapi": [
        {"groupname":"adminapi","username":"adminapi"},
        {"groupname":"root","username":"adminapi"},
        {"groupname":"systemd-journal","username":"adminapi"},
        {"groupname":"admin","username":"adminapi"},
        {"groupname":"postgres","username":"adminapi"},
        {"groupname":"pgbouncer","username":"adminapi"},
        {"groupname":"wal-g","username":"adminapi"},
        {"groupname":"postgrest","username":"adminapi"},
        {"groupname":"envoy","username":"adminapi"},
        {"groupname":"kong","username":"adminapi"},
        {"groupname":"vector","username":"adminapi"}
    ],
    "postgrest": [
        {"groupname":"postgrest","username":"postgrest"}
    ],
    "tcpdump": [
        {"groupname":"tcpdump","username":"tcpdump"}
    ],
    "systemd-coredump": [
        {"groupname":"systemd-coredump","username":"systemd-coredump"}
    ]
}
# This program depends on osquery being installed on the system
# Function to run osquery
def normalize_results(results):
    """Sort results by groupname to ensure consistent comparison."""
    return sorted(results, key=lambda x: x['groupname'])

def run_osquery(query):
    process = subprocess.Popen(['osqueryi', '--json', query], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    output, error = process.communicate()
    return output.decode('utf-8')

def parse_json(json_str):
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print("Error decoding JSON:", e)
        sys.exit(1)

def compare_results(username, query_result):
    expected_result = expected_results.get(username)
    if expected_result is None:
        print(f"No expected result defined for user '{username}'")
        sys.exit(1)

    # Normalize both results before comparison
    normalized_expected = normalize_results(expected_result)
    normalized_actual = normalize_results(query_result)

    if normalized_actual == normalized_expected:
        print(f"The query result for user '{username}' matches the expected result.")
    else:
        print(f"The query result for user '{username}' does not match the expected result.")
        print("Expected:", expected_result)
        print("Got:", query_result)
        sys.exit(1)

def check_nixbld_users():
    query = """
    SELECT u.username, g.groupname 
    FROM users u 
    JOIN user_groups ug ON u.uid = ug.uid 
    JOIN groups g ON ug.gid = g.gid 
    WHERE u.username LIKE 'nixbld%';
    """
    query_result = run_osquery(query)
    parsed_result = parse_json(query_result)
    
    for user in parsed_result:
        if user['groupname'] != 'nixbld':
            print(f"User '{user['username']}' is in group '{user['groupname']}' instead of 'nixbld'.")
            sys.exit(1)
    
    print("All nixbld users are in the 'nixbld' group.")

# Keep your existing usernames list as is
# Define usernames for which you want to compare results
usernames = ["postgres", "ubuntu", "root", "daemon", "bin", "sys", "sync", "games","man","lp","mail","news","uucp","proxy","www-data","backup","list","irc","gnats","nobody","systemd-network","systemd-resolve","systemd-timesync","messagebus","ec2-instance-connect","sshd","wal-g","pgbouncer","gotrue","envoy","kong","nginx","vector","adminapi","postgrest","tcpdump","systemd-coredump"]

# Iterate over usernames, run the query, and compare results
# Modify the query to include ordering
for username in usernames:
    query = f"""
    SELECT u.username, g.groupname 
    FROM users u 
    JOIN user_groups ug ON u.uid = ug.uid 
    JOIN groups g ON ug.gid = g.gid 
    WHERE u.username = '{username}' 
    ORDER BY g.groupname;
    """
    query_result = run_osquery(query)
    parsed_result = parse_json(query_result)
    compare_results(username, parsed_result)

# Check if all nixbld users are in the nixbld group
check_nixbld_users()

'''
'''--- /postgres/ansible/files/systemd-resolved.conf ---
# the default is RestartSec=0. If the service fails to start because
# of a systemic issue (e.g. rare case when disk is full) it will
# quickly hit the burst limit (default of 5 failures within 10secs)
# and thereafter be placed in a failed state. By increasing the
# restart interval, we avoid that, and ensure that the service will be
# started back up once any underlying issues are resolved.
[Service]
RestartSec=3

'''
'''--- /postgres/ansible/files/commence-backup.service.j2 ---
[Unit]
Description=Async commence physical backup

[Service]
Type=simple
User=adminapi
ExecStart=/usr/bin/admin-mgr commence-backup --run-as-service true
Restart=no
OOMScoreAdjust=-1000

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/postgrest-optimizations.service.j2 ---
[Unit]
Description=Postgrest optimizations

[Service]
Type=oneshot
# we don't want failures from this command to cause PG startup to fail
ExecStart=/bin/bash -c "/opt/supabase-admin-api optimize postgrest --destination-config-file-path /etc/postgrest/generated.conf ; exit 0"
User=postgrest

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/database-optimizations.service.j2 ---
[Unit]
Description=Postgresql optimizations

[Service]
Type=oneshot
# we do not want failures from these commands to cause downstream service startup to fail
ExecStart=-/opt/supabase-admin-api optimize db --destination-config-file-path /etc/postgresql-custom/generated-optimizations.conf
ExecStart=-/opt/supabase-admin-api optimize pgbouncer --destination-config-file-path /etc/pgbouncer-custom/generated-optimizations.ini
User=adminapi

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/logind.conf ---
[Login]
RemoveIPC=no

'''
'''--- /postgres/ansible/files/adminapi.sudoers.conf ---
Cmnd_Alias ENVOY = /bin/systemctl start envoy.service, /bin/systemctl stop envoy.service, /bin/systemctl restart envoy.service, /bin/systemctl disable envoy.service, /bin/systemctl enable envoy.service, /bin/systemctl reload envoy.service, /bin/systemctl try-restart envoy.service
Cmnd_Alias KONG = /bin/systemctl start kong.service, /bin/systemctl stop kong.service, /bin/systemctl restart kong.service, /bin/systemctl disable kong.service, /bin/systemctl enable kong.service, /bin/systemctl reload kong.service, /bin/systemctl try-restart kong.service
Cmnd_Alias POSTGREST = /bin/systemctl start postgrest.service, /bin/systemctl stop postgrest.service, /bin/systemctl restart postgrest.service, /bin/systemctl disable postgrest.service, /bin/systemctl enable postgrest.service, /bin/systemctl try-restart postgrest.service
Cmnd_Alias GOTRUE = /bin/systemctl start gotrue.service, /bin/systemctl stop gotrue.service, /bin/systemctl restart gotrue.service, /bin/systemctl disable gotrue.service, /bin/systemctl enable gotrue.service, /bin/systemctl try-restart gotrue.service
Cmnd_Alias PGBOUNCER = /bin/systemctl start pgbouncer.service, /bin/systemctl stop pgbouncer.service, /bin/systemctl restart pgbouncer.service, /bin/systemctl disable pgbouncer.service, /bin/systemctl enable pgbouncer.service, /bin/systemctl reload pgbouncer.service, /bin/systemctl try-restart pgbouncer.service

%adminapi ALL= NOPASSWD: /root/grow_fs.sh
%adminapi ALL= NOPASSWD: /root/manage_readonly_mode.sh
%adminapi ALL= NOPASSWD: /etc/adminapi/pg_upgrade_scripts/prepare.sh
%adminapi ALL= NOPASSWD: /etc/adminapi/pg_upgrade_scripts/initiate.sh
%adminapi ALL= NOPASSWD: /etc/adminapi/pg_upgrade_scripts/complete.sh
%adminapi ALL= NOPASSWD: /etc/adminapi/pg_upgrade_scripts/check.sh
%adminapi ALL= NOPASSWD: /etc/adminapi/pg_upgrade_scripts/common.sh
%adminapi ALL= NOPASSWD: /etc/adminapi/pg_upgrade_scripts/pgsodium_getkey.sh
%adminapi ALL= NOPASSWD: /usr/bin/systemctl daemon-reload
%adminapi ALL= NOPASSWD: /usr/bin/systemctl reload postgresql.service
%adminapi ALL= NOPASSWD: /usr/bin/systemctl restart postgresql.service
%adminapi ALL= NOPASSWD: /usr/bin/systemctl show -p NRestarts postgresql.service
%adminapi ALL= NOPASSWD: /usr/bin/systemctl restart adminapi.service
%adminapi ALL= NOPASSWD: /usr/bin/systemctl is-active commence-backup.service
%adminapi ALL= NOPASSWD: /usr/bin/systemctl start commence-backup.service
%adminapi ALL= NOPASSWD: /bin/systemctl daemon-reload
%adminapi ALL= NOPASSWD: /bin/systemctl restart services.slice
%adminapi ALL= NOPASSWD: /usr/sbin/nft -f /etc/nftables/supabase_managed.conf
%adminapi ALL= NOPASSWD: /usr/bin/admin-mgr
%adminapi ALL= NOPASSWD: ENVOY
%adminapi ALL= NOPASSWD: KONG
%adminapi ALL= NOPASSWD: POSTGREST
%adminapi ALL= NOPASSWD: GOTRUE
%adminapi ALL= NOPASSWD: PGBOUNCER

'''
'''--- /postgres/ansible/files/gotrue.service.j2 ---
[Unit]
Description=Gotrue

[Service]
Type=simple
WorkingDirectory=/opt/gotrue
ExecStart=/opt/gotrue/gotrue
User=gotrue
Restart=always
RestartSec=3

MemoryAccounting=true
MemoryMax=50%

EnvironmentFile=-/etc/gotrue.generated.env
EnvironmentFile=/etc/gotrue.env
EnvironmentFile=-/etc/gotrue.overrides.env

Slice=services.slice

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/manifest.json ---
{{ vars | to_json }}

'''
'''--- /postgres/ansible/files/nginx.service.j2 ---
[Unit]
Description=nginx server
After=postgrest.service gotrue.service adminapi.service
Wants=postgrest.service gotrue.service adminapi.service

[Service]
Type=forking
ExecStart=/usr/local/nginx/sbin/nginx -c /etc/nginx/nginx.conf
ExecReload=/usr/local/nginx/sbin/nginx -s reload -c /etc/nginx/nginx.conf
ExecStop=/usr/local/nginx/sbin/nginx -s quit
User=nginx
Slice=services.slice
Restart=always
RestartSec=3
LimitNOFILE=100000

# Via systemd we grant the process a set of privileges to bind to 80/443
# See http://archive.vn/36zJU
AmbientCapabilities=CAP_NET_BIND_SERVICE

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/ansible-pull.timer ---
[Unit]
Description=Run ansible roughly every 3 hours

[Timer]
OnBootSec=1h
OnUnitActiveSec=3h
RandomizedDelaySec=1h
Persistent=true

[Install]
WantedBy=timers.target

'''
'''--- /postgres/ansible/files/services.slice.j2 ---
# Used for general services grouping for easy visibility when running
#    systemctl status
# See http://archive.vn/94IGa for an in depth article on systemd slices
[Unit]
Description=Slice used for PostgreSQL
Before=slices.target

'''
'''--- /postgres/ansible/files/sysstat.sysstat ---
# How long to keep log files (in days).
# Used by sa2(8) script
# If value is greater than 28, then use sadc's option -D to prevent older
# data files from being overwritten. See sadc(8) and sysstat(5) manual pages.
HISTORY=7

# Compress (using xz, gzip or bzip2) sa and sar files older than (in days):
COMPRESSAFTER=10

# Parameters for the system activity data collector (see sadc(8) manual page)
# which are used for the generation of log files.
# By default contains the `-S DISK' option responsible for generating disk
# statisitcs. Use `-S XALL' to collect all available statistics.
SADC_OPTIONS="-S DISK"

# Directory where sa and sar files are saved. The directory must exist.
SA_DIR=/var/log/sysstat

# Compression program to use.
ZIP="xz"

# By default sa2 script generates yesterday's summary, since the cron job
# usually runs right after midnight. If you want sa2 to generate the summary
# of the same day (for example when cron job runs at 23:53) set this variable.
#YESTERDAY=no

# By default sa2 script generates reports files (the so called sarDD files).
# Set this variable to false to disable reports generation.
#REPORTS=false

# The sa1 and sa2 scripts generate system activity data and report files in
# the /var/log/sysstat directory. By default the files are created with umask 0022
# and are therefore readable for all users. Change this variable to restrict
# the permissions on the files (e.g. use 0027 to adhere to more strict
# security standards).
UMASK=0022

'''
'''--- /postgres/ansible/files/postgrest.service.j2 ---
[Unit]
Description=PostgREST
Requires=postgrest-optimizations.service
After=postgrest-optimizations.service

[Service]
Type=simple
# We allow the base config (sent from the worker) to override the generated config
ExecStartPre=/etc/postgrest/merge.sh /etc/postgrest/generated.conf /etc/postgrest/base.conf
ExecStart=/opt/postgrest /etc/postgrest/merged.conf
User=postgrest
Slice=services.slice
Restart=always
RestartSec=3
LimitNOFILE=100000

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/pgsodium_getkey_urandom.sh.j2 ---
#!/bin/bash

set -euo pipefail

KEY_FILE=/etc/postgresql-custom/pgsodium_root.key

if [[ ! -f "${KEY_FILE}" ]]; then
    head -c 32 /dev/urandom | od -A n -t x1 | tr -d ' \n' > "${KEY_FILE}"
fi
cat $KEY_FILE

'''
'''--- /postgres/ansible/files/pg_egress_collect.service.j2 ---
[Unit]
Description=Postgres Egress Collector

[Service]
Type=simple
ExecStart=/bin/bash -c "tcpdump -s 128 -Q out -nn -tt -vv -p -l 'tcp and (port 5432 or port 6543)' | perl /root/pg_egress_collect.pl"
User=root
Slice=services.slice
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/sodium_extension.sql ---
create schema if not exists pgsodium;
create extension if not exists pgsodium with schema pgsodium cascade;

grant pgsodium_keyiduser to postgres with admin option;
grant pgsodium_keyholder to postgres with admin option;
grant pgsodium_keymaker  to postgres with admin option;

'''
'''--- /postgres/ansible/files/apt_periodic ---
APT::Periodic::Update-Package-Lists "1";
APT::Periodic::Download-Upgradeable-Packages "1";
APT::Periodic::AutocleanInterval "7";
APT::Periodic::Unattended-Upgrade "1";
'''
'''--- /postgres/ansible/files/fail2ban_config/fail2ban.service.conf ---
[Unit]
After=nftables.service
Wants=nftables.service

[Service]
ExecStartPost=/bin/bash -c "sleep 5 && chmod g+w /var/run/fail2ban/fail2ban.sock"

'''
'''--- /postgres/ansible/files/fail2ban_config/jail.local ---
[DEFAULT]

banaction = nftables-multiport
banaction_allports = nftables-allports

'''
'''--- /postgres/ansible/files/fail2ban_config/jail-ssh.conf ---
[sshd]

backend = systemd
mode = aggressive

'''
'''--- /postgres/ansible/files/fail2ban_config/jail-postgresql.conf.j2 ---
[postgresql]
enabled = true
port    = 5432
protocol = tcp
filter = postgresql
logpath = /var/log/postgresql/auth-failures.csv
maxretry = 3
ignoreip = 192.168.0.0/16 172.17.1.0/20

'''
'''--- /postgres/ansible/files/fail2ban_config/jail-pgbouncer.conf.j2 ---
[pgbouncer]
enabled = true
port    = 6543
protocol = tcp
filter = pgbouncer
backend = systemd[journalflags=1]
maxretry = 3

'''
'''--- /postgres/ansible/files/fail2ban_config/filter-postgresql.conf.j2 ---
[Definition]
failregex = ^.*,.*,.*,.*,"<HOST>:.*password authentication failed for user.*$
ignoreregex = ^.*,.*,.*,.*,"127\.0\.0\.1.*password authentication failed for user.*$
'''
'''--- /postgres/ansible/files/fail2ban_config/filter-pgbouncer.conf.j2 ---
[Definition]
failregex = ^.+@<HOST>:.+password authentication failed$
journalmatch = _SYSTEMD_UNIT=pgbouncer.service

'''
'''--- /postgres/ansible/files/postgresql_extension_custom_scripts/before-create.sql ---
-- If the following are true:
-- * the extension to be created is a TLE
-- * the extension is created with `cascade`
--
-- then we pre-`create` all nested extension dependencies which are part of
-- `supautils.privileged_extensions`. This is because supautils can't intercept
-- the extension creation for dependencies - it can only intercept the `create
-- extension` statement.
do $$
declare
  _extname text := @extname@;
  _extschema text := @extschema@;
  _extversion text := @extversion@;
  _extcascade bool := @extcascade@;
  _r record;
begin
  if not _extcascade then
    return;
  end if;

  if not exists (select from pg_extension where extname = 'pg_tle') then
    return;
  end if;

  if not exists (select from pgtle.available_extensions() where name = _extname) then
    return;
  end if;

  if _extversion is null then
    select default_version
    from pgtle.available_extensions()
    where name = _extname
    into _extversion;
  end if;

  if _extschema is null then
    select schema
    from pgtle.available_extension_versions()
    where name = _extname and version = _extversion
    into _extschema;
  end if;

  for _r in (
    with recursive available_extensions(name, default_version) as (
      select name, default_version
      from pg_available_extensions
      union
      select name, default_version
      from pgtle.available_extensions()
    )
    , available_extension_versions(name, version, requires) as (
      select name, version, requires
      from pg_available_extension_versions
      union
      select name, version, requires
      from pgtle.available_extension_versions()
    )
    , all_dependencies(name, dependency) as (
      select e.name, unnest(ev.requires) as dependency
      from available_extensions e
      join available_extension_versions ev on ev.name = e.name and ev.version = e.default_version
    )
    , dependencies(name) AS (
        select unnest(requires)
        from available_extension_versions
        where name = _extname and version = _extversion
        union
        select all_dependencies.dependency
        from all_dependencies
        join dependencies d on d.name = all_dependencies.name
    )
    select name
    from dependencies
    intersect
    select name
    from regexp_split_to_table(current_setting('supautils.privileged_extensions', true), '\s*,\s*') as t(name)
  ) loop
    if _extschema is null then
      execute(format('create extension if not exists %I cascade', _r.name));
    else
      execute(format('create extension if not exists %I schema %I cascade', _r.name, _extschema));
    end if;
  end loop;
end $$;

'''
'''--- /postgres/ansible/files/postgresql_extension_custom_scripts/postgres_fdw/after-create.sql ---
do $$
declare
  is_super boolean;
begin
  is_super = (
    select usesuper
    from pg_user
    where usename = 'postgres'
  );

  -- Need to be superuser to own FDWs, so we temporarily make postgres superuser.
  if not is_super then
    alter role postgres superuser;
  end if;

  alter foreign data wrapper postgres_fdw owner to postgres;

  if not is_super then
    alter role postgres nosuperuser;
  end if;
end $$;

'''
'''--- /postgres/ansible/files/postgresql_extension_custom_scripts/pg_repack/after-create.sql ---
grant all on all tables in schema repack to postgres;
grant all on schema repack to postgres;
alter default privileges in schema repack grant all on tables to postgres;
alter default privileges in schema repack grant all on sequences to postgres;

'''
'''--- /postgres/ansible/files/postgresql_extension_custom_scripts/pg_tle/after-create.sql ---
grant pgtle_admin to postgres;

'''
'''--- /postgres/ansible/files/postgresql_extension_custom_scripts/pg_cron/after-create.sql ---
grant usage on schema cron to postgres with grant option;
grant all on all functions in schema cron to postgres with grant option;

alter default privileges for user supabase_admin in schema cron grant all
    on sequences to postgres with grant option;
alter default privileges for user supabase_admin in schema cron grant all
    on tables to postgres with grant option;
alter default privileges for user supabase_admin in schema cron grant all
    on functions to postgres with grant option;

grant all privileges on all tables in schema cron to postgres with grant option;
revoke all on table cron.job from postgres;
grant select on table cron.job to postgres with grant option;

'''
'''--- /postgres/ansible/files/postgresql_extension_custom_scripts/dblink/after-create.sql ---
do $$
declare
  r record;
begin
  for r in (select oid, (aclexplode(proacl)).grantee from pg_proc where proname = 'dblink_connect_u') loop
   continue when r.grantee = 'supabase_admin'::regrole;
   execute(
     format(
       'revoke all on function %s(%s) from %s;', r.oid::regproc, pg_get_function_identity_arguments(r.oid), r.grantee::regrole
     )
   );
  end loop;
end
$$;

'''
'''--- /postgres/ansible/files/postgresql_extension_custom_scripts/pgsodium/after-create.sql ---
grant execute on function pgsodium.crypto_aead_det_decrypt(bytea, bytea, uuid, bytea) to service_role;
grant execute on function pgsodium.crypto_aead_det_encrypt(bytea, bytea, uuid, bytea) to service_role;
grant execute on function pgsodium.crypto_aead_det_keygen to service_role;

CREATE OR REPLACE FUNCTION pgsodium.mask_role(masked_role regrole, source_name text, view_name text)
RETURNS void
LANGUAGE plpgsql
SECURITY DEFINER
SET search_path TO ''
AS $function$
BEGIN
  EXECUTE format(
    'GRANT SELECT ON pgsodium.key TO %s',
    masked_role);

  EXECUTE format(
    'GRANT pgsodium_keyiduser, pgsodium_keyholder TO %s',
    masked_role);

  EXECUTE format(
    'GRANT ALL ON %I TO %s',
    view_name,
    masked_role);
  RETURN;
END
$function$;

'''
'''--- /postgres/ansible/files/postgresql_extension_custom_scripts/pgsodium/before-create.sql ---
do $$
declare
  _extversion text := @extversion@;
  _r record;
begin
  if _extversion is not null and _extversion != '3.1.8' then
    raise exception 'only pgsodium 3.1.8 is supported';
  end if;
end $$;

'''
'''--- /postgres/ansible/files/postgresql_extension_custom_scripts/pgmq/after-create.sql ---
do $$
declare
  extoid oid := (select oid from pg_extension where extname = 'pgmq');
  r record;
  cls pg_class%rowtype;
begin

  set local search_path = '';

/*
    Override the pgmq.drop_queue to check if relevant tables are owned
    by the pgmq extension before attempting to run
    `alter extension pgmq drop table ...`
    this is necessary becasue, to enable nightly logical backups to include user queues
    we automatically detach them from pgmq.

    this update is backwards compatible with version 1.4.4 but should be removed once we're on
    physical backups everywhere
*/
-- Detach and delete the official function
alter extension pgmq drop function pgmq.drop_queue;
drop function pgmq.drop_queue;

-- Create and reattach the patched function
CREATE FUNCTION pgmq.drop_queue(queue_name TEXT)
RETURNS BOOLEAN AS $func$
DECLARE
    qtable TEXT := pgmq.format_table_name(queue_name, 'q');
    qtable_seq TEXT := qtable || '_msg_id_seq';
    fq_qtable TEXT := 'pgmq.' || qtable;
    atable TEXT := pgmq.format_table_name(queue_name, 'a');
    fq_atable TEXT := 'pgmq.' || atable;
    partitioned BOOLEAN;
BEGIN
    EXECUTE FORMAT(
        $QUERY$
        SELECT is_partitioned FROM pgmq.meta WHERE queue_name = %L
        $QUERY$,
        queue_name
    ) INTO partitioned;

    -- NEW CONDITIONAL CHECK
    if exists (
        select 1
        from pg_class c
        join pg_depend d on c.oid = d.objid
        join pg_extension e on d.refobjid = e.oid
        where c.relname = qtable and e.extname = 'pgmq'
    ) then

        EXECUTE FORMAT(
            $QUERY$
            ALTER EXTENSION pgmq DROP TABLE pgmq.%I
            $QUERY$,
            qtable
        );

    end if;

    -- NEW CONDITIONAL CHECK
    if exists (
        select 1
        from pg_class c
        join pg_depend d on c.oid = d.objid
        join pg_extension e on d.refobjid = e.oid
        where c.relname = qtable_seq and e.extname = 'pgmq'
    ) then    
        EXECUTE FORMAT(
            $QUERY$
            ALTER EXTENSION pgmq DROP SEQUENCE pgmq.%I
            $QUERY$,
            qtable_seq
        );

    end if;

    -- NEW CONDITIONAL CHECK
    if exists (
        select 1
        from pg_class c
        join pg_depend d on c.oid = d.objid
        join pg_extension e on d.refobjid = e.oid
        where c.relname = atable and e.extname = 'pgmq'
    ) then

    EXECUTE FORMAT(
        $QUERY$
        ALTER EXTENSION pgmq DROP TABLE pgmq.%I
        $QUERY$,
        atable
    );

    end if;

    -- NO CHANGES PAST THIS POINT

    EXECUTE FORMAT(
        $QUERY$
        DROP TABLE IF EXISTS pgmq.%I
        $QUERY$,
        qtable
    );

    EXECUTE FORMAT(
        $QUERY$
        DROP TABLE IF EXISTS pgmq.%I
        $QUERY$,
        atable
    );

     IF EXISTS (
          SELECT 1
          FROM information_schema.tables
          WHERE table_name = 'meta' and table_schema = 'pgmq'
     ) THEN
        EXECUTE FORMAT(
            $QUERY$
            DELETE FROM pgmq.meta WHERE queue_name = %L
            $QUERY$,
            queue_name
        );
     END IF;

     IF partitioned THEN
        EXECUTE FORMAT(
          $QUERY$
          DELETE FROM %I.part_config where parent_table in (%L, %L)
          $QUERY$,
          pgmq._get_pg_partman_schema(), fq_qtable, fq_atable
        );
     END IF;

    RETURN TRUE;
END;
$func$ LANGUAGE plpgsql;

alter extension pgmq add function pgmq.drop_queue;


  update pg_extension set extowner = 'postgres'::regrole where extname = 'pgmq';

  for r in (select * from pg_depend where refobjid = extoid) loop


    if r.classid = 'pg_type'::regclass then

      -- store the type's relkind
      select * into cls from pg_class c where c.reltype = r.objid;

      if r.objid::regtype::text like '%[]' then
        -- do nothing (skipping array type)

      elsif cls.relkind in ('r', 'p', 'f', 'm') then
        -- table-like objects (regular table, partitioned, foreign, materialized view)
        execute format('alter table pgmq.%I owner to postgres;', cls.relname);

      else
        execute(format('alter type %s owner to postgres;', r.objid::regtype));

      end if;

    elsif r.classid = 'pg_proc'::regclass then
      execute(format('alter function %s(%s) owner to postgres;', r.objid::regproc, pg_get_function_identity_arguments(r.objid)));

    elsif r.classid = 'pg_class'::regclass then
      execute(format('alter table %s owner to postgres;', r.objid::regclass));

    else
      raise exception 'error on pgmq after-create script: unexpected object type %', r.classid;

    end if;
  end loop;
end $$;

'''
'''--- /postgres/ansible/files/postgresql_extension_custom_scripts/postgis_tiger_geocoder/after-create.sql ---
-- These schemas are created by extension to house all tiger related functions, owned by supabase_admin
grant usage on schema tiger, tiger_data to postgres with grant option;
-- Give postgres permission to all existing entities, also allows postgres to grant other roles
grant all on all tables in schema tiger, tiger_data to postgres with grant option;
grant all on all routines in schema tiger, tiger_data to postgres with grant option;
grant all on all sequences in schema tiger, tiger_data to postgres with grant option;
-- Update default privileges so that new entities are also accessible by postgres
alter default privileges in schema tiger, tiger_data grant all on tables to postgres with grant option;
alter default privileges in schema tiger, tiger_data grant all on routines to postgres with grant option;
alter default privileges in schema tiger, tiger_data grant all on sequences to postgres with grant option;

'''
'''--- /postgres/ansible/files/postgresql_config/postgresql.service.j2 ---
[Unit]
Description=PostgreSQL database server
Documentation=man:postgres(1)
{% if supabase_internal is defined %}
Requires=database-optimizations.service
After=database-optimizations.service
{% endif %}

[Service]
Type=notify
User=postgres
ExecStart=/usr/lib/postgresql/bin/postgres -D /etc/postgresql
ExecStartPre=+/usr/local/bin/postgres_prestart.sh
ExecReload=/bin/kill -HUP $MAINPID
KillMode=mixed
KillSignal=SIGINT
TimeoutStopSec=90
TimeoutStartSec=86400
Restart=always
RestartSec=5
OOMScoreAdjust=-1000
EnvironmentFile=-/etc/environment.d/postgresql.env

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/postgresql_config/tmpfiles.postgresql.conf ---
# unchanged from upstream package
d /run/postgresql 2775 postgres postgres - -
# Log directory - ensure that our logging setup gets preserved
# and that vector can keep writing to a file here as well
d /var/log/postgresql 1775 postgres postgres - -

'''
'''--- /postgres/ansible/files/postgresql_config/supautils.conf.j2 ---
supautils.extensions_parameter_overrides = '{"pg_cron":{"schema":"pg_catalog"}}'
supautils.policy_grants = '{"postgres":["auth.audit_log_entries","auth.identities","auth.refresh_tokens","auth.sessions","auth.users","realtime.messages","storage.buckets","storage.migrations","storage.objects","storage.s3_multipart_uploads","storage.s3_multipart_uploads_parts"]}'
supautils.drop_trigger_grants = '{"postgres":["auth.audit_log_entries","auth.identities","auth.refresh_tokens","auth.sessions","auth.users","realtime.messages","storage.buckets","storage.migrations","storage.objects","storage.s3_multipart_uploads","storage.s3_multipart_uploads_parts"]}'
# full list:                                 address_standardizer, address_standardizer_data_us, adminpack, amcheck, autoinc, bloom, btree_gin, btree_gist, citext, cube, dblink, dict_int, dict_xsyn, earthdistance, file_fdw, fuzzystrmatch, hstore, http, hypopg, index_advisor, insert_username, intagg, intarray, isn, lo, ltree, moddatetime, old_snapshot, orioledb, pageinspect, pg_buffercache, pg_cron, pg_freespacemap, pg_graphql, pg_hashids, pg_jsonschema, pg_net, pg_prewarm, pg_repack, pg_stat_monitor, pg_stat_statements, pg_surgery, pg_tle, pg_trgm, pg_visibility, pg_walinspect, pgaudit, pgcrypto, pgjwt, pgmq, pgroonga, pgroonga_database, pgrouting, pgrowlocks, pgsodium, pgstattuple, pgtap, plcoffee, pljava, plls, plpgsql, plpgsql_check, plv8, postgis, postgis_raster, postgis_sfcgal, postgis_tiger_geocoder, postgis_topology, postgres_fdw, refint, rum, seg, sslinfo, supabase_vault, supautils, tablefunc, tcn, timescaledb, tsm_system_rows, tsm_system_time, unaccent, uuid-ossp, vector, wrappers, xml2
# omitted because may be unsafe:             adminpack, amcheck, file_fdw, lo, old_snapshot, pageinspect, pg_buffercache, pg_freespacemap, pg_surgery, pg_visibility
# omitted because deprecated:                intagg, xml2
# omitted because doesn't require superuser: pgmq
supautils.privileged_extensions = 'address_standardizer, address_standardizer_data_us, autoinc, bloom, btree_gin, btree_gist, citext, cube, dblink, dict_int, dict_xsyn, earthdistance, fuzzystrmatch, hstore, http, hypopg, index_advisor, insert_username, intarray, isn, ltree, moddatetime, orioledb, pg_cron, pg_graphql, pg_hashids, pg_jsonschema, pg_net, pg_prewarm, pg_repack, pg_stat_monitor, pg_stat_statements, pg_tle, pg_trgm, pg_walinspect, pgaudit, pgcrypto, pgjwt, pgroonga, pgroonga_database, pgrouting, pgrowlocks, pgsodium, pgstattuple, pgtap, plcoffee, pljava, plls, plpgsql, plpgsql_check, plv8, postgis, postgis_raster, postgis_sfcgal, postgis_tiger_geocoder, postgis_topology, postgres_fdw, refint, rum, seg, sslinfo, supabase_vault, supautils, tablefunc, tcn, timescaledb, tsm_system_rows, tsm_system_time, unaccent, uuid-ossp, vector, wrappers'
supautils.privileged_extensions_custom_scripts_path = '/etc/postgresql-custom/extension-custom-scripts'
supautils.privileged_extensions_superuser = 'supabase_admin'
supautils.privileged_role = 'postgres'
supautils.privileged_role_allowed_configs = 'auto_explain.*, log_lock_waits, log_min_duration_statement, log_min_messages, log_statement, log_temp_files, pg_net.batch_size, pg_net.ttl, pg_stat_statements.*, pgaudit.log, pgaudit.log_catalog, pgaudit.log_client, pgaudit.log_level, pgaudit.log_relation, pgaudit.log_rows, pgaudit.log_statement, pgaudit.log_statement_once, pgaudit.role, pgrst.*, plan_filter.*, safeupdate.enabled, session_replication_role, track_io_timing, wal_compression'
supautils.reserved_memberships = 'pg_read_server_files, pg_write_server_files, pg_execute_server_program, supabase_admin, supabase_auth_admin, supabase_storage_admin, supabase_read_only_user, supabase_realtime_admin, supabase_replication_admin, dashboard_user, pgbouncer, authenticator'
supautils.reserved_roles = 'supabase_admin, supabase_auth_admin, supabase_storage_admin, supabase_read_only_user, supabase_realtime_admin, supabase_replication_admin, dashboard_user, pgbouncer, service_role*, authenticator*, authenticated*, anon*'

'''
'''--- /postgres/ansible/files/postgresql_config/postgresql.conf.j2 ---
# -----------------------------
# PostgreSQL configuration file
# -----------------------------
#
# This file consists of lines of the form:
#
#   name = value
#
# (The "=" is optional.)  Whitespace may be used.  Comments are introduced with
# "#" anywhere on a line.  The complete list of parameter names and allowed
# values can be found in the PostgreSQL documentation.
#
# The commented-out settings shown in this file represent the default values.
# Re-commenting a setting is NOT sufficient to revert it to the default value;
# you need to reload the server.
#
# This file is read on server startup and when the server receives a SIGHUP
# signal.  If you edit the file on a running system, you have to SIGHUP the
# server for the changes to take effect, run "pg_ctl reload", or execute
# "SELECT pg_reload_conf()".  Some parameters, which are marked below,
# require a server shutdown and restart to take effect.
#
# Any parameter can also be given as a command-line option to the server, e.g.,
# "postgres -c log_connections=on".  Some parameters can be changed at run time
# with the "SET" SQL command.
#
# Memory units:  B  = bytes            Time units:  us  = microseconds
#                kB = kilobytes                     ms  = milliseconds
#                MB = megabytes                     s   = seconds
#                GB = gigabytes                     min = minutes
#                TB = terabytes                     h   = hours
#                                                   d   = days


#------------------------------------------------------------------------------
# FILE LOCATIONS
#------------------------------------------------------------------------------

# The default values of these variables are driven from the -D command-line
# option or PGDATA environment variable, represented here as ConfigDir.

data_directory = '/var/lib/postgresql/data'		# use data in another directory
					# (change requires restart)
hba_file = '/etc/postgresql/pg_hba.conf'	# host-based authentication file
					# (change requires restart)
ident_file = '/etc/postgresql/pg_ident.conf'	# ident configuration file
					# (change requires restart)

# If external_pid_file is not explicitly set, no extra PID file is written.
#external_pid_file = ''			# write an extra PID file
					# (change requires restart)


#------------------------------------------------------------------------------
# CONNECTIONS AND AUTHENTICATION
#------------------------------------------------------------------------------

# - Connection Settings -

listen_addresses = '*'		# what IP address(es) to listen on;
					# comma-separated list of addresses;
					# defaults to 'localhost'; use '*' for all
					# (change requires restart)
#port = 5432				# (change requires restart)
#max_connections = 100			# (change requires restart)
#superuser_reserved_connections = 3	# (change requires restart)
#unix_socket_directories = '/tmp'	# comma-separated list of directories
					# (change requires restart)
#unix_socket_group = ''			# (change requires restart)
#unix_socket_permissions = 0777		# begin with 0 to use octal notation
					# (change requires restart)
#bonjour = off				# advertise server via Bonjour
					# (change requires restart)
#bonjour_name = ''			# defaults to the computer name
					# (change requires restart)

# - TCP settings -
# see "man tcp" for details

#tcp_keepalives_idle = 0		# TCP_KEEPIDLE, in seconds;
					# 0 selects the system default
#tcp_keepalives_interval = 0		# TCP_KEEPINTVL, in seconds;
					# 0 selects the system default
#tcp_keepalives_count = 0		# TCP_KEEPCNT;
					# 0 selects the system default
#tcp_user_timeout = 0			# TCP_USER_TIMEOUT, in milliseconds;
					# 0 selects the system default

#client_connection_check_interval = 0	# time between checks for client
					# disconnection while running queries;
					# 0 for never

# - Authentication -

authentication_timeout = 1min		# 1s-600s
password_encryption = scram-sha-256	# scram-sha-256 or md5
db_user_namespace = off

# GSSAPI using Kerberos
#krb_server_keyfile = 'FILE:${sysconfdir}/krb5.keytab'
#krb_caseins_users = off

# - SSL -

ssl = off
ssl_ca_file = ''
ssl_cert_file = ''
ssl_crl_file = ''
ssl_crl_dir = ''
ssl_key_file = ''
ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # allowed SSL ciphers
ssl_prefer_server_ciphers = on
ssl_ecdh_curve = 'prime256v1'
ssl_min_protocol_version = 'TLSv1.2'
ssl_max_protocol_version = ''
ssl_dh_params_file = ''
ssl_passphrase_command = ''
ssl_passphrase_command_supports_reload = off


#------------------------------------------------------------------------------
# RESOURCE USAGE (except WAL)
#------------------------------------------------------------------------------

# - Memory -

shared_buffers = 128MB			# min 128kB
					# (change requires restart)
#huge_pages = try			# on, off, or try
					# (change requires restart)
#huge_page_size = 0			# zero for system default
					# (change requires restart)
#temp_buffers = 8MB			# min 800kB
#max_prepared_transactions = 0		# zero disables the feature
					# (change requires restart)
# Caution: it is not advisable to set max_prepared_transactions nonzero unless
# you actively intend to use prepared transactions.
#work_mem = 4MB				# min 64kB
#hash_mem_multiplier = 1.0		# 1-1000.0 multiplier on hash table work_mem
#maintenance_work_mem = 64MB		# min 1MB
#autovacuum_work_mem = -1		# min 1MB, or -1 to use maintenance_work_mem
#logical_decoding_work_mem = 64MB	# min 64kB
#max_stack_depth = 2MB			# min 100kB
#shared_memory_type = mmap		# the default is the first option
					# supported by the operating system:
					#   mmap
					#   sysv
					#   windows
					# (change requires restart)
#dynamic_shared_memory_type = posix	# the default is the first option
					# supported by the operating system:
					#   posix
					#   sysv
					#   windows
					#   mmap
					# (change requires restart)
#min_dynamic_shared_memory = 0MB	# (change requires restart)

# - Disk -

#temp_file_limit = -1			# limits per-process temp file space
					# in kilobytes, or -1 for no limit

# - Kernel Resources -

#max_files_per_process = 1000		# min 64
					# (change requires restart)

# - Cost-Based Vacuum Delay -

#vacuum_cost_delay = 0			# 0-100 milliseconds (0 disables)
#vacuum_cost_page_hit = 1		# 0-10000 credits
#vacuum_cost_page_miss = 2		# 0-10000 credits
#vacuum_cost_page_dirty = 20		# 0-10000 credits
#vacuum_cost_limit = 200		# 1-10000 credits

# - Background Writer -

#bgwriter_delay = 200ms			# 10-10000ms between rounds
#bgwriter_lru_maxpages = 100		# max buffers written/round, 0 disables
#bgwriter_lru_multiplier = 2.0		# 0-10.0 multiplier on buffers scanned/round
#bgwriter_flush_after = 0		# measured in pages, 0 disables

# - Asynchronous Behavior -

#backend_flush_after = 0		# measured in pages, 0 disables
#effective_io_concurrency = 1		# 1-1000; 0 disables prefetching
#maintenance_io_concurrency = 10	# 1-1000; 0 disables prefetching
#max_worker_processes = 8		# (change requires restart)
#max_parallel_workers_per_gather = 2	# taken from max_parallel_workers
#max_parallel_maintenance_workers = 2	# taken from max_parallel_workers
#max_parallel_workers = 8		# maximum number of max_worker_processes that
					# can be used in parallel operations
#parallel_leader_participation = on
#old_snapshot_threshold = -1		# 1min-60d; -1 disables; 0 is immediate
					# (change requires restart)


#------------------------------------------------------------------------------
# WRITE-AHEAD LOG
#------------------------------------------------------------------------------

# - Settings -

wal_level = logical			# minimal, replica, or logical
					# (change requires restart)
#fsync = on				# flush data to disk for crash safety
					# (turning this off can cause
					# unrecoverable data corruption)
#synchronous_commit = on		# synchronization level;
					# off, local, remote_write, remote_apply, or on
#wal_sync_method = fsync		# the default is the first option
					# supported by the operating system:
					#   open_datasync
					#   fdatasync (default on Linux and FreeBSD)
					#   fsync
					#   fsync_writethrough
					#   open_sync
#full_page_writes = on			# recover from partial page writes
#wal_log_hints = off			# also do full page writes of non-critical updates
					# (change requires restart)
#wal_compression = off			# enable compression of full-page writes
#wal_init_zero = on			# zero-fill new WAL files
#wal_recycle = on			# recycle WAL files
#wal_buffers = -1			# min 32kB, -1 sets based on shared_buffers
					# (change requires restart)
#wal_writer_delay = 200ms		# 1-10000 milliseconds
#wal_writer_flush_after = 1MB		# measured in pages, 0 disables
#wal_skip_threshold = 2MB

#commit_delay = 0			# range 0-100000, in microseconds
#commit_siblings = 5			# range 1-1000

# - Checkpoints -

#checkpoint_timeout = 5min		# range 30s-1d
checkpoint_completion_target = 0.5	# checkpoint target duration, 0.0 - 1.0
checkpoint_flush_after = 256kB		# measured in pages, 0 disables
#checkpoint_warning = 30s		# 0 disables
#max_wal_size = 1GB
#min_wal_size = 80MB

# - Archiving -

#archive_mode = off		# enables archiving; off, on, or always
				# (change requires restart)
#archive_command = ''		# command to use to archive a logfile segment
				# placeholders: %p = path of file to archive
				#               %f = file name only
				# e.g. 'test ! -f /mnt/server/archivedir/%f && cp %p /mnt/server/archivedir/%f'
#archive_timeout = 0		# force a logfile segment switch after this
				# number of seconds; 0 disables

# - Archive Recovery -

# These are only used in recovery mode.

#restore_command = ''		# command to use to restore an archived logfile segment
				# placeholders: %p = path of file to restore
				#               %f = file name only
				# e.g. 'cp /mnt/server/archivedir/%f %p'
#archive_cleanup_command = ''	# command to execute at every restartpoint
#recovery_end_command = ''	# command to execute at completion of recovery

# - Recovery Target -

# Set these only when performing a targeted recovery.

#recovery_target = ''		# 'immediate' to end recovery as soon as a
                                # consistent state is reached
				# (change requires restart)
#recovery_target_name = ''	# the named restore point to which recovery will proceed
				# (change requires restart)
#recovery_target_time = ''	# the time stamp up to which recovery will proceed
				# (change requires restart)
#recovery_target_xid = ''	# the transaction ID up to which recovery will proceed
				# (change requires restart)
#recovery_target_lsn = ''	# the WAL LSN up to which recovery will proceed
				# (change requires restart)
#recovery_target_inclusive = on # Specifies whether to stop:
				# just after the specified recovery target (on)
				# just before the recovery target (off)
				# (change requires restart)
#recovery_target_timeline = 'latest'	# 'current', 'latest', or timeline ID
				# (change requires restart)
#recovery_target_action = 'pause'	# 'pause', 'promote', 'shutdown'
				# (change requires restart)


#------------------------------------------------------------------------------
# REPLICATION
#------------------------------------------------------------------------------

# - Sending Servers -

# Set these on the primary and on any standby that will send replication data.

max_wal_senders = 10		# max number of walsender processes
				# (change requires restart)
max_replication_slots = 5	# max number of replication slots
				# (change requires restart)
#wal_keep_size = 0		# in megabytes; 0 disables
max_slot_wal_keep_size = 4096   # in megabytes; -1 disables
#wal_sender_timeout = 60s	# in milliseconds; 0 disables
#track_commit_timestamp = off	# collect timestamp of transaction commit
				# (change requires restart)

# - Primary Server -

# These settings are ignored on a standby server.

#synchronous_standby_names = ''	# standby servers that provide sync rep
				# method to choose sync standbys, number of sync standbys,
				# and comma-separated list of application_name
				# from standby(s); '*' = all
#vacuum_defer_cleanup_age = 0	# number of xacts by which cleanup is delayed

# - Standby Servers -

# These settings are ignored on a primary server.

#primary_conninfo = ''			# connection string to sending server
#primary_slot_name = ''			# replication slot on sending server
#promote_trigger_file = ''		# file name whose presence ends recovery
#hot_standby = on			# "off" disallows queries during recovery
					# (change requires restart)
#max_standby_archive_delay = 30s	# max delay before canceling queries
					# when reading WAL from archive;
					# -1 allows indefinite delay
#max_standby_streaming_delay = 30s	# max delay before canceling queries
					# when reading streaming WAL;
					# -1 allows indefinite delay
#wal_receiver_create_temp_slot = off	# create temp slot if primary_slot_name
					# is not set
#wal_receiver_status_interval = 10s	# send replies at least this often
					# 0 disables
#hot_standby_feedback = off		# send info from standby to prevent
					# query conflicts
#wal_receiver_timeout = 60s		# time that receiver waits for
					# communication from primary
					# in milliseconds; 0 disables
#wal_retrieve_retry_interval = 5s	# time to wait before retrying to
					# retrieve WAL after a failed attempt
#recovery_min_apply_delay = 0		# minimum delay for applying changes during recovery

# - Subscribers -

# These settings are ignored on a publisher.

#max_logical_replication_workers = 4	# taken from max_worker_processes
					# (change requires restart)
#max_sync_workers_per_subscription = 2	# taken from max_logical_replication_workers


#------------------------------------------------------------------------------
# QUERY TUNING
#------------------------------------------------------------------------------

# - Planner Method Configuration -

#enable_async_append = on
#enable_bitmapscan = on
#enable_gathermerge = on
#enable_hashagg = on
#enable_hashjoin = on
#enable_incremental_sort = on
#enable_indexscan = on
#enable_indexonlyscan = on
#enable_material = on
#enable_resultcache = on
#enable_mergejoin = on
#enable_nestloop = on
#enable_parallel_append = on
#enable_parallel_hash = on
#enable_partition_pruning = on
#enable_partitionwise_join = off
#enable_partitionwise_aggregate = off
#enable_seqscan = on
#enable_sort = on
#enable_tidscan = on

# - Planner Cost Constants -

#seq_page_cost = 1.0			# measured on an arbitrary scale
#random_page_cost = 4.0			# same scale as above
#cpu_tuple_cost = 0.01			# same scale as above
#cpu_index_tuple_cost = 0.005		# same scale as above
#cpu_operator_cost = 0.0025		# same scale as above
#parallel_setup_cost = 1000.0	# same scale as above
#parallel_tuple_cost = 0.1		# same scale as above
#min_parallel_table_scan_size = 8MB
#min_parallel_index_scan_size = 512kB
effective_cache_size = 128MB

#jit_above_cost = 100000		# perform JIT compilation if available
					# and query more expensive than this;
					# -1 disables
#jit_inline_above_cost = 500000		# inline small functions if query is
					# more expensive than this; -1 disables
#jit_optimize_above_cost = 500000	# use expensive JIT optimizations if
					# query is more expensive than this;
					# -1 disables

# - Genetic Query Optimizer -

#geqo = on
#geqo_threshold = 12
#geqo_effort = 5			# range 1-10
#geqo_pool_size = 0			# selects default based on effort
#geqo_generations = 0			# selects default based on effort
#geqo_selection_bias = 2.0		# range 1.5-2.0
#geqo_seed = 0.0			# range 0.0-1.0

# - Other Planner Options -

#default_statistics_target = 100	# range 1-10000
#constraint_exclusion = partition	# on, off, or partition
#cursor_tuple_fraction = 0.1		# range 0.0-1.0
#from_collapse_limit = 8
#jit = on				# allow JIT compilation
#join_collapse_limit = 8		# 1 disables collapsing of explicit
					# JOIN clauses
#plan_cache_mode = auto			# auto, force_generic_plan or
					# force_custom_plan


#------------------------------------------------------------------------------
# REPORTING AND LOGGING
#------------------------------------------------------------------------------

include = '/etc/postgresql/logging.conf'

# These are relevant when logging to syslog:
#syslog_facility = 'LOCAL0'
#syslog_ident = 'postgres'
#syslog_sequence_numbers = on
#syslog_split_messages = on

# This is only relevant when logging to eventlog (Windows):
# (change requires restart)
#event_source = 'PostgreSQL'

# - When to Log -

#log_min_messages = warning		# values in order of decreasing detail:
					#   debug5
					#   debug4
					#   debug3
					#   debug2
					#   debug1
					#   info
					#   notice
					#   warning
					#   error
					#   log
					#   fatal
					#   panic

#log_min_error_statement = error	# values in order of decreasing detail:
					#   debug5
					#   debug4
					#   debug3
					#   debug2
					#   debug1
					#   info
					#   notice
					#   warning
					#   error
					#   log
					#   fatal
					#   panic (effectively off)

#log_min_duration_statement = -1	# -1 is disabled, 0 logs all statements
					# and their durations, > 0 logs only
					# statements running at least this number
					# of milliseconds

#log_min_duration_sample = -1		# -1 is disabled, 0 logs a sample of statements
					# and their durations, > 0 logs only a sample of
					# statements running at least this number
					# of milliseconds;
					# sample fraction is determined by log_statement_sample_rate

#log_statement_sample_rate = 1.0	# fraction of logged statements exceeding
					# log_min_duration_sample to be logged;
					# 1.0 logs all such statements, 0.0 never logs


#log_transaction_sample_rate = 0.0	# fraction of transactions whose statements
					# are logged regardless of their duration; 1.0 logs all
					# statements from all transactions, 0.0 never logs

# - What to Log -

#debug_print_parse = off
#debug_print_rewritten = off
#debug_print_plan = off
#debug_pretty_print = on
#log_autovacuum_min_duration = -1	# log autovacuum activity;
					# -1 disables, 0 logs all actions and
					# their durations, > 0 logs only
					# actions running at least this number
					# of milliseconds.
#log_checkpoints = off
#log_connections = off
#log_disconnections = off
#log_duration = off
#log_error_verbosity = default		# terse, default, or verbose messages
#log_hostname = off
log_line_prefix = '%h %m [%p] %q%u@%d '		# special values:
					#   %a = application name
					#   %u = user name
					#   %d = database name
					#   %r = remote host and port
					#   %h = remote host
					#   %b = backend type
					#   %p = process ID
					#   %P = process ID of parallel group leader
					#   %t = timestamp without milliseconds
					#   %m = timestamp with milliseconds
					#   %n = timestamp with milliseconds (as a Unix epoch)
					#   %Q = query ID (0 if none or not computed)
					#   %i = command tag
					#   %e = SQL state
					#   %c = session ID
					#   %l = session line number
					#   %s = session start timestamp
					#   %v = virtual transaction ID
					#   %x = transaction ID (0 if none)
					#   %q = stop here in non-session
					#        processes
					#   %% = '%'
					# e.g. '<%u%%%d> '
#log_lock_waits = off			# log lock waits >= deadlock_timeout
#log_recovery_conflict_waits = off	# log standby recovery conflict waits
					# >= deadlock_timeout
#log_parameter_max_length = -1		# when logging statements, limit logged
					# bind-parameter values to N bytes;
					# -1 means print in full, 0 disables
#log_parameter_max_length_on_error = 0	# when logging an error, limit logged
					# bind-parameter values to N bytes;
					# -1 means print in full, 0 disables
log_statement = 'none'			# none, ddl, mod, all
#log_replication_commands = off
#log_temp_files = -1			# log temporary files equal or larger
					# than the specified size in kilobytes;
					# -1 disables, 0 logs all temp files
log_timezone = 'UTC'

#------------------------------------------------------------------------------
# PROCESS TITLE
#------------------------------------------------------------------------------

cluster_name = 'main'			# added to process titles if nonempty
					# (change requires restart)
#update_process_title = on


#------------------------------------------------------------------------------
# STATISTICS
#------------------------------------------------------------------------------

# - Query and Index Statistics Collector -

#track_activities = on
#track_activity_query_size = 1024	# (change requires restart)
#track_counts = on
#track_io_timing = off
#track_wal_io_timing = off
#track_functions = none			# none, pl, all
#stats_temp_directory = 'pg_stat_tmp'


# - Monitoring -

#compute_query_id = auto
#log_statement_stats = off
#log_parser_stats = off
#log_planner_stats = off
#log_executor_stats = off


#------------------------------------------------------------------------------
# AUTOVACUUM
#------------------------------------------------------------------------------

#autovacuum = on			# Enable autovacuum subprocess?  'on'
					# requires track_counts to also be on.
#autovacuum_max_workers = 3		# max number of autovacuum subprocesses
					# (change requires restart)
#autovacuum_naptime = 1min		# time between autovacuum runs
#autovacuum_vacuum_threshold = 50	# min number of row updates before
					# vacuum
#autovacuum_vacuum_insert_threshold = 1000	# min number of row inserts
					# before vacuum; -1 disables insert
					# vacuums
#autovacuum_analyze_threshold = 50	# min number of row updates before
					# analyze
#autovacuum_vacuum_scale_factor = 0.2	# fraction of table size before vacuum
#autovacuum_vacuum_insert_scale_factor = 0.2	# fraction of inserts over table
					# size before insert vacuum
#autovacuum_analyze_scale_factor = 0.1	# fraction of table size before analyze
#autovacuum_freeze_max_age = 200000000	# maximum XID age before forced vacuum
					# (change requires restart)
#autovacuum_multixact_freeze_max_age = 400000000	# maximum multixact age
					# before forced vacuum
					# (change requires restart)
#autovacuum_vacuum_cost_delay = 2ms	# default vacuum cost delay for
					# autovacuum, in milliseconds;
					# -1 means use vacuum_cost_delay
#autovacuum_vacuum_cost_limit = -1	# default vacuum cost limit for
					# autovacuum, -1 means use
					# vacuum_cost_limit


#------------------------------------------------------------------------------
# CLIENT CONNECTION DEFAULTS
#------------------------------------------------------------------------------

# - Statement Behavior -

#client_min_messages = notice		# values in order of decreasing detail:
					#   debug5
					#   debug4
					#   debug3
					#   debug2
					#   debug1
					#   log
					#   notice
					#   warning
					#   error
#search_path = '"$user", public'	# schema names
row_security = on
#default_table_access_method = 'heap'
#default_tablespace = ''		# a tablespace name, '' uses the default
#default_toast_compression = 'pglz'	# 'pglz' or 'lz4'
#temp_tablespaces = ''			# a list of tablespace names, '' uses
					# only default tablespace
#check_function_bodies = on
#default_transaction_isolation = 'read committed'
#default_transaction_read_only = off
#default_transaction_deferrable = off
#session_replication_role = 'origin'
#statement_timeout = 0			# in milliseconds, 0 is disabled
#lock_timeout = 0			# in milliseconds, 0 is disabled
#idle_in_transaction_session_timeout = 0	# in milliseconds, 0 is disabled
#idle_session_timeout = 0		# in milliseconds, 0 is disabled
#vacuum_freeze_table_age = 150000000
#vacuum_freeze_min_age = 50000000
#vacuum_failsafe_age = 1600000000
#vacuum_multixact_freeze_table_age = 150000000
#vacuum_multixact_freeze_min_age = 5000000
#vacuum_multixact_failsafe_age = 1600000000
#bytea_output = 'hex'			# hex, escape
#xmlbinary = 'base64'
#xmloption = 'content'
#gin_pending_list_limit = 4MB

# - Locale and Formatting -

#datestyle = 'iso, mdy'
#intervalstyle = 'postgres'
timezone = 'UTC'
#timezone_abbreviations = 'Default'     # Select the set of available time zone
					# abbreviations.  Currently, there are
					#   Default
					#   Australia (historical usage)
					#   India
					# You can create your own file in
					# share/timezonesets/.
extra_float_digits = 0			# min -15, max 3; any value >0 actually
					# selects precise output mode
#client_encoding = sql_ascii		# actually, defaults to database
					# encoding

# These settings are initialized by initdb, but they can be changed.
lc_messages = 'en_US.UTF-8'			# locale for system error message
					# strings
lc_monetary = 'en_US.UTF-8'			# locale for monetary formatting
lc_numeric = 'en_US.UTF-8'			# locale for number formatting
lc_time = 'en_US.UTF-8'				# locale for time formatting

# default configuration for text search
default_text_search_config = 'pg_catalog.english'

# - Shared Library Preloading -

#local_preload_libraries = ''
#session_preload_libraries = ''

shared_preload_libraries = 'pg_stat_statements, pgaudit, plpgsql, plpgsql_check, pg_cron, pg_net, pgsodium, timescaledb, auto_explain, pg_tle, plan_filter'	# (change requires restart)
jit_provider = 'llvmjit'		# JIT library to use

# - Other Defaults -

#dynamic_library_path = '$libdir'
#gin_fuzzy_search_limit = 0

#------------------------------------------------------------------------------
# LOCK MANAGEMENT
#------------------------------------------------------------------------------

#deadlock_timeout = 1s
#max_locks_per_transaction = 64		# min 10
					# (change requires restart)
#max_pred_locks_per_transaction = 64	# min 10
					# (change requires restart)
#max_pred_locks_per_relation = -2	# negative values mean
					# (max_pred_locks_per_transaction
					#  / -max_pred_locks_per_relation) - 1
#max_pred_locks_per_page = 2            # min 0


#------------------------------------------------------------------------------
# VERSION AND PLATFORM COMPATIBILITY
#------------------------------------------------------------------------------

# - Previous PostgreSQL Versions -

#array_nulls = on
#backslash_quote = safe_encoding	# on, off, or safe_encoding
#escape_string_warning = on
#lo_compat_privileges = off
#quote_all_identifiers = off
#standard_conforming_strings = on
#synchronize_seqscans = on

# - Other Platforms and Clients -

#transform_null_equals = off


#------------------------------------------------------------------------------
# ERROR HANDLING
#------------------------------------------------------------------------------

#exit_on_error = off			# terminate session on any error?
#restart_after_crash = on		# reinitialize after backend crash?
#data_sync_retry = off			# retry or panic on failure to fsync
					# data?
					# (change requires restart)
#recovery_init_sync_method = fsync	# fsync, syncfs (Linux 5.8+)


#------------------------------------------------------------------------------
# CONFIG FILE INCLUDES
#------------------------------------------------------------------------------

# These options allow settings to be loaded from files other than the
# default postgresql.conf.  Note that these are directives, not variable
# assignments, so they can usefully be given more than once.

#include_dir = '...'			# include files ending in '.conf' from
					# a directory, e.g., 'conf.d'
#include_if_exists = '...'		# include file only if it exists
#include = '...'			# include file

# Automatically generated optimizations
#include = '/etc/postgresql-custom/generated-optimizations.conf'
# User-supplied custom parameters, override any automatically generated ones
#include = '/etc/postgresql-custom/custom-overrides.conf'

# WAL-G specific configurations
#include = '/etc/postgresql-custom/wal-g.conf'

# read replica specific configurations
include = '/etc/postgresql-custom/read-replica.conf'

# supautils specific configurations
#include = '/etc/postgresql-custom/supautils.conf'

#------------------------------------------------------------------------------
# CUSTOMIZED OPTIONS
#------------------------------------------------------------------------------

# Add settings for extensions here
auto_explain.log_min_duration = 10s
cron.database_name = 'postgres'

'''
'''--- /postgres/ansible/files/postgresql_config/pg_hba.conf.j2 ---
# PostgreSQL Client Authentication Configuration File
# ===================================================
#
# Refer to the "Client Authentication" section in the PostgreSQL
# documentation for a complete description of this file.  A short
# synopsis follows.
#
# This file controls: which hosts are allowed to connect, how clients
# are authenticated, which PostgreSQL user names they can use, which
# databases they can access.  Records take one of these forms:
#
# local         DATABASE  USER  METHOD  [OPTIONS]
# host          DATABASE  USER  ADDRESS  METHOD  [OPTIONS]
# hostssl       DATABASE  USER  ADDRESS  METHOD  [OPTIONS]
# hostnossl     DATABASE  USER  ADDRESS  METHOD  [OPTIONS]
# hostgssenc    DATABASE  USER  ADDRESS  METHOD  [OPTIONS]
# hostnogssenc  DATABASE  USER  ADDRESS  METHOD  [OPTIONS]
#
# (The uppercase items must be replaced by actual values.)
#
# The first field is the connection type: "local" is a Unix-domain
# socket, "host" is either a plain or SSL-encrypted TCP/IP socket,
# "hostssl" is an SSL-encrypted TCP/IP socket, and "hostnossl" is a
# non-SSL TCP/IP socket.  Similarly, "hostgssenc" uses a
# GSSAPI-encrypted TCP/IP socket, while "hostnogssenc" uses a
# non-GSSAPI socket.
#
# DATABASE can be "all", "sameuser", "samerole", "replication", a
# database name, or a comma-separated list thereof. The "all"
# keyword does not match "replication". Access to replication
# must be enabled in a separate record (see example below).
#
# USER can be "all", a user name, a group name prefixed with "+", or a
# comma-separated list thereof.  In both the DATABASE and USER fields
# you can also write a file name prefixed with "@" to include names
# from a separate file.
#
# ADDRESS specifies the set of hosts the record matches.  It can be a
# host name, or it is made up of an IP address and a CIDR mask that is
# an integer (between 0 and 32 (IPv4) or 128 (IPv6) inclusive) that
# specifies the number of significant bits in the mask.  A host name
# that starts with a dot (.) matches a suffix of the actual host name.
# Alternatively, you can write an IP address and netmask in separate
# columns to specify the set of hosts.  Instead of a CIDR-address, you
# can write "samehost" to match any of the server's own IP addresses,
# or "samenet" to match any address in any subnet that the server is
# directly connected to.
#
# METHOD can be "trust", "reject", "md5", "password", "scram-sha-256",
# "gss", "sspi", "ident", "peer", "pam", "ldap", "radius" or "cert".
# Note that "password" sends passwords in clear text; "md5" or
# "scram-sha-256" are preferred since they send encrypted passwords.
#
# OPTIONS are a set of options for the authentication in the format
# NAME=VALUE.  The available options depend on the different
# authentication methods -- refer to the "Client Authentication"
# section in the documentation for a list of which options are
# available for which authentication methods.
#
# Database and user names containing spaces, commas, quotes and other
# special characters must be quoted.  Quoting one of the keywords
# "all", "sameuser", "samerole" or "replication" makes the name lose
# its special character, and just match a database or username with
# that name.
#
# This file is read on server startup and when the server receives a
# SIGHUP signal.  If you edit the file on a running system, you have to
# SIGHUP the server for the changes to take effect, run "pg_ctl reload",
# or execute "SELECT pg_reload_conf()".
#
# Put your actual configuration here
# ----------------------------------
#
# If you want to allow non-local connections, you need to add more
# "host" records.  In that case you will also need to make PostgreSQL
# listen on a non-local interface via the listen_addresses
# configuration parameter, or via the -i or -h command line switches.

# TYPE  DATABASE        USER            ADDRESS                 METHOD

# trust local connections
local all  supabase_admin     scram-sha-256
local all  all                peer map=supabase_map
host  all  all  127.0.0.1/32  trust
host  all  all  ::1/128       trust

# IPv4 external connections
host  all  all  10.0.0.0/8  scram-sha-256
host  all  all  172.16.0.0/12  scram-sha-256
host  all  all  192.168.0.0/16  scram-sha-256
host  all  all  0.0.0.0/0     scram-sha-256

# IPv6 external connections
host  all  all  ::0/0     scram-sha-256

'''
'''--- /postgres/ansible/files/postgresql_config/custom_read_replica.conf.j2 ---
# hot_standby = on
# restore_command = '/usr/bin/admin-mgr wal-fetch %f %p >> /var/log/wal-g/wal-fetch.log 2>&1'
# recovery_target_timeline = 'latest'

# primary_conninfo = 'host=localhost port=6543 user=replication'

'''
'''--- /postgres/ansible/files/postgresql_config/postgresql-csvlog.conf ---
# - Where to Log -

log_destination = 'csvlog'		# Valid values are combinations of
					# stderr, csvlog, syslog, and eventlog,
					# depending on platform.  csvlog
					# requires logging_collector to be on.

# This is used when logging to stderr:
logging_collector = on		# Enable capturing of stderr and csvlog
					# into log files. Required to be on for
					# csvlogs.
					# (change requires restart)

# These are only used if logging_collector is on:
log_directory = '/var/log/postgresql'			# directory where log files are written,
					# can be absolute or relative to PGDATA
log_filename = 'postgresql.log'	# log file name pattern,
					# can include strftime() escapes
log_file_mode = 0640			# creation mode for log files,
					# begin with 0 to use octal notation
log_rotation_age = 0			# Automatic rotation of logfiles will
					# happen after that time.  0 disables.
log_rotation_size = 0		# Automatic rotation of logfiles will
					# happen after that much log output.
					# 0 disables.
#log_truncate_on_rotation = off		# If on, an existing log file with the
					# same name as the new log file will be
					# truncated rather than appended to.
					# But such truncation only occurs on
					# time-driven rotation, not on restarts
					# or size-driven rotation.  Default is
					# off, meaning append to existing files
					# in all cases.

'''
'''--- /postgres/ansible/files/postgresql_config/pg_ident.conf.j2 ---
# PostgreSQL User Name Maps
# =========================
#
# Refer to the PostgreSQL documentation, chapter "Client
# Authentication" for a complete description.  A short synopsis
# follows.
#
# This file controls PostgreSQL user name mapping.  It maps external
# user names to their corresponding PostgreSQL user names.  Records
# are of the form:
#
# MAPNAME  SYSTEM-USERNAME  PG-USERNAME
#
# (The uppercase quantities must be replaced by actual values.)
#
# MAPNAME is the (otherwise freely chosen) map name that was used in
# pg_hba.conf.  SYSTEM-USERNAME is the detected user name of the
# client.  PG-USERNAME is the requested PostgreSQL user name.  The
# existence of a record specifies that SYSTEM-USERNAME may connect as
# PG-USERNAME.
#
# If SYSTEM-USERNAME starts with a slash (/), it will be treated as a
# regular expression.  Optionally this can contain a capture (a
# parenthesized subexpression).  The substring matching the capture
# will be substituted for \1 (backslash-one) if present in
# PG-USERNAME.
#
# Multiple maps may be specified in this file and used by pg_hba.conf.
#
# No map names are defined in the default configuration.  If all
# system user names and PostgreSQL user names are the same, you don't
# need anything in this file.
#
# This file is read on server startup and when the postmaster receives
# a SIGHUP signal.  If you edit the file on a running system, you have
# to SIGHUP the postmaster for the changes to take effect.  You can
# use "pg_ctl reload" to do that.

# Put your actual configuration here
# ----------------------------------

# MAPNAME       SYSTEM-USERNAME         PG-USERNAME
supabase_map  postgres   postgres
supabase_map  root       postgres
supabase_map  ubuntu     postgres

# supabase-specific users
supabase_map  gotrue     supabase_auth_admin
supabase_map  postgrest  authenticator
supabase_map  adminapi   postgres

'''
'''--- /postgres/ansible/files/postgresql_config/postgresql-stdout-log.conf ---
logging_collector = off		# Enable capturing of stderr and csvlog
					# into log files. Required to be on for
					# csvlogs.
					# (change requires restart)

'''
'''--- /postgres/ansible/files/postgresql_config/custom_walg.conf.j2 ---
# - Archiving -

#archive_mode = on
#archive_command = '/usr/bin/admin-mgr wal-push %p >> /var/log/wal-g/wal-push.log 2>&1'
#archive_timeout = 120


# - Archive Recovery -

#restore_command = '/usr/bin/admin-mgr wal-fetch %f %p >> /var/log/wal-g/wal-fetch.log 2>&1'

# - Recovery Target -

#recovery_target_lsn = ''
#recovery_target_time = ''
#recovery_target_action = 'promote'
#recovery_target_timeline = 'current'
#recovery_target_inclusive = off

# - Hot Standby -
hot_standby = off

'''
'''--- /postgres/ansible/files/pgbouncer_config/tmpfiles.d-pgbouncer.conf.j2 ---
# Directory for PostgreSQL sockets, lockfiles and stats tempfiles
d /run/pgbouncer 2775 pgbouncer postgres - -
'''
'''--- /postgres/ansible/files/pgbouncer_config/pgbouncer_auth_schema.sql ---
CREATE USER pgbouncer;

REVOKE ALL PRIVILEGES ON SCHEMA public FROM pgbouncer;

CREATE SCHEMA pgbouncer AUTHORIZATION pgbouncer;

CREATE OR REPLACE FUNCTION pgbouncer.get_auth(p_usename TEXT)
RETURNS TABLE(username TEXT, password TEXT) AS
$$
BEGIN
    RAISE WARNING 'PgBouncer auth request: %', p_usename;

    RETURN QUERY
    SELECT usename::TEXT, passwd::TEXT FROM pg_catalog.pg_shadow
    WHERE usename = p_usename;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

REVOKE ALL ON FUNCTION pgbouncer.get_auth(p_usename TEXT) FROM PUBLIC;
GRANT EXECUTE ON FUNCTION pgbouncer.get_auth(p_usename TEXT) TO pgbouncer;

'''
'''--- /postgres/ansible/files/pgbouncer_config/pgbouncer.service.j2 ---
[Unit]
Description=connection pooler for PostgreSQL
Documentation=man:pgbouncer(1)
Documentation=https://www.pgbouncer.org/
After=network.target
{% if supabase_internal is defined %}
Requires=database-optimizations.service
After=database-optimizations.service
{% endif %}

[Service]
Type=notify
User=pgbouncer
ExecStart=/usr/local/bin/pgbouncer /etc/pgbouncer/pgbouncer.ini
ExecReload=/bin/kill -HUP $MAINPID
KillSignal=SIGINT
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/pgbouncer_config/pgbouncer.ini.j2 ---
;;;
;;; PgBouncer configuration file
;;;

;; database name = connect string
;;
;; connect string params:
;;   dbname= host= port= user= password= auth_user=
;;   client_encoding= datestyle= timezone=
;;   pool_size= reserve_pool= max_db_connections=
;;   pool_mode= connect_query= application_name=
[databases]
* = host=localhost auth_user=pgbouncer

;; foodb over Unix socket
;foodb =

;; redirect bardb to bazdb on localhost
;bardb = host=localhost dbname=bazdb

;; access to dest database will go with single user
;forcedb = host=localhost port=300 user=baz password=foo client_encoding=UNICODE datestyle=ISO connect_query='SELECT 1'

;; use custom pool sizes
;nondefaultdb = pool_size=50 reserve_pool=10

;; use auth_user with auth_query if user not present in auth_file
;; auth_user must exist in auth_file
; foodb = auth_user=bar

;; fallback connect string
;* = host=testserver

;; User-specific configuration
[users]

;user1 = pool_mode=transaction max_user_connections=10

;; Configuration section
[pgbouncer]

;;;
;;; Administrative settings
;;;

;logfile = /var/log/pgbouncer.log
pidfile = /var/run/pgbouncer/pgbouncer.pid

;;;
;;; Where to wait for clients
;;;

;; IP address or * which means all IPs
listen_addr = *
listen_port = 6543

;; Unix socket is also used for -R.
;; On Debian it should be /var/run/postgresql
unix_socket_dir = /tmp
;unix_socket_mode = 0777
;unix_socket_group =

;;;
;;; TLS settings for accepting clients
;;;

;; disable, allow, require, verify-ca, verify-full
;client_tls_sslmode = disable

;; Path to file that contains trusted CA certs
;client_tls_ca_file = <system default>

;; Private key and cert to present to clients.
;; Required for accepting TLS connections from clients.
;client_tls_key_file =
;client_tls_cert_file =

;; fast, normal, secure, legacy, <ciphersuite string>
;client_tls_ciphers = fast

;; all, secure, tlsv1.0, tlsv1.1, tlsv1.2, tlsv1.3
;client_tls_protocols = secure

;; none, auto, legacy
;client_tls_dheparams = auto

;; none, auto, <curve name>
;client_tls_ecdhcurve = auto

;;;
;;; TLS settings for connecting to backend databases
;;;

;; disable, allow, require, verify-ca, verify-full
;server_tls_sslmode = disable

;; Path to that contains trusted CA certs
;server_tls_ca_file = <system default>

;; Private key and cert to present to backend.
;; Needed only if backend server require client cert.
;server_tls_key_file =
;server_tls_cert_file =

;; all, secure, tlsv1.0, tlsv1.1, tlsv1.2, tlsv1.3
;server_tls_protocols = secure

;; fast, normal, secure, legacy, <ciphersuite string>
;server_tls_ciphers = fast

;;;
;;; Authentication settings
;;;

;; any, trust, plain, md5, cert, hba, pam
auth_type = scram-sha-256
auth_file = /etc/pgbouncer/userlist.txt

;; Path to HBA-style auth config
;auth_hba_file =

;; Query to use to fetch password from database.  Result
;; must have 2 columns - username and password hash.
auth_query = SELECT * FROM pgbouncer.get_auth($1)

;;;
;;; Users allowed into database 'pgbouncer'
;;;

;; comma-separated list of users who are allowed to change settings
admin_users = pgbouncer

;; comma-separated list of users who are just allowed to use SHOW command
stats_users = pgbouncer

;;;
;;; Pooler personality questions
;;;

;; When server connection is released back to pool:
;;   session      - after client disconnects (default)
;;   transaction  - after transaction finishes
;;   statement    - after statement finishes
pool_mode = transaction

;; Query for cleaning connection immediately after releasing from
;; client.  No need to put ROLLBACK here, pgbouncer does not reuse
;; connections where transaction is left open.
;server_reset_query = DISCARD ALL

;; Whether server_reset_query should run in all pooling modes.  If it
;; is off, server_reset_query is used only for session-pooling.
;server_reset_query_always = 0

;; Comma-separated list of parameters to ignore when given in startup
;; packet.  Newer JDBC versions require the extra_float_digits here.
ignore_startup_parameters = extra_float_digits

;; When taking idle server into use, this query is run first.
;server_check_query = select 1

;; If server was used more recently that this many seconds ago,
; skip the check query.  Value 0 may or may not run in immediately.
;server_check_delay = 30

;; Close servers in session pooling mode after a RECONNECT, RELOAD,
;; etc. when they are idle instead of at the end of the session.
;server_fast_close = 0

;; Use <appname - host> as application_name on server.
;application_name_add_host = 0

;; Period for updating aggregated stats.
;stats_period = 60

;;;
;;; Connection limits
;;;

;; Total number of clients that can connect
;max_client_conn = 100

;; Default pool size.  20 is good number when transaction pooling
;; is in use, in session pooling it needs to be the number of
;; max clients you want to handle at any moment
default_pool_size = 15

;; Minimum number of server connections to keep in pool.
;min_pool_size = 0

; how many additional connection to allow in case of trouble
;reserve_pool_size = 0

;; If a clients needs to wait more than this many seconds, use reserve
;; pool.
;reserve_pool_timeout = 5

;; Maximum number of server connections for a database
;max_db_connections = 0

;; Maximum number of server connections for a user
;max_user_connections = 0

;; If off, then server connections are reused in LIFO manner
;server_round_robin = 0

;;;
;;; Logging
;;;

;; Syslog settings
;syslog = 0
;syslog_facility = daemon
;syslog_ident = pgbouncer

;; log if client connects or server connection is made
;log_connections = 1

;; log if and why connection was closed
;log_disconnections = 1

;; log error messages pooler sends to clients
;log_pooler_errors = 1

;; write aggregated stats into log
;log_stats = 1

;; Logging verbosity.  Same as -v switch on command line.
;verbose = 0

;;;
;;; Timeouts
;;;

;; Close server connection if its been connected longer.
;server_lifetime = 3600

;; Close server connection if its not been used in this time.  Allows
;; to clean unnecessary connections from pool after peak.
;server_idle_timeout = 600

;; Cancel connection attempt if server does not answer takes longer.
;server_connect_timeout = 15

;; If server login failed (server_connect_timeout or auth failure)
;; then wait this many second.
;server_login_retry = 15

;; Dangerous.  Server connection is closed if query does not return in
;; this time.  Should be used to survive network problems, _not_ as
;; statement_timeout. (default: 0)
;query_timeout = 0

;; Dangerous.  Client connection is closed if the query is not
;; assigned to a server in this time.  Should be used to limit the
;; number of queued queries in case of a database or network
;; failure. (default: 120)
;query_wait_timeout = 120

;; Dangerous.  Client connection is closed if no activity in this
;; time.  Should be used to survive network problems. (default: 0)
;client_idle_timeout = 0

;; Disconnect clients who have not managed to log in after connecting
;; in this many seconds.
;client_login_timeout = 60

;; Clean automatically created database entries (via "*") if they stay
;; unused in this many seconds.
; autodb_idle_timeout = 3600

;; Close connections which are in "IDLE in transaction" state longer
;; than this many seconds.
;idle_transaction_timeout = 0

;; How long SUSPEND/-R waits for buffer flush before closing
;; connection.
;suspend_timeout = 10

;;;
;;; Low-level tuning options
;;;

;; buffer for streaming packets
;pkt_buf = 4096

;; man 2 listen
;listen_backlog = 128

;; Max number pkt_buf to process in one event loop.
;sbuf_loopcnt = 5

;; Maximum PostgreSQL protocol packet size.
;max_packet_size = 2147483647

;; Set SO_REUSEPORT socket option
;so_reuseport = 0

;; networking options, for info: man 7 tcp

;; Linux: Notify program about new connection only if there is also
;; data received.  (Seconds to wait.)  On Linux the default is 45, on
;; other OS'es 0.
;tcp_defer_accept = 0

;; In-kernel buffer size (Linux default: 4096)
;tcp_socket_buffer = 0

;; whether tcp keepalive should be turned on (0/1)
;tcp_keepalive = 1

;; The following options are Linux-specific.  They also require
;; tcp_keepalive=1.

;; Count of keepalive packets
;tcp_keepcnt = 0

;; How long the connection can be idle before sending keepalive
;; packets
;tcp_keepidle = 0

;; The time between individual keepalive probes
;tcp_keepintvl = 0

;; How long may transmitted data remain unacknowledged before TCP
;; connection is closed (in milliseconds)
;tcp_user_timeout = 0

;; DNS lookup caching time
;dns_max_ttl = 15

;; DNS zone SOA lookup period
;dns_zone_check_period = 0

;; DNS negative result caching time
;dns_nxdomain_ttl = 15

;; Custom resolv.conf file, to set custom DNS servers or other options
;; (default: empty = use OS settings)
;resolv_conf = /etc/pgbouncer/resolv.conf

;;;
;;; Random stuff
;;;

;; Hackish security feature.  Helps against SQL injection: when PQexec
;; is disabled, multi-statement cannot be made.
;disable_pqexec = 0

;; Config file to use for next RELOAD/SIGHUP
;; By default contains config file from command line.
;conffile

;; Windows service name to register as.  job_name is alias for
;; service_name, used by some Skytools scripts.
;service_name = pgbouncer
;job_name = pgbouncer

;; Read additional config from other file
;%include /etc/pgbouncer/pgbouncer-other.ini

%include /etc/pgbouncer-custom/generated-optimizations.ini
%include /etc/pgbouncer-custom/custom-overrides.ini
%include /etc/pgbouncer-custom/ssl-config.ini

'''
'''--- /postgres/ansible/files/admin_api_scripts/grow_fs.sh ---
#! /usr/bin/env bash

set -euo pipefail

VOLUME_TYPE=${1:-data}

if pgrep resizefs; then
    echo "resize2fs is already running"
    exit 1
fi

if [ -b /dev/nvme1n1 ] ; then
    if [[ "${VOLUME_TYPE}" == "data" ]]; then
        resize2fs /dev/nvme1n1

    elif [[ "${VOLUME_TYPE}" == "root" ]] ; then
        PLACEHOLDER_FL=/home/ubuntu/50M_PLACEHOLDER
        rm -f "${PLACEHOLDER_FL}" || true
        growpart /dev/nvme0n1 2
        resize2fs /dev/nvme0n1p2
        if [[ ! -f "${PLACEHOLDER_FL}" ]] ; then
            fallocate -l50M "${PLACEHOLDER_FL}"
        fi
    else
        echo "Invalid disk specified: ${VOLUME_TYPE}"
        exit 1
    fi
else
    growpart /dev/nvme0n1 2
    resize2fs /dev/nvme0n1p2
fi
echo "Done resizing disk"

'''
'''--- /postgres/ansible/files/admin_api_scripts/manage_readonly_mode.sh ---
#! /usr/bin/env bash

set -euo pipefail

SUBCOMMAND=$1

function set_mode {
    MODE=$1
    psql -h localhost -U supabase_admin -d postgres -c "ALTER SYSTEM SET default_transaction_read_only to ${MODE};"
    psql -h localhost -U supabase_admin -d postgres -c "SELECT pg_reload_conf();"
}

function check_override {
    COMMAND=$(cat <<EOF
WITH role_comment as (
    SELECT pg_catalog.shobj_description(r.oid, 'pg_authid') AS content
    FROM pg_catalog.pg_roles r
    WHERE r.rolname = 'postgres'
)
SELECT
    CASE
           WHEN role_comment.content LIKE 'readonly mode overridden until%' THEN
                   (NOW() < to_timestamp(role_comment.content, '"readonly mode overridden until "YYYY-MM-DD\THH24:MI:SS'))::int
           ELSE 0
           END as override_active
FROM role_comment;
EOF
)
    RESULT=$(psql -h localhost -U supabase_admin -d postgres -At -c "$COMMAND")
    echo -n "$RESULT"
}

case $SUBCOMMAND in
    "check_override")
        check_override
        ;;
    "set")
       shift
        set_mode "$@"
        ;;
    *)
        echo "Error: '$SUBCOMMAND' is not a known subcommand."
        exit 1
        ;;
esac
'''
'''--- /postgres/ansible/files/admin_api_scripts/pg_egress_collect.pl ---
#!/usr/bin/env perl

# This script receive tcpdump output through STDIN and does:
#
# 1. extract outgoing TCP packet length on the 1st non-loopback device port 5432 and 6543
# 2. sum the length up to one minute
# 3. save the total length to file (default is /tmp/pg_egress_collect.txt) per minute
#
# Usage:
#
# tcpdump -s 128 -Q out -nn -tt -vv -p -l 'tcp and (port 5432 or port 6543)' | perl pg_egress_collect.pl -o /tmp/output.txt
#

use POSIX;
use List::Util qw(sum);
use Getopt::Long 'HelpMessage';
use IO::Async::Loop;
use IO::Async::Stream;
use IO::Async::Timer::Periodic;

use strict;
use warnings;

# total captured packets lenth in a time frame
my $captured_len = 0;

# extract tcp packet length captured by tcpdump
#
# Sample IPv4 input lines:
#
# 1674013833.940253 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 60)
#     10.112.101.122.5432 > 220.235.16.223.62599: Flags [S.], cksum 0x5de3 (incorrect -> 0x63da), seq 2314200657, ack 2071735457, win 62643, options [mss 8961,sackOK,TS val 3358598837 ecr 1277499190,nop,wscale 7], length 0
# 1674013833.989257 IP (tos 0x0, ttl 64, id 24975, offset 0, flags [DF], proto TCP (6), length 52)
#     10.112.101.122.5432 > 220.235.16.223.62599: Flags [.], cksum 0x5ddb (incorrect -> 0xa25b), seq 1, ack 9, win 490, options [nop,nop,TS val 3358598885 ecr 1277499232], length 0
#
# Sample IPv6 input lines:
#
# 1706483718.836526 IP6 (flowlabel 0x0bf27, hlim 64, next-header TCP (6) payload length: 125) 2406:da18:4fd:9b00:959:c52:ce68:10c8.5432 > 2406:da12:d78:f501:1273:296c:2482:c7a7.50530: Flags [P.], seq 25:118, ack 125, win 488, options [nop,nop,TS val 1026340732 ecr 1935666426], length 93
# 1706483718.912083 IP6 (flowlabel 0x0bf27, hlim 64, next-header TCP (6) payload length: 501) 2406:da18:4fd:9b00:959:c52:ce68:10c8.5432 > 2406:da12:d78:f501:1273:296c:2482:c7a7.50530: Flags [P.], seq 118:587, ack 234, win 488, options [nop,nop,TS val 1026340807 ecr 1935666497], length 469
# 1706483718.984001 IP6 (flowlabel 0x0bf27, hlim 64, next-header TCP (6) payload length: 151) 2406:da18:4fd:9b00:959:c52:ce68:10c8.5432 > 2406:da12:d78:f501:1273:296c:2482:c7a7.50530: Flags [P.], seq 587:706, ack 448, win 487, options [nop,nop,TS val 1026340879 ecr 1935666569], length 119
sub extract_packet_length {
    my ($line) = @_;

    #print("debug: >> " . $line);

    if ($line =~ /^.*, length (\d+)$/) {
        # extract tcp packet length and add it up
        my $len = $1;
        $captured_len += $len;
    }
}

# write total length to file
sub write_file {
    my ($output) = @_;

    my $now = strftime "%F %T", localtime time;
    print "[$now] write captured len $captured_len to $output\n";

    open(my $fh, "+>", $output) or die "Could not open file '$output' $!";
    print $fh "$captured_len";
    close($fh) or die "Could not write file '$output' $!";
}

# main
sub main {
    # get arguments
    GetOptions(
        "interval:i"    => \(my $interval = 60),
        "output:s"      => \(my $output = "/tmp/pg_egress_collect.txt"),
        "help"          => sub { HelpMessage(0) },
    ) or HelpMessage(1);

    my $loop = IO::Async::Loop->new;

    # tcpdump extractor
    my $extractor = IO::Async::Stream->new_for_stdin(
        on_read => sub {
            my ($self, $buffref, $eof) = @_;

            while($$buffref =~ s/^(.*\n)//) {
                my $line = $1;
                extract_packet_length($line);
            }

            return 0;
        },
    );

    # schedule file writer per minute
    my $writer = IO::Async::Timer::Periodic->new(
        interval => $interval,
        on_tick => sub {
            write_file($output);

            # reset total captured length
            $captured_len = 0;
        },
    );
    $writer->start;

    print "pg_egress_collect started, egress data will be saved to $output at interval $interval seconds.\n";

    $loop->add($extractor);
    $loop->add($writer);
    $loop->run;
}

main();

__END__

=head1 NAME

pg_egress_collect.pl - collect egress from tcpdump output, extract TCP packet length, aggregate in specified interval and write to output file.

=head1 SYNOPSIS

pg_egress_collect.pl [-i interval] [-o output]

Options:

    -i, --interval interval
        output file write interval, in seconds, default is 60 seconds

    -o, --output output
        output file path, default is /tmp/pg_egress_collect.txt

    -h, --help
        print this help message

=cut

'''
'''--- /postgres/ansible/files/admin_api_scripts/pg_upgrade_scripts/initiate.sh ---
#! /usr/bin/env bash

## This script is run on the old (source) instance, mounting the data disk
## of the newly launched instance, disabling extensions containing regtypes,
## and running pg_upgrade.
## It reports the current status of the upgrade process to /tmp/pg-upgrade-status,
## which can then be subsequently checked through check.sh.

# Extensions to disable before running pg_upgrade.
# Running an upgrade with these extensions enabled will result in errors due to
# them depending on regtypes referencing system OIDs or outdated library files.
EXTENSIONS_TO_DISABLE=(
    "pg_graphql"
    "pg_stat_monitor"
)

PG14_EXTENSIONS_TO_DISABLE=(
    "wrappers"
    "pgrouting"
)

PG13_EXTENSIONS_TO_DISABLE=(
    "pgrouting"
)

set -eEuo pipefail

SCRIPT_DIR=$(dirname -- "$0";)
# shellcheck disable=SC1091
source "$SCRIPT_DIR/common.sh"

IS_CI=${IS_CI:-}
IS_LOCAL_UPGRADE=${IS_LOCAL_UPGRADE:-}
IS_NIX_UPGRADE=${IS_NIX_UPGRADE:-}
IS_NIX_BASED_SYSTEM="false"

PGVERSION=$1
MOUNT_POINT="/data_migration"
LOG_FILE="/var/log/pg-upgrade-initiate.log"

POST_UPGRADE_EXTENSION_SCRIPT="/tmp/pg_upgrade/pg_upgrade_extensions.sql"
POST_UPGRADE_POSTGRES_PERMS_SCRIPT="/tmp/pg_upgrade/pg_upgrade_postgres_perms.sql"
OLD_PGVERSION=$(run_sql -A -t -c "SHOW server_version;")

SERVER_LC_COLLATE=$(run_sql -A -t -c "SHOW lc_collate;")
SERVER_LC_CTYPE=$(run_sql -A -t -c "SHOW lc_ctype;")
SERVER_ENCODING=$(run_sql -A -t -c "SHOW server_encoding;")

POSTGRES_CONFIG_PATH="/etc/postgresql/postgresql.conf"
PGBINOLD="/usr/lib/postgresql/bin"

PG_UPGRADE_BIN_DIR="/tmp/pg_upgrade_bin/$PGVERSION"
NIX_INSTALLER_PATH="/tmp/persistent/nix-installer"
NIX_INSTALLER_PACKAGE_PATH="$NIX_INSTALLER_PATH.tar.gz"

if [ -L "$PGBINOLD/pg_upgrade" ]; then
    BINARY_PATH=$(readlink -f "$PGBINOLD/pg_upgrade")
    if [[ "$BINARY_PATH" == *"nix"* ]]; then
        IS_NIX_BASED_SYSTEM="true"
    fi
fi

# If upgrading from older major PG versions, disable specific extensions
if [[ "$OLD_PGVERSION" =~ ^14.* ]]; then
    EXTENSIONS_TO_DISABLE+=("${PG14_EXTENSIONS_TO_DISABLE[@]}")
elif [[ "$OLD_PGVERSION" =~ ^13.* ]]; then
    EXTENSIONS_TO_DISABLE+=("${PG13_EXTENSIONS_TO_DISABLE[@]}")
elif [[ "$OLD_PGVERSION" =~ ^12.* ]]; then
    POSTGRES_CONFIG_PATH="/etc/postgresql/12/main/postgresql.conf"
    PGBINOLD="/usr/lib/postgresql/12/bin"
fi

if [ -n "$IS_CI" ]; then
    PGBINOLD="$(pg_config --bindir)"
    echo "Running in CI mode; using pg_config bindir: $PGBINOLD"
    echo "PGVERSION: $PGVERSION"
fi

OLD_BOOTSTRAP_USER=$(run_sql -A -t -c "select rolname from pg_authid where oid = 10;")

cleanup() {
    UPGRADE_STATUS=${1:-"failed"}
    EXIT_CODE=${?:-0}

    if [ "$UPGRADE_STATUS" = "failed" ]; then
        EXIT_CODE=1
    fi

    if [ "$UPGRADE_STATUS" = "failed" ]; then
        echo "Upgrade job failed. Cleaning up and exiting."
    fi

    if [ -d "${MOUNT_POINT}/pgdata/pg_upgrade_output.d/" ]; then
        echo "Copying pg_upgrade output to /var/log"
        cp -R "${MOUNT_POINT}/pgdata/pg_upgrade_output.d/" /var/log/ || true
        chown -R postgres:postgres /var/log/pg_upgrade_output.d/
        chmod -R 0750 /var/log/pg_upgrade_output.d/
        ship_logs "$LOG_FILE" || true
        tail -n +1 /var/log/pg_upgrade_output.d/*/* > /var/log/pg_upgrade_output.d/pg_upgrade.log || true
        ship_logs "/var/log/pg_upgrade_output.d/pg_upgrade.log" || true
    fi

    if [ -L "/usr/share/postgresql/${PGVERSION}" ]; then
        rm "/usr/share/postgresql/${PGVERSION}"

        if [ -f "/usr/share/postgresql/${PGVERSION}.bak" ]; then
            mv "/usr/share/postgresql/${PGVERSION}.bak" "/usr/share/postgresql/${PGVERSION}"
        fi

        if [ -d "/usr/share/postgresql/${PGVERSION}.bak" ]; then
            mv "/usr/share/postgresql/${PGVERSION}.bak" "/usr/share/postgresql/${PGVERSION}"
        fi
    fi

    echo "Restarting postgresql"
    if [ -z "$IS_CI" ]; then
        systemctl enable postgresql
        retry 5 systemctl restart postgresql
    else
        CI_start_postgres
    fi

    retry 8 pg_isready -h localhost -U supabase_admin

    echo "Re-enabling extensions"
    if [ -f $POST_UPGRADE_EXTENSION_SCRIPT ]; then
        retry 5 run_sql -f $POST_UPGRADE_EXTENSION_SCRIPT
    fi

    echo "Removing SUPERUSER grant from postgres"
    retry 5 run_sql -c "ALTER USER postgres WITH NOSUPERUSER;"

    echo "Resetting postgres database connection limit"
    retry 5 run_sql -c "ALTER DATABASE postgres CONNECTION LIMIT -1;"

    echo "Making sure postgres still has access to pg_shadow"
    cat << EOF >> $POST_UPGRADE_POSTGRES_PERMS_SCRIPT
DO \$\$
begin
  if exists (select from pg_authid where rolname = 'pg_read_all_data') then
    execute('grant pg_read_all_data to postgres');
  end if;
end
\$\$;
grant pg_signal_backend to postgres;
EOF

    if [ -f $POST_UPGRADE_POSTGRES_PERMS_SCRIPT ]; then
        retry 5 run_sql -f $POST_UPGRADE_POSTGRES_PERMS_SCRIPT
    fi

    if [ -z "$IS_CI" ] && [ -z "$IS_LOCAL_UPGRADE" ]; then
        echo "Unmounting data disk from ${MOUNT_POINT}"
        retry 3 umount $MOUNT_POINT
    fi
    echo "$UPGRADE_STATUS" > /tmp/pg-upgrade-status

    if [ -z "$IS_CI" ]; then
        exit "$EXIT_CODE"
    else 
        echo "CI run complete with code ${EXIT_CODE}. Exiting."
        exit "$EXIT_CODE"
    fi
}

function handle_extensions {
    if [ -z "$IS_CI" ]; then
        retry 5 systemctl restart postgresql
    else
        CI_start_postgres
    fi

    retry 8 pg_isready -h localhost -U supabase_admin

    rm -f $POST_UPGRADE_EXTENSION_SCRIPT
    touch $POST_UPGRADE_EXTENSION_SCRIPT

    PASSWORD_ENCRYPTION_SETTING=$(run_sql -A -t -c "SHOW password_encryption;")
    if [ "$PASSWORD_ENCRYPTION_SETTING" = "md5" ]; then
        echo "ALTER SYSTEM SET password_encryption = 'md5';" >> $POST_UPGRADE_EXTENSION_SCRIPT
    fi

    cat << EOF >> $POST_UPGRADE_EXTENSION_SCRIPT
ALTER SYSTEM SET jit = off;
SELECT pg_reload_conf();
EOF

    # Disable extensions if they're enabled
    # Generate SQL script to re-enable them after upgrade
    for EXTENSION in "${EXTENSIONS_TO_DISABLE[@]}"; do
        EXTENSION_ENABLED=$(run_sql -A -t -c "SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = '${EXTENSION}');")
        if [ "$EXTENSION_ENABLED" = "t" ]; then
            echo "Disabling extension ${EXTENSION}"
            run_sql -c "DROP EXTENSION IF EXISTS ${EXTENSION} CASCADE;"
            cat << EOF >> $POST_UPGRADE_EXTENSION_SCRIPT
DO \$\$
BEGIN
    IF EXISTS (SELECT 1 FROM pg_available_extensions WHERE name = '${EXTENSION}') THEN
        CREATE EXTENSION IF NOT EXISTS ${EXTENSION} CASCADE;
    END IF;
END;
\$\$;
EOF
        fi
    done
}

function initiate_upgrade {
    mkdir -p "$MOUNT_POINT"
    SHARED_PRELOAD_LIBRARIES=$(cat "$POSTGRES_CONFIG_PATH" | grep shared_preload_libraries | sed "s/shared_preload_libraries =\s\{0,1\}'\(.*\)'.*/\1/")

    # Wrappers officially launched in PG15; PG14 version is incompatible
    if [[ "$OLD_PGVERSION" =~ 14* ]]; then
        SHARED_PRELOAD_LIBRARIES=$(echo "$SHARED_PRELOAD_LIBRARIES" | sed "s/wrappers//" | xargs)
    fi
    SHARED_PRELOAD_LIBRARIES=$(echo "$SHARED_PRELOAD_LIBRARIES" | sed "s/pg_cron//" | xargs)
    SHARED_PRELOAD_LIBRARIES=$(echo "$SHARED_PRELOAD_LIBRARIES" | sed "s/pg_net//" | xargs)
    SHARED_PRELOAD_LIBRARIES=$(echo "$SHARED_PRELOAD_LIBRARIES" | sed "s/check_role_membership//" | xargs)
    SHARED_PRELOAD_LIBRARIES=$(echo "$SHARED_PRELOAD_LIBRARIES" | sed "s/safeupdate//" | xargs)

    # Exclude empty-string entries, as well as leading/trailing commas and spaces resulting from the above lib exclusions
    #  i.e. " , pg_stat_statements, , pgsodium, " -> "pg_stat_statements, pgsodium"
    SHARED_PRELOAD_LIBRARIES=$(echo "$SHARED_PRELOAD_LIBRARIES" | tr ',' ' ' | tr -s ' ' | tr ' ' ', ')

    # Account for trailing comma
    # eg. "...,auto_explain,pg_tle,plan_filter," -> "...,auto_explain,pg_tle,plan_filter"
    if [[ "${SHARED_PRELOAD_LIBRARIES: -1}" = "," ]]; then
        # clean up trailing comma
        SHARED_PRELOAD_LIBRARIES=$(echo "$SHARED_PRELOAD_LIBRARIES" | sed "s/.$//" | xargs)
    fi

    PGDATAOLD=$(cat "$POSTGRES_CONFIG_PATH" | grep data_directory | sed "s/data_directory = '\(.*\)'.*/\1/")

    PGDATANEW="$MOUNT_POINT/pgdata"

    # running upgrade using at least 1 cpu core
    WORKERS=$(nproc | awk '{ print ($1 == 1 ? 1 : $1 - 1) }')

    # To make nix-based upgrades work for testing, create a pg binaries tarball with the following contents:
    #  - nix_flake_version - a7189a68ed4ea78c1e73991b5f271043636cf074
    # Where the value is the commit hash of the nix flake that contains the binaries

    if [ -n "$IS_LOCAL_UPGRADE" ]; then
        mkdir -p "$PG_UPGRADE_BIN_DIR"
        mkdir -p /tmp/persistent/
        echo "a7189a68ed4ea78c1e73991b5f271043636cf074" > "$PG_UPGRADE_BIN_DIR/nix_flake_version"
        tar -czf "/tmp/persistent/pg_upgrade_bin.tar.gz" -C "/tmp/pg_upgrade_bin" .
        rm -rf /tmp/pg_upgrade_bin/
    fi
    
    echo "1. Extracting pg_upgrade binaries"
    mkdir -p "/tmp/pg_upgrade_bin"
    tar zxf "/tmp/persistent/pg_upgrade_bin.tar.gz" -C "/tmp/pg_upgrade_bin"

    PGSHARENEW="$PG_UPGRADE_BIN_DIR/share"

    if [ -f "$PG_UPGRADE_BIN_DIR/nix_flake_version" ]; then
        IS_NIX_UPGRADE="true"
        NIX_FLAKE_VERSION=$(cat "$PG_UPGRADE_BIN_DIR/nix_flake_version")

        if [ "$IS_NIX_BASED_SYSTEM" = "false" ]; then
            if [ ! -f /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh ]; then            
                if ! command -v nix > /dev/null; then
                    echo "1.1. Nix is not installed; installing."

                    if [ -f "$NIX_INSTALLER_PACKAGE_PATH" ]; then
                        echo "1.1.1. Installing Nix using the provided installer"
                        tar -xzf "$NIX_INSTALLER_PACKAGE_PATH" -C /tmp/persistent/
                        chmod +x "$NIX_INSTALLER_PATH"
                        "$NIX_INSTALLER_PATH" install --no-confirm \
                        --extra-conf "substituters = https://cache.nixos.org https://nix-postgres-artifacts.s3.amazonaws.com" \
                        --extra-conf "trusted-public-keys = nix-postgres-artifacts:dGZlQOvKcNEjvT7QEAJbcV6b6uk7VF/hWMjhYleiaLI=% cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY="
                    else
                        echo "1.1.1. Installing Nix using the official installer"

                        curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install --no-confirm \
                        --extra-conf "substituters = https://cache.nixos.org https://nix-postgres-artifacts.s3.amazonaws.com" \
                        --extra-conf "trusted-public-keys = nix-postgres-artifacts:dGZlQOvKcNEjvT7QEAJbcV6b6uk7VF/hWMjhYleiaLI=% cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY="
                    fi
                else 
                    echo "1.1. Nix is installed; moving on."
                fi
            fi
        fi

        echo "1.2. Installing flake revision: $NIX_FLAKE_VERSION"
        # shellcheck disable=SC1091
        source /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh
        nix-collect-garbage -d > /tmp/pg_upgrade-nix-gc.log 2>&1 || true
        PG_UPGRADE_BIN_DIR=$(nix build "github:supabase/postgres/${NIX_FLAKE_VERSION}#psql_15/bin" --no-link --print-out-paths --extra-experimental-features nix-command --extra-experimental-features flakes)
        PGSHARENEW="$PG_UPGRADE_BIN_DIR/share/postgresql"
    fi

    PGBINNEW="$PG_UPGRADE_BIN_DIR/bin"
    PGLIBNEW="$PG_UPGRADE_BIN_DIR/lib"

    # copy upgrade-specific pgsodium_getkey script into the share dir
    chmod +x "$SCRIPT_DIR/pgsodium_getkey.sh"
    mkdir -p "$PGSHARENEW/extension"
    cp  "$SCRIPT_DIR/pgsodium_getkey.sh" "$PGSHARENEW/extension/pgsodium_getkey"
    if [ -d "/var/lib/postgresql/extension/" ]; then
        cp  "$SCRIPT_DIR/pgsodium_getkey.sh" "/var/lib/postgresql/extension/pgsodium_getkey"
        chown postgres:postgres "/var/lib/postgresql/extension/pgsodium_getkey"
    fi

    chown -R postgres:postgres "/tmp/pg_upgrade_bin/$PGVERSION"

    # upgrade job outputs a log in the cwd; needs write permissions
    mkdir -p /tmp/pg_upgrade/
    chown -R postgres:postgres /tmp/pg_upgrade/
    cd /tmp/pg_upgrade/

    # Fixing erros generated by previous dpkg executions (package upgrades et co)
    echo "2. Fixing potential errors generated by dpkg"
    DEBIAN_FRONTEND=noninteractive dpkg --configure -a --force-confold || true # handle errors generated by dpkg

    # Needed for PostGIS, since it's compiled with Protobuf-C support now
    echo "3. Installing libprotobuf-c1 and libicu66 if missing"
    if [[ ! "$(apt list --installed libprotobuf-c1 | grep "installed")" ]]; then
        apt-get update -y
        apt --fix-broken install -y libprotobuf-c1 libicu66 || true
    fi

    echo "4. Setup locale if required"
    if ! grep -q "^en_US.UTF-8" /etc/locale.gen ; then
        echo "en_US.UTF-8 UTF-8" >> /etc/locale.gen
    fi
    if ! grep -q "^C.UTF-8" /etc/locale.gen ; then
        echo "C.UTF-8 UTF-8" >> /etc/locale.gen
    fi
    locale-gen

    if [ -z "$IS_CI" ] && [ -z "$IS_LOCAL_UPGRADE" ]; then
        # awk NF==3 prints lines with exactly 3 fields, which are the block devices currently not mounted anywhere
        # excluding nvme0 since it is the root disk
        echo "5. Determining block device to mount"
        BLOCK_DEVICE=$(lsblk -dprno name,size,mountpoint,type | grep "disk" | grep -v "nvme0" | awk 'NF==3 { print $1; }')
        echo "Block device found: $BLOCK_DEVICE"

        mkdir -p "$MOUNT_POINT"
        echo "6. Mounting block device"

        sleep 5
        e2fsck -pf "$BLOCK_DEVICE"

        sleep 1
        mount "$BLOCK_DEVICE" "$MOUNT_POINT"

        sleep 1
        resize2fs "$BLOCK_DEVICE"
    else 
        mkdir -p "$MOUNT_POINT"
    fi

    if [ -f "$MOUNT_POINT/pgsodium_root.key" ]; then
        cp "$MOUNT_POINT/pgsodium_root.key" /etc/postgresql-custom/pgsodium_root.key
        chown postgres:postgres /etc/postgresql-custom/pgsodium_root.key
        chmod 600 /etc/postgresql-custom/pgsodium_root.key
    fi

    echo "7. Disabling extensions and generating post-upgrade script"
    handle_extensions

    echo "8.1. Granting SUPERUSER to postgres user"
    run_sql -c "ALTER USER postgres WITH SUPERUSER;"

    if [ "$OLD_BOOTSTRAP_USER" = "postgres" ]; then
        echo "8.2. Swap postgres & supabase_admin roles as we're upgrading a project with postgres as bootstrap user"
        swap_postgres_and_supabase_admin
    fi

    if [ -z "$IS_NIX_UPGRADE" ]; then
        if [ -d "/usr/share/postgresql/${PGVERSION}" ]; then
            mv "/usr/share/postgresql/${PGVERSION}" "/usr/share/postgresql/${PGVERSION}.bak"
        fi

        ln -s "$PGSHARENEW" "/usr/share/postgresql/${PGVERSION}"
        cp --remove-destination "$PGLIBNEW"/*.control "$PGSHARENEW/extension/"
        cp --remove-destination "$PGLIBNEW"/*.sql "$PGSHARENEW/extension/"

        export LD_LIBRARY_PATH="${PGLIBNEW}"
    fi

    echo "9. Creating new data directory, initializing database"
    chown -R postgres:postgres "$MOUNT_POINT/"
    rm -rf "${PGDATANEW:?}/"

    if [ "$IS_NIX_UPGRADE" = "true" ]; then
        LC_ALL=en_US.UTF-8 LC_CTYPE=$SERVER_LC_CTYPE LC_COLLATE=$SERVER_LC_COLLATE LANGUAGE=en_US.UTF-8 LANG=en_US.UTF-8 LOCALE_ARCHIVE=/usr/lib/locale/locale-archive su -c ". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && $PGBINNEW/initdb --encoding=$SERVER_ENCODING --lc-collate=$SERVER_LC_COLLATE --lc-ctype=$SERVER_LC_CTYPE -L $PGSHARENEW -D $PGDATANEW/ --username=supabase_admin" -s "$SHELL" postgres
    else
        su -c "$PGBINNEW/initdb -L $PGSHARENEW -D $PGDATANEW/ --username=supabase_admin" -s "$SHELL" postgres
    fi

    # This line avoids the need to supply the supabase_admin password on the old
    # instance, since pg_upgrade connects to the db as supabase_admin using unix
    # sockets, which is gated behind scram-sha-256 per pg_hba.conf.j2. The new
    # instance is unaffected.
    if ! grep -q "local all supabase_admin trust" /etc/postgresql/pg_hba.conf; then
        echo "local all supabase_admin trust
$(cat /etc/postgresql/pg_hba.conf)" > /etc/postgresql/pg_hba.conf
        run_sql -c "select pg_reload_conf();"
    fi

    UPGRADE_COMMAND=$(cat <<EOF
    time ${PGBINNEW}/pg_upgrade \
    --old-bindir="${PGBINOLD}" \
    --new-bindir=${PGBINNEW} \
    --old-datadir=${PGDATAOLD} \
    --new-datadir=${PGDATANEW} \
    --username=supabase_admin \
    --jobs="${WORKERS}" -r \
    --old-options='-c config_file=${POSTGRES_CONFIG_PATH}' \
    --old-options="-c shared_preload_libraries='${SHARED_PRELOAD_LIBRARIES}'" \
    --new-options="-c data_directory=${PGDATANEW}" \
    --new-options="-c shared_preload_libraries='${SHARED_PRELOAD_LIBRARIES}'"
EOF
    )

    if [ "$IS_NIX_BASED_SYSTEM" = "true" ]; then
        UPGRADE_COMMAND=". /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh && $UPGRADE_COMMAND"
    fi 
    GRN_PLUGINS_DIR=/var/lib/postgresql/.nix-profile/lib/groonga/plugins LC_ALL=en_US.UTF-8 LC_CTYPE=$SERVER_LC_CTYPE LC_COLLATE=$SERVER_LC_COLLATE LANGUAGE=en_US.UTF-8 LANG=en_US.UTF-8 LOCALE_ARCHIVE=/usr/lib/locale/locale-archive su -pc "$UPGRADE_COMMAND --check" -s "$SHELL" postgres

    echo "10. Stopping postgres; running pg_upgrade"
    # Extra work to ensure postgres is actually stopped
    #  Mostly needed for PG12 projects with odd systemd unit behavior
    if [ -z "$IS_CI" ]; then
        retry 5 systemctl restart postgresql
        systemctl disable postgresql
        retry 5 systemctl stop postgresql

        sleep 3
        systemctl stop postgresql
    else
        CI_stop_postgres
    fi

    GRN_PLUGINS_DIR=/var/lib/postgresql/.nix-profile/lib/groonga/plugins LC_ALL=en_US.UTF-8 LC_CTYPE=$SERVER_LC_CTYPE LC_COLLATE=$SERVER_LC_COLLATE LANGUAGE=en_US.UTF-8 LANG=en_US.UTF-8 LOCALE_ARCHIVE=/usr/lib/locale/locale-archive su -pc "$UPGRADE_COMMAND" -s "$SHELL" postgres

    # copying custom configurations
    echo "11. Copying custom configurations"
    mkdir -p "$MOUNT_POINT/conf"
    cp -R /etc/postgresql-custom/* "$MOUNT_POINT/conf/"
    # removing supautils config as to allow the latest one provided by the latest image to be used
    rm -f "$MOUNT_POINT/conf/supautils.conf" || true
    rm -rf "$MOUNT_POINT/conf/extension-custom-scripts" || true

    # removing wal-g config as to allow it to be explicitly enabled on the new instance
    rm -f "$MOUNT_POINT/conf/wal-g.conf"

    # copy sql files generated by pg_upgrade
    echo "12. Copying sql files generated by pg_upgrade"
    mkdir -p "$MOUNT_POINT/sql"
    cp /tmp/pg_upgrade/*.sql "$MOUNT_POINT/sql/" || true
    chown -R postgres:postgres "$MOUNT_POINT/sql/"

    echo "13. Cleaning up"
    cleanup "complete"
}

trap cleanup ERR

echo "running" > /tmp/pg-upgrade-status
if [ -z "$IS_CI" ] && [ -z "$IS_LOCAL_UPGRADE" ]; then
    initiate_upgrade >> "$LOG_FILE" 2>&1 &
    echo "Upgrade initiate job completed"
else
    rm -f /tmp/pg-upgrade-status
    initiate_upgrade
fi

'''
'''--- /postgres/ansible/files/admin_api_scripts/pg_upgrade_scripts/common.sh ---
#! /usr/bin/env bash

# Common functions and variables used by initiate.sh and complete.sh

REPORTING_PROJECT_REF="ihmaxnjpcccasmrbkpvo"
REPORTING_CREDENTIALS_FILE="/root/upgrade-reporting-credentials"

REPORTING_ANON_KEY=""
if [ -f "$REPORTING_CREDENTIALS_FILE" ]; then
    REPORTING_ANON_KEY=$(cat "$REPORTING_CREDENTIALS_FILE")
fi

# shellcheck disable=SC2120
# Arguments are passed in other files
function run_sql {
    psql -h localhost -U supabase_admin -d postgres "$@"
}

function ship_logs {
    LOG_FILE=$1

    if [ -z "$REPORTING_ANON_KEY" ]; then
        echo "No reporting key found. Skipping log upload."
        return 0
    fi

    if [ ! -f "$LOG_FILE" ]; then
        echo "No log file found. Skipping log upload."
        return 0
    fi

    if [ ! -s "$LOG_FILE" ]; then
        echo "Log file is empty. Skipping log upload."
        return 0
    fi

    HOSTNAME=$(hostname)
    DERIVED_REF="${HOSTNAME##*-}"

    printf -v BODY '{ "ref": "%s", "step": "%s", "content": %s }' "$DERIVED_REF" "completion" "$(cat "$LOG_FILE" | jq -Rs '.')"
    curl -sf -X POST "https://$REPORTING_PROJECT_REF.supabase.co/rest/v1/error_logs" \
         -H "apikey: ${REPORTING_ANON_KEY}" \
         -H 'Content-type: application/json' \
         -d "$BODY"
}

function retry {
  local retries=$1
  shift

  local count=0
  until "$@"; do
    exit=$?
    wait=$((2 ** (count + 1)))
    count=$((count + 1))
    if [ $count -lt "$retries" ]; then
        echo "Command $* exited with code $exit, retrying..."
        sleep $wait
    else
        echo "Command $* exited with code $exit, no more retries left."
        return $exit
    fi
  done
  return 0
}

CI_stop_postgres() {
    BINDIR=$(pg_config --bindir)
    ARG=${1:-""}

    if [ "$ARG" = "--new-bin" ]; then
        BINDIR="/tmp/pg_upgrade_bin/$PG_MAJOR_VERSION/bin"
    fi

    su postgres -c "$BINDIR/pg_ctl stop -o '-c config_file=/etc/postgresql/postgresql.conf' -l /tmp/postgres.log"
}

CI_start_postgres() {
    BINDIR=$(pg_config --bindir)
    ARG=${1:-""}

    if [ "$ARG" = "--new-bin" ]; then
        BINDIR="/tmp/pg_upgrade_bin/$PG_MAJOR_VERSION/bin"
    fi

    su postgres -c "$BINDIR/pg_ctl start -o '-c config_file=/etc/postgresql/postgresql.conf' -l /tmp/postgres.log"
}

swap_postgres_and_supabase_admin() {
    run_sql <<'EOSQL'
alter database postgres connection limit 0;
select pg_terminate_backend(pid) from pg_stat_activity where backend_type = 'client backend' and pid != pg_backend_pid();
EOSQL

    if [ -z "$IS_CI" ]; then
        retry 5 systemctl restart postgresql
    else
        CI_start_postgres ""
    fi

    retry 8 pg_isready -h localhost -U supabase_admin

    run_sql <<'EOSQL'
set statement_timeout = '600s';
begin;
create role supabase_tmp superuser;
set session authorization supabase_tmp;

-- to handle snowflakes that happened in the past
revoke supabase_admin from authenticator;

do $$
begin
  if exists (select from pg_extension where extname = 'timescaledb') then
    execute(format('select %s.timescaledb_pre_restore()', (select pronamespace::regnamespace from pg_proc where proname = 'timescaledb_pre_restore')));
  end if;
end
$$;

do $$
declare
  postgres_rolpassword text := (select rolpassword from pg_authid where rolname = 'postgres');
  supabase_admin_rolpassword text := (select rolpassword from pg_authid where rolname = 'supabase_admin');
  role_settings jsonb[] := (
    select coalesce(array_agg(jsonb_build_object('database', d.datname, 'role', a.rolname, 'configs', s.setconfig)), '{}')
    from pg_db_role_setting s
    left join pg_database d on d.oid = s.setdatabase
    join pg_authid a on a.oid = s.setrole
    where a.rolname in ('postgres', 'supabase_admin')
  );
  event_triggers jsonb[] := (select coalesce(array_agg(jsonb_build_object('name', evtname)), '{}') from pg_event_trigger where evtowner = 'postgres'::regrole);
  user_mappings jsonb[] := (
    select coalesce(array_agg(jsonb_build_object('oid', um.oid, 'role', a.rolname, 'server', s.srvname, 'options', um.umoptions)), '{}')
    from pg_user_mapping um
    join pg_authid a on a.oid = um.umuser
    join pg_foreign_server s on s.oid = um.umserver
    where a.rolname in ('postgres', 'supabase_admin')
  );
  -- Objects can have initial privileges either by having those privileges set
  -- when the system is initialized (by initdb) or when the object is created
  -- during a CREATE EXTENSION and the extension script sets initial
  -- privileges using the GRANT system. (https://www.postgresql.org/docs/current/catalog-pg-init-privs.html)
  -- We only care about swapping init_privs for extensions.
  init_privs jsonb[] := (
    select coalesce(array_agg(jsonb_build_object('objoid', objoid, 'classoid', classoid, 'initprivs', initprivs::text)), '{}')
    from pg_init_privs
    where privtype = 'e'
  );
  default_acls jsonb[] := (
    select coalesce(array_agg(jsonb_build_object('oid', d.oid, 'role', a.rolname, 'schema', n.nspname, 'objtype', d.defaclobjtype, 'acl', defaclacl::text)), '{}')
    from pg_default_acl d
    join pg_authid a on a.oid = d.defaclrole
    left join pg_namespace n on n.oid = d.defaclnamespace
  );
  schemas jsonb[] := (
    select coalesce(array_agg(jsonb_build_object('oid', n.oid, 'owner', a.rolname, 'acl', nspacl::text)), '{}')
    from pg_namespace n
    join pg_authid a on a.oid = n.nspowner
    where true
      and n.nspname != 'information_schema'
      and not starts_with(n.nspname, 'pg_')
  );
  types jsonb[] := (
    select coalesce(array_agg(jsonb_build_object('oid', t.oid, 'owner', a.rolname, 'acl', t.typacl::text)), '{}')
    from pg_type t
    join pg_namespace n on n.oid = t.typnamespace
    join pg_authid a on a.oid = t.typowner
    where true
      and n.nspname != 'information_schema'
      and not starts_with(n.nspname, 'pg_')
      and (
        t.typrelid = 0
        or (
          select
            c.relkind = 'c'
          from
            pg_class c
          where
            c.oid = t.typrelid
        )
      )
      and not exists (
        select
        from
          pg_type el
        where
          el.oid = t.typelem
          and el.typarray = t.oid
      )
  );
  functions jsonb[] := (
    select coalesce(array_agg(jsonb_build_object('oid', p.oid, 'owner', a.rolname, 'kind', p.prokind, 'acl', p.proacl::text)), '{}')
    from pg_proc p
    join pg_namespace n on n.oid = p.pronamespace
    join pg_authid a on a.oid = p.proowner
    where true
      and n.nspname != 'information_schema'
      and not starts_with(n.nspname, 'pg_')
  );
  relations jsonb[] := (
    select coalesce(array_agg(jsonb_build_object('oid', c.oid, 'owner', a.rolname, 'acl', c.relacl::text)), '{}')
    from (
      -- Sequences must appear after tables, so we order by relkind
      select * from pg_class order by relkind desc
    ) c
    join pg_namespace n on n.oid = c.relnamespace
    join pg_authid a on a.oid = c.relowner
    where true
      and n.nspname != 'information_schema'
      and not starts_with(n.nspname, 'pg_')
      and c.relkind not in ('c', 'i', 'I')
  );
  rec record;
  obj jsonb;
begin
  set local search_path = '';

  if exists (select from pg_event_trigger where evtname = 'pgsodium_trg_mask_update') then
    alter event trigger pgsodium_trg_mask_update disable;
  end if;

  alter role postgres rename to supabase_admin_;
  alter role supabase_admin rename to postgres;
  alter role supabase_admin_ rename to supabase_admin;

  -- role grants
  for rec in
    select * from pg_auth_members
  loop
    execute(format('revoke %s from %s;', rec.roleid::regrole, rec.member::regrole));
    execute(format(
      'grant %s to %s %s granted by %s;',
      case
        when rec.roleid = 'postgres'::regrole then 'supabase_admin'
        when rec.roleid = 'supabase_admin'::regrole then 'postgres'
        else rec.roleid::regrole
      end,
      case
        when rec.member = 'postgres'::regrole then 'supabase_admin'
        when rec.member = 'supabase_admin'::regrole then 'postgres'
        else rec.member::regrole
      end,
      case
        when rec.admin_option then 'with admin option'
        else ''
      end,
      case
        when rec.grantor = 'postgres'::regrole then 'supabase_admin'
        when rec.grantor = 'supabase_admin'::regrole then 'postgres'
        else rec.grantor::regrole
      end
    ));
  end loop;

  -- role passwords
  execute(format('alter role postgres password %L;', postgres_rolpassword));
  execute(format('alter role supabase_admin password %L;', supabase_admin_rolpassword));

  -- role settings
  foreach obj in array role_settings
  loop
    execute(format('alter role %I %s reset all',
                   case when obj->>'role' = 'postgres' then 'supabase_admin' else 'postgres' end,
                   case when obj->>'database' is null then '' else format('in database %I', obj->>'database') end
    ));
  end loop;
  foreach obj in array role_settings
  loop
    for rec in
      select split_part(value, '=', 1) as key, substr(value, strpos(value, '=') + 1) as value
      from jsonb_array_elements_text(obj->'configs')
    loop
      execute(format('alter role %I %s set %I to %s',
                     obj->>'role',
                     case when obj->>'database' is null then '' else format('in database %I', obj->>'database') end,
                     rec.key,
                     -- https://github.com/postgres/postgres/blob/70d1c664f4376fd3499e3b0c6888cf39b65d722b/src/bin/pg_dump/dumputils.c#L861
                     case
                       when rec.key in ('local_preload_libraries', 'search_path', 'session_preload_libraries', 'shared_preload_libraries', 'temp_tablespaces', 'unix_socket_directories')
                         then rec.value
                       else quote_literal(rec.value)
                     end
      ));
    end loop;
  end loop;

  reassign owned by postgres to supabase_admin;

  -- databases
  for rec in
    select * from pg_database where datname not in ('template0')
  loop
    execute(format('alter database %I owner to postgres;', rec.datname));
  end loop;

  -- event triggers
  foreach obj in array event_triggers
  loop
    execute(format('alter event trigger %I owner to postgres;', obj->>'name'));
  end loop;

  -- publications
  for rec in
    select * from pg_publication
  loop
    execute(format('alter publication %I owner to postgres;', rec.pubname));
  end loop;

  -- FDWs
  for rec in
    select * from pg_foreign_data_wrapper
  loop
    execute(format('alter foreign data wrapper %I owner to postgres;', rec.fdwname));
  end loop;

  -- foreign servers
  for rec in
    select * from pg_foreign_server
  loop
    execute(format('alter server %I owner to postgres;', rec.srvname));
  end loop;

  -- user mappings
  foreach obj in array user_mappings
  loop
    execute(format('drop user mapping for %I server %I', case when obj->>'role' = 'postgres' then 'supabase_admin' else 'postgres' end, obj->>'server'));
  end loop;
  foreach obj in array user_mappings
  loop
    execute(format('create user mapping for %I server %I', obj->>'role', obj->>'server'));
    for rec in
      select split_part(value, '=', 1) as key, substr(value, strpos(value, '=') + 1) as value
      from jsonb_array_elements_text(obj->'options')
    loop
      execute(format('alter user mapping for %I server %I options (%I %L)', obj->>'role', obj->>'server', rec.key, rec.value));
    end loop;
  end loop;

  -- init privs
  foreach obj in array init_privs
  loop
    -- We need to modify system catalog directly here because there's no ALTER INIT PRIVILEGES.
    update pg_init_privs set initprivs = (obj->>'initprivs')::aclitem[] where objoid = (obj->>'objoid')::oid and classoid = (obj->>'classoid')::oid;
  end loop;

  -- default acls
  foreach obj in array default_acls
  loop
    for rec in
      select grantor, grantee, privilege_type, is_grantable
      from aclexplode((obj->>'acl')::aclitem[])
    loop
      if obj->>'role' in ('postgres', 'supabase_admin') or rec.grantee::regrole in ('postgres', 'supabase_admin') then
        execute(format('alter default privileges for role %I %s revoke %s on %s from %s'
                     , case when obj->>'role' = 'postgres' then 'supabase_admin'
                            when obj->>'role' = 'supabase_admin' then 'postgres'
                            else obj->>'role'
                       end
                     , case when obj->>'schema' is null then ''
                            else format('in schema %I', obj->>'schema')
                       end
                     , rec.privilege_type
                     , case when obj->>'objtype' = 'r' then 'tables'
                            when obj->>'objtype' = 'S' then 'sequences'
                            when obj->>'objtype' = 'f' then 'functions'
                            when obj->>'objtype' = 'T' then 'types'
                            when obj->>'objtype' = 'n' then 'schemas'
                       end
                     , case when rec.grantee = 'postgres'::regrole then 'supabase_admin'
                            when rec.grantee = 'supabase_admin'::regrole then 'postgres'
                            when rec.grantee = 0 then 'public'
                            else rec.grantee::regrole::text
                       end
                     ));
      end if;
    end loop;
  end loop;

  foreach obj in array default_acls
  loop
    for rec in
      select grantor, grantee, privilege_type, is_grantable
      from aclexplode((obj->>'acl')::aclitem[])
    loop
      if obj->>'role' in ('postgres', 'supabase_admin') or rec.grantee::regrole in ('postgres', 'supabase_admin') then
        execute(format('alter default privileges for role %I %s grant %s on %s to %s %s'
                     , obj->>'role'
                     , case when obj->>'schema' is null then ''
                            else format('in schema %I', obj->>'schema')
                       end
                     , rec.privilege_type
                     , case when obj->>'objtype' = 'r' then 'tables'
                            when obj->>'objtype' = 'S' then 'sequences'
                            when obj->>'objtype' = 'f' then 'functions'
                            when obj->>'objtype' = 'T' then 'types'
                            when obj->>'objtype' = 'n' then 'schemas'
                       end
                     , case when rec.grantee = 0 then 'public' else rec.grantee::regrole::text end
                     , case when rec.is_grantable then 'with grant option' else '' end
                     ));
      end if;
    end loop;
  end loop;

  -- schemas
  foreach obj in array schemas
  loop
    if obj->>'owner' = 'postgres' then
      execute(format('alter schema %s owner to postgres;', (obj->>'oid')::regnamespace));
    end if;
    for rec in
      select grantor, grantee, privilege_type, is_grantable
      from aclexplode((obj->>'acl')::aclitem[])
      where grantee::regrole in ('postgres', 'supabase_admin')
    loop
      execute(format('revoke %s on schema %s from %I', rec.privilege_type, (obj->>'oid')::regnamespace, case when rec.grantee = 'postgres'::regrole then 'supabase_admin' else 'postgres' end));
    end loop;
  end loop;
  foreach obj in array schemas
  loop
    for rec in
      select grantor, grantee, privilege_type, is_grantable
      from aclexplode((obj->>'acl')::aclitem[])
      where grantee::regrole in ('postgres', 'supabase_admin')
    loop
      execute(format('grant %s on schema %s to %s %s', rec.privilege_type, (obj->>'oid')::regnamespace, rec.grantee::regrole, case when rec.is_grantable then 'with grant option' else '' end));
    end loop;
  end loop;

  -- types
  foreach obj in array types
  loop
    if obj->>'owner' = 'postgres' then
      execute(format('alter type %s owner to postgres;', (obj->>'oid')::regtype));
    end if;
    for rec in
      select grantor, grantee, privilege_type, is_grantable
      from aclexplode((obj->>'acl')::aclitem[])
      where grantee::regrole in ('postgres', 'supabase_admin')
    loop
      execute(format('revoke %s on type %s from %I', rec.privilege_type, (obj->>'oid')::regtype, case when rec.grantee = 'postgres'::regrole then 'supabase_admin' else 'postgres' end));
    end loop;
  end loop;
  foreach obj in array types
  loop
    for rec in
      select grantor, grantee, privilege_type, is_grantable
      from aclexplode((obj->>'acl')::aclitem[])
      where grantee::regrole in ('postgres', 'supabase_admin')
    loop
      execute(format('grant %s on type %s to %s %s', rec.privilege_type, (obj->>'oid')::regtype, rec.grantee::regrole, case when rec.is_grantable then 'with grant option' else '' end));
    end loop;
  end loop;

  -- functions
  foreach obj in array functions
  loop
    if obj->>'owner' = 'postgres' then
      execute(format('alter %s %s(%s) owner to postgres;'
                     , case when obj->>'kind' = 'p' then 'procedure' else 'function' end
                     , (obj->>'oid')::regproc
                     , pg_get_function_identity_arguments((obj->>'oid')::regproc)));
    end if;
    for rec in
      select grantor, grantee, privilege_type, is_grantable
      from aclexplode((obj->>'acl')::aclitem[])
      where grantee::regrole in ('postgres', 'supabase_admin')
    loop
      execute(format('revoke %s on %s %s(%s) from %I'
          , rec.privilege_type
          , case
              when obj->>'kind' = 'p' then 'procedure'
              else 'function'
            end
          , (obj->>'oid')::regproc
          , pg_get_function_identity_arguments((obj->>'oid')::regproc)
          , case when rec.grantee = 'postgres'::regrole then 'supabase_admin' else 'postgres' end
          ));
    end loop;
  end loop;
  foreach obj in array functions
  loop
    for rec in
      select grantor, grantee, privilege_type, is_grantable
      from aclexplode((obj->>'acl')::aclitem[])
      where grantee::regrole in ('postgres', 'supabase_admin')
    loop
      execute(format('grant %s on %s %s(%s) to %s %s'
          , rec.privilege_type
          , case
              when obj->>'kind' = 'p' then 'procedure'
              else 'function'
            end
          , (obj->>'oid')::regproc
          , pg_get_function_identity_arguments((obj->>'oid')::regproc)
          , rec.grantee::regrole
          , case when rec.is_grantable then 'with grant option' else '' end
          ));
    end loop;
  end loop;

  -- relations
  foreach obj in array relations
  loop
    -- obj->>'oid' (text) needs to be casted to oid first for some reason

    if obj->>'owner' = 'postgres' then
      execute(format('alter table %s owner to postgres;', (obj->>'oid')::oid::regclass));
    end if;
    for rec in
      select grantor, grantee, privilege_type, is_grantable
      from aclexplode((obj->>'acl')::aclitem[])
      where grantee::regrole in ('postgres', 'supabase_admin')
    loop
      execute(format('revoke %s on table %s from %I', rec.privilege_type, (obj->>'oid')::oid::regclass, case when rec.grantee = 'postgres'::regrole then 'supabase_admin' else 'postgres' end));
    end loop;
  end loop;
  foreach obj in array relations
  loop
    -- obj->>'oid' (text) needs to be casted to oid first for some reason

    for rec in
      select grantor, grantee, privilege_type, is_grantable
      from aclexplode((obj->>'acl')::aclitem[])
      where grantee::regrole in ('postgres', 'supabase_admin')
    loop
      execute(format('grant %s on table %s to %s %s', rec.privilege_type, (obj->>'oid')::oid::regclass, rec.grantee::regrole, case when rec.is_grantable then 'with grant option' else '' end));
    end loop;
  end loop;

  if exists (select from pg_event_trigger where evtname = 'pgsodium_trg_mask_update') then
    alter event trigger pgsodium_trg_mask_update enable;
  end if;
end
$$;

do $$
begin
  if exists (select from pg_extension where extname = 'timescaledb') then
    execute(format('select %s.timescaledb_post_restore()', (select pronamespace::regnamespace from pg_proc where proname = 'timescaledb_post_restore')));
  end if;
end
$$;

alter database postgres connection limit -1;

-- #incident-2024-09-12-project-upgrades-are-temporarily-disabled
do $$
begin
  if exists (select from pg_authid where rolname = 'pg_read_all_data') then
    execute('grant pg_read_all_data to postgres');
  end if;
end
$$;
grant pg_signal_backend to postgres;

set session authorization supabase_admin;
drop role supabase_tmp;
commit;
EOSQL
}

'''
'''--- /postgres/ansible/files/admin_api_scripts/pg_upgrade_scripts/complete.sh ---
#! /usr/bin/env bash

## This script is run on the newly launched instance which is to be promoted to
## become the primary database instance once the upgrade successfully completes.
## The following commands copy custom PG configs and enable previously disabled
## extensions, containing regtypes referencing system OIDs.

set -eEuo pipefail

SCRIPT_DIR=$(dirname -- "$0";)
# shellcheck disable=SC1091
source "$SCRIPT_DIR/common.sh"

IS_CI=${IS_CI:-}
LOG_FILE="/var/log/pg-upgrade-complete.log"

function cleanup {
    UPGRADE_STATUS=${1:-"failed"}
    EXIT_CODE=${?:-0}

    echo "$UPGRADE_STATUS" > /tmp/pg-upgrade-status

    ship_logs "$LOG_FILE" || true

    exit "$EXIT_CODE"
}

function execute_extension_upgrade_patches {
    if [ -f "/var/lib/postgresql/extension/wrappers--0.3.1--0.4.1.sql" ] && [ ! -f "/usr/share/postgresql/15/extension/wrappers--0.3.0--0.4.1.sql" ]; then
        cp /var/lib/postgresql/extension/wrappers--0.3.1--0.4.1.sql /var/lib/postgresql/extension/wrappers--0.3.0--0.4.1.sql
        ln -s /var/lib/postgresql/extension/wrappers--0.3.0--0.4.1.sql /usr/share/postgresql/15/extension/wrappers--0.3.0--0.4.1.sql
    fi
}

function execute_patches {
    # Patch pg_net grants
    PG_NET_ENABLED=$(run_sql -A -t -c "select count(*) > 0 from pg_extension where extname = 'pg_net';")

    if [ "$PG_NET_ENABLED" = "t" ]; then
        PG_NET_GRANT_QUERY=$(cat <<EOF
        GRANT USAGE ON SCHEMA net TO supabase_functions_admin, postgres, anon, authenticated, service_role;

        ALTER function net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) SECURITY DEFINER;
        ALTER function net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) SECURITY DEFINER;

        ALTER function net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) SET search_path = net;
        ALTER function net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) SET search_path = net;

        REVOKE ALL ON FUNCTION net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) FROM PUBLIC;
        REVOKE ALL ON FUNCTION net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) FROM PUBLIC;

        GRANT EXECUTE ON FUNCTION net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) TO supabase_functions_admin, postgres, anon, authenticated, service_role;
        GRANT EXECUTE ON FUNCTION net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) TO supabase_functions_admin, postgres, anon, authenticated, service_role;
EOF
        )

        run_sql -c "$PG_NET_GRANT_QUERY"
    fi

    # Patching pg_cron ownership as it resets during upgrade
    HAS_PG_CRON_OWNED_BY_POSTGRES=$(run_sql -A -t -c "select count(*) > 0 from pg_extension where extname = 'pg_cron' and extowner::regrole::text = 'postgres';")

    if [ "$HAS_PG_CRON_OWNED_BY_POSTGRES" = "t" ]; then
        RECREATE_PG_CRON_QUERY=$(cat <<EOF
        begin;
        create temporary table cron_job as select * from cron.job;
        create temporary table cron_job_run_details as select * from cron.job_run_details;
        drop extension pg_cron;
        create extension pg_cron schema pg_catalog;
        insert into cron.job select * from cron_job;
        insert into cron.job_run_details select * from cron_job_run_details;
        select setval('cron.jobid_seq', coalesce(max(jobid), 0) + 1, false) from cron.job;
        select setval('cron.runid_seq', coalesce(max(runid), 0) + 1, false) from cron.job_run_details;
        update cron.job set username = 'postgres' where username = 'supabase_admin';
        commit;
EOF
        )

        run_sql -c "$RECREATE_PG_CRON_QUERY"
    fi

    # Patching pgmq ownership as it resets during upgrade
    HAS_PGMQ=$(run_sql -A -t -c "select count(*) > 0 from pg_extension where extname = 'pgmq';")
    if [ "$HAS_PGMQ" = "t" ]; then
        PATCH_PGMQ_QUERY=$(cat <<EOF
        do \$\$
        declare
            tbl record;
            seq_name text;
            new_seq_name text;
            archive_table_name text;
        begin
            -- Loop through each table in the pgmq schema starting with 'q_'
            -- Rebuild the pkey column's default to avoid pg_dumpall segfaults
            for tbl in
                select c.relname as table_name
                from pg_catalog.pg_attribute a
                join pg_catalog.pg_class c on c.oid = a.attrelid
                join pg_catalog.pg_namespace n on n.oid = c.relnamespace
                where n.nspname = 'pgmq'
                    and c.relname like 'q_%'
                    and a.attname = 'msg_id'
                    and a.attidentity in ('a', 'd') -- 'a' for ALWAYS, 'd' for BY DEFAULT
            loop
                -- Check if msg_id is an IDENTITY column for idempotency
                -- Define sequence names
                seq_name := 'pgmq.' || format ('"%s_msg_id_seq"', tbl.table_name);
                new_seq_name := 'pgmq.' || format ('"%s_msg_id_seq2"', tbl.table_name);
                archive_table_name := regexp_replace(tbl.table_name, '^q_', 'a_');
                -- Execute dynamic SQL to perform the required operations
                execute format('
                    create sequence %s;
                    select setval(''%s'', nextval(''%s''));
                    alter table %s."%s" alter column msg_id drop identity;
                    alter table %s."%s" alter column msg_id set default nextval(''%s'');
                    alter sequence %s rename to "%s";',
                    -- Parameters for format placeholders
                    new_seq_name,
                    new_seq_name, seq_name,
                    'pgmq', tbl.table_name,
                    'pgmq', tbl.table_name,
                    new_seq_name,
                    -- alter seq
                    new_seq_name, 
                    tbl.table_name || '_msg_id_seq'
                );
            end loop;
            -- No tables should be owned by the extension.
            -- We want them to be included in logical backups
            for tbl in
                select c.relname as table_name
                from pg_class c
                join pg_depend d
                    on c.oid = d.objid
                join pg_extension e
                    on d.refobjid = e.oid
                where 
                c.relkind in ('r', 'p', 'u')
                and e.extname = 'pgmq'
                and (c.relname like 'q_%' or c.relname like 'a_%')
            loop
            execute format('
                alter extension pgmq drop table pgmq."%s";',
                tbl.table_name
            );
            end loop;
        end \$\$;
EOF
        )

        run_sql -c "$PATCH_PGMQ_QUERY"
        run_sql -c "update pg_extension set extowner = 'postgres'::regrole where extname = 'pgmq';"
    fi

    run_sql -c "grant pg_read_all_data, pg_signal_backend to postgres"
}

function complete_pg_upgrade {
    if [ -f /tmp/pg-upgrade-status ]; then
        echo "Upgrade job already started. Bailing."
        exit 0
    fi

    echo "running" > /tmp/pg-upgrade-status

    echo "1. Mounting data disk"
    if [ -z "$IS_CI" ]; then
        retry 8 mount -a -v
    else
        echo "Skipping mount -a -v"
    fi

    # copying custom configurations
    echo "2. Copying custom configurations"
    retry 3 copy_configs

    echo "3. Starting postgresql"
    if [ -z "$IS_CI" ]; then
        retry 3 service postgresql start
    else
        CI_start_postgres --new-bin
    fi

    execute_extension_upgrade_patches || true

    echo "4. Running generated SQL files"
    retry 3 run_generated_sql

    echo "4.1. Applying patches"
    execute_patches || true

    run_sql -c "ALTER USER postgres WITH NOSUPERUSER;"

    echo "4.2. Applying authentication scheme updates"
    retry 3 apply_auth_scheme_updates

    sleep 5

    echo "5. Restarting postgresql"
    if [ -z "$IS_CI" ]; then
        retry 3 service postgresql restart
        
        echo "5.1. Restarting gotrue and postgrest"
        retry 3 service gotrue restart
        retry 3 service postgrest restart
    else
        retry 3 CI_stop_postgres || true
        retry 3 CI_start_postgres
    fi

    echo "6. Starting vacuum analyze"
    retry 3 start_vacuum_analyze
}

function copy_configs {
    cp -R /data/conf/* /etc/postgresql-custom/
    chown -R postgres:postgres /var/lib/postgresql/data
    chown -R postgres:postgres /data/pgdata
    chmod -R 0750 /data/pgdata
}

function run_generated_sql {
    if [ -d /data/sql ]; then
        for FILE in /data/sql/*.sql; do
            if [ -f "$FILE" ]; then
                run_sql -f "$FILE" || true
            fi
        done
    fi
}

# Projects which had their passwords hashed using md5 need to have their passwords reset
# Passwords for managed roles are already present in /etc/postgresql.schema.sql
function apply_auth_scheme_updates {
    PASSWORD_ENCRYPTION_SETTING=$(run_sql -A -t -c "SHOW password_encryption;")
    if [ "$PASSWORD_ENCRYPTION_SETTING" = "md5" ]; then
        run_sql -c "ALTER SYSTEM SET password_encryption TO 'scram-sha-256';"
        run_sql -c "SELECT pg_reload_conf();"

        if [ -z "$IS_CI" ]; then
            run_sql -f /etc/postgresql.schema.sql
        fi
    fi
}

function start_vacuum_analyze {
    echo "complete" > /tmp/pg-upgrade-status

    # shellcheck disable=SC1091
    if [ -f "/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh" ]; then
        # shellcheck disable=SC1091
        source "/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh"
    fi
    vacuumdb --all --analyze-in-stages -U supabase_admin -h localhost -p 5432
    echo "Upgrade job completed"
}

trap cleanup ERR

echo "C.UTF-8 UTF-8" > /etc/locale.gen
echo "en_US.UTF-8 UTF-8" >> /etc/locale.gen
locale-gen

if [ -z "$IS_CI" ]; then
    complete_pg_upgrade >> $LOG_FILE 2>&1 &
else 
    CI_stop_postgres || true

    rm -f /tmp/pg-upgrade-status
    mv /data_migration /data

    rm -rf /var/lib/postgresql/data
    ln -s /data/pgdata /var/lib/postgresql/data

    complete_pg_upgrade
fi

'''
'''--- /postgres/ansible/files/admin_api_scripts/pg_upgrade_scripts/prepare.sh ---
#! /usr/bin/env bash
## This script is runs in advance of the database version upgrade, on the newly
## launched instance which will eventually be promoted to become the primary
## database instance once the upgrade successfully completes, terminating the
## previous (source) instance.
## The following commands safely stop the Postgres service and unmount
## the data disk off the newly launched instance, to be re-attached to the
## source instance and run the upgrade there.

set -euo pipefail

systemctl stop postgresql

cp /etc/postgresql-custom/pgsodium_root.key /data/pgsodium_root.key
umount /data

'''
'''--- /postgres/ansible/files/admin_api_scripts/pg_upgrade_scripts/check.sh ---
#! /usr/bin/env bash
## This script provides a method to check the status of the database upgrade
## process, which is updated in /tmp/pg-upgrade-status by initiate.sh
## This runs on the old (source) instance.

set -euo pipefail

STATUS_FILE="/tmp/pg-upgrade-status"

if [ -f "${STATUS_FILE}" ]; then
    STATUS=$(cat "${STATUS_FILE}")
    echo -n "${STATUS}"
else
    echo -n "unknown"
fi


'''
'''--- /postgres/ansible/files/admin_api_scripts/pg_upgrade_scripts/pgsodium_getkey.sh ---
#!/bin/bash

set -euo pipefail

KEY_FILE=/etc/postgresql-custom/pgsodium_root.key

# if key file doesn't exist (project previously didn't use pgsodium), generate a new key
if [[ ! -f "${KEY_FILE}" ]]; then
    head -c 32 /dev/urandom | od -A n -t x1 | tr -d ' \n' > $KEY_FILE
fi

cat $KEY_FILE

'''
'''--- /postgres/ansible/files/envoy_config/envoy.yaml ---
dynamic_resources:
  cds_config:
    path_config_source:
      path: /etc/envoy/cds.yaml
    resource_api_version: V3
  lds_config:
    path_config_source:
      path: /etc/envoy/lds.yaml
    resource_api_version: V3
node:
  cluster: cluster_0
  id: node_0
overload_manager:
  resource_monitors:
    - name: envoy.resource_monitors.global_downstream_max_connections
      typed_config:
        '@type': >-
          type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig
        max_active_downstream_connections: 30000
stats_config:
  stats_matcher:
    reject_all: true


'''
'''--- /postgres/ansible/files/envoy_config/cds.yaml ---
resources:
  - '@type': type.googleapis.com/envoy.config.cluster.v3.Cluster
    name: admin_api
    load_assignment:
      cluster_name: admin_api
      endpoints:
        - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: 127.0.0.1
                    port_value: 8085
    circuit_breakers:
      thresholds:
        - priority: DEFAULT
          max_connections: 10000
          max_pending_requests: 10000
          max_requests: 10000
          retry_budget:
            budget_percent:
              value: 100
            min_retry_concurrency: 100
  - '@type': type.googleapis.com/envoy.config.cluster.v3.Cluster
    name: gotrue
    load_assignment:
      cluster_name: gotrue
      endpoints:
        - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: 127.0.0.1
                    port_value: 9999
    circuit_breakers:
      thresholds:
        - priority: DEFAULT
          max_connections: 10000
          max_pending_requests: 10000
          max_requests: 10000
          retry_budget:
            budget_percent:
              value: 100
            min_retry_concurrency: 100
  - '@type': type.googleapis.com/envoy.config.cluster.v3.Cluster
    name: postgrest
    load_assignment:
      cluster_name: postgrest
      endpoints:
        - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: 127.0.0.1
                    port_value: 3000
    circuit_breakers:
      thresholds:
        - priority: DEFAULT
          max_connections: 10000
          max_pending_requests: 10000
          max_requests: 10000
          retry_budget:
            budget_percent:
              value: 100
            min_retry_concurrency: 100
  - '@type': type.googleapis.com/envoy.config.cluster.v3.Cluster
    name: postgrest_admin
    load_assignment:
      cluster_name: postgrest_admin
      endpoints:
        - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: 127.0.0.1
                    port_value: 3001
    circuit_breakers:
      thresholds:
        - priority: DEFAULT
          max_connections: 10000
          max_pending_requests: 10000
          max_requests: 10000
          retry_budget:
            budget_percent:
              value: 100
            min_retry_concurrency: 100


'''
'''--- /postgres/ansible/files/envoy_config/lds.yaml ---
resources:
  - '@type': type.googleapis.com/envoy.config.listener.v3.Listener
    name: http_listener
    address:
      socket_address:
        address: '::'
        port_value: 80
        ipv4_compat: true
    filter_chains:
      - filters: &ref_1
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              '@type': >-
                type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              access_log:
                - name: envoy.access_loggers.stdout
                  filter:
                    status_code_filter:
                      comparison:
                        op: GE
                        value:
                          default_value: 400
                          runtime_key: unused
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog
              generate_request_id: false
              http_filters:
                - name: envoy.filters.http.cors
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors
                - name: envoy.filters.http.rbac
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC
                    rules:
                      action: DENY
                      policies:
                        api_key_missing:
                          permissions:
                            - any: true
                          principals:
                            - not_id:
                                or_ids:
                                  ids:
                                    - header:
                                        name: apikey
                                        present_match: true
                                    - header:
                                        name: ':path'
                                        string_match:
                                          contains: apikey=
                        api_key_not_valid:
                          permissions:
                            - any: true
                          principals:
                            - not_id:
                                or_ids:
                                  ids:
                                    - header:
                                        name: apikey
                                        string_match:
                                          exact: anon_key
                                    - header:
                                        name: apikey
                                        string_match:
                                          exact: service_key
                                    - header:
                                        name: apikey
                                        string_match:
                                          exact: supabase_admin_key
                                    - header:
                                        name: ':path'
                                        string_match:
                                          contains: apikey=anon_key
                                    - header:
                                        name: ':path'
                                        string_match:
                                          contains: apikey=service_key
                                    - header:
                                        name: ':path'
                                        string_match:
                                          contains: apikey=supabase_admin_key
                - name: envoy.filters.http.lua
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua
                    source_codes:
                      remove_apikey_and_empty_key_query_parameters:
                        inline_string: |-
                          function envoy_on_request(request_handle)
                            local path = request_handle:headers():get(":path")
                            request_handle
                              :headers()
                              :replace(":path", path:gsub("&=[^&]*", ""):gsub("?=[^&]*$", ""):gsub("?=[^&]*&", "?"):gsub("&apikey=[^&]*", ""):gsub("?apikey=[^&]*$", ""):gsub("?apikey=[^&]*&", "?"))
                          end
                      remove_empty_key_query_parameters:
                        inline_string: |-
                          function envoy_on_request(request_handle)
                            local path = request_handle:headers():get(":path")
                            request_handle
                              :headers()
                              :replace(":path", path:gsub("&=[^&]*", ""):gsub("?=[^&]*$", ""):gsub("?=[^&]*&", "?"))
                          end
                - name: envoy.filters.http.compressor.brotli
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.compressor.v3.Compressor
                    response_direction_config:
                      common_config:
                        min_content_length: 100
                        content_type:
                          - application/vnd.pgrst.object+json
                          - application/vnd.pgrst.array+json
                          - application/openapi+json
                          - application/geo+json
                          - text/csv
                          - application/vnd.pgrst.plan
                          - application/vnd.pgrst.object
                          - application/vnd.pgrst.array
                          - application/javascript
                          - application/json
                          - application/xhtml+xml
                          - image/svg+xml
                          - text/css
                          - text/html
                          - text/plain
                          - text/xml
                      disable_on_etag_header: true
                    request_direction_config:
                      common_config:
                        enabled:
                          default_value: false
                          runtime_key: request_compressor_enabled
                    compressor_library:
                      name: text_optimized
                      typed_config:
                        '@type': >-
                          type.googleapis.com/envoy.extensions.compression.brotli.compressor.v3.Brotli
                - name: envoy.filters.http.compressor.gzip
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.compressor.v3.Compressor
                    response_direction_config:
                      common_config:
                        min_content_length: 100
                        content_type:
                          - application/vnd.pgrst.object+json
                          - application/vnd.pgrst.array+json
                          - application/openapi+json
                          - application/geo+json
                          - text/csv
                          - application/vnd.pgrst.plan
                          - application/vnd.pgrst.object
                          - application/vnd.pgrst.array
                          - application/javascript
                          - application/json
                          - application/xhtml+xml
                          - image/svg+xml
                          - text/css
                          - text/html
                          - text/plain
                          - text/xml
                      disable_on_etag_header: true
                    request_direction_config:
                      common_config:
                        enabled:
                          default_value: false
                          runtime_key: request_compressor_enabled
                    compressor_library:
                      name: text_optimized
                      typed_config:
                        '@type': >-
                          type.googleapis.com/envoy.extensions.compression.gzip.compressor.v3.Gzip
                - name: envoy.filters.http.router
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
                    dynamic_stats: false
              local_reply_config:
                mappers:
                  - filter:
                      and_filter:
                        filters:
                          - status_code_filter:
                              comparison:
                                value:
                                  default_value: 403
                                  runtime_key: unused
                          - header_filter:
                              header:
                                name: ':path'
                                string_match:
                                  prefix: /customer/v1/privileged/
                    status_code: 401
                    body:
                      inline_string: Unauthorized
                    headers_to_add:
                      - header:
                          key: WWW-Authenticate
                          value: Basic realm="Unknown"
                  - filter:
                      and_filter:
                        filters:
                          - status_code_filter:
                              comparison:
                                value:
                                  default_value: 403
                                  runtime_key: unused
                          - header_filter:
                              header:
                                name: ':path'
                                string_match:
                                  prefix: /metrics/aggregated
                                invert_match: true
                    status_code: 401
                    body_format_override:
                      json_format:
                        message: >-
                          `apikey` request header or query parameter is either
                          missing or invalid. Double check your Supabase `anon`
                          or `service_role` API key.
                        hint: '%RESPONSE_CODE_DETAILS%'
                      json_format_options:
                        sort_properties: false
              merge_slashes: true
              route_config:
                name: route_config_0
                virtual_hosts:
                  - name: virtual_host_0
                    domains:
                      - '*'
                    typed_per_filter_config:
                      envoy.filters.http.cors:
                        '@type': >-
                          type.googleapis.com/envoy.extensions.filters.http.cors.v3.CorsPolicy
                        allow_origin_string_match:
                          - safe_regex:
                              regex: \*
                        allow_methods: GET,HEAD,PUT,PATCH,POST,DELETE,OPTIONS,TRACE,CONNECT
                        allow_headers: apikey,authorization,x-client-info
                        max_age: '3600'
                    routes:
                      - match:
                          path: /health
                        direct_response:
                          status: 200
                          body:
                            inline_string: Healthy
                        typed_per_filter_config: &ref_0
                          envoy.filters.http.rbac:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBACPerRoute
                      - match:
                          safe_regex:
                            google_re2:
                              max_program_size: 150
                            regex: >-
                              /auth/v1/(verify|callback|authorize|sso/saml/(acs|metadata|slo)|\.well-known/(openid-configuration|jwks\.json))
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: gotrue
                          regex_rewrite:
                            pattern:
                              regex: ^/auth/v1
                            substitution: ''
                          retry_policy:
                            num_retries: 3
                            retry_on: 5xx
                          timeout: 35s
                        typed_per_filter_config: *ref_0
                      - match:
                          prefix: /auth/v1/
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: gotrue
                          prefix_rewrite: /
                          timeout: 35s
                      - match:
                          prefix: /rest/v1/
                          query_parameters:
                            - name: apikey
                              present_match: true
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: postgrest
                          prefix_rewrite: /
                          timeout: 125s
                        typed_per_filter_config:
                          envoy.filters.http.lua:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.lua.v3.LuaPerRoute
                            name: remove_apikey_and_empty_key_query_parameters
                      - match:
                          prefix: /rest/v1/
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: postgrest
                          prefix_rewrite: /
                          timeout: 125s
                        typed_per_filter_config:
                          envoy.filters.http.lua:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.lua.v3.LuaPerRoute
                            name: remove_empty_key_query_parameters
                      - match:
                          prefix: /rest-admin/v1/
                          query_parameters:
                            - name: apikey
                              present_match: true
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: postgrest_admin
                          prefix_rewrite: /
                        typed_per_filter_config:
                          envoy.filters.http.lua:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.lua.v3.LuaPerRoute
                            name: remove_apikey_and_empty_key_query_parameters
                      - match:
                          prefix: /rest-admin/v1/
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: postgrest_admin
                          prefix_rewrite: /
                      - match:
                          path: /graphql/v1
                        request_headers_to_add:
                          header:
                            key: Content-Profile
                            value: graphql_public
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: postgrest
                          prefix_rewrite: /rpc/graphql
                          timeout: 125s
                      - match:
                          prefix: /admin/v1/
                        request_headers_to_remove:
                          - sb-opk
                        route:
                          cluster: admin_api
                          prefix_rewrite: /
                          timeout: 600s
                      - match:
                          prefix: /customer/v1/privileged/
                        request_headers_to_remove:
                          - sb-opk
                        route:
                          cluster: admin_api
                          prefix_rewrite: /privileged/
                        typed_per_filter_config:
                          envoy.filters.http.rbac:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBACPerRoute
                            rbac:
                              rules:
                                action: DENY
                                policies:
                                  basic_auth:
                                    permissions:
                                      - any: true
                                    principals:
                                      - header:
                                          name: authorization
                                          invert_match: true
                                          string_match:
                                            exact: Basic c2VydmljZV9yb2xlOnNlcnZpY2Vfa2V5
                                          treat_missing_header_as_empty: true
                      - match:
                          prefix: /metrics/aggregated
                        request_headers_to_remove:
                          - sb-opk
                        route:
                          cluster: admin_api
                          prefix_rewrite: /supabase-internal/metrics
                        typed_per_filter_config:
                          envoy.filters.http.rbac:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBACPerRoute
                            rbac:
                              rules:
                                action: DENY
                                policies:
                                  not_private_ip:
                                    permissions:
                                      - any: true
                                    principals:
                                      - not_id:
                                          direct_remote_ip:
                                            address_prefix: 10.0.0.0
                                            prefix_len: 8
                    include_attempt_count_in_response: true
                    retry_policy:
                      num_retries: 5
                      retry_back_off:
                        base_interval: 0.1s
                        max_interval: 1s
                      retry_on: gateway-error
              stat_prefix: ingress_http
  - '@type': type.googleapis.com/envoy.config.listener.v3.Listener
    name: https_listener
    address:
      socket_address:
        address: '::'
        port_value: 443
        ipv4_compat: true
    filter_chains:
      - filters: *ref_1
        transport_socket:
          name: envoy.transport_sockets.tls
          typed_config:
            '@type': >-
              type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext
            common_tls_context:
              tls_certificates:
                - certificate_chain:
                    filename: /etc/envoy/fullChain.pem
                  private_key:
                    filename: /etc/envoy/privKey.pem


'''
'''--- /postgres/ansible/files/envoy_config/lds.supabase.yaml ---
resources:
  - '@type': type.googleapis.com/envoy.config.listener.v3.Listener
    name: http_listener
    address:
      socket_address:
        address: '::'
        port_value: 80
        ipv4_compat: true
    filter_chains:
      - filters: &ref_1
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              '@type': >-
                type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              access_log:
                - name: envoy.access_loggers.stdout
                  filter:
                    status_code_filter:
                      comparison:
                        op: GE
                        value:
                          default_value: 400
                          runtime_key: unused
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog
              generate_request_id: false
              http_filters:
                - name: envoy.filters.http.cors
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors
                - name: envoy.filters.http.rbac
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC
                    rules:
                      action: DENY
                      policies:
                        api_key_missing:
                          permissions:
                            - any: true
                          principals:
                            - not_id:
                                or_ids:
                                  ids:
                                    - header:
                                        name: apikey
                                        present_match: true
                                    - header:
                                        name: ':path'
                                        string_match:
                                          contains: apikey=
                        api_key_not_valid:
                          permissions:
                            - any: true
                          principals:
                            - not_id:
                                or_ids:
                                  ids:
                                    - header:
                                        name: apikey
                                        string_match:
                                          exact: anon_key
                                    - header:
                                        name: apikey
                                        string_match:
                                          exact: service_key
                                    - header:
                                        name: apikey
                                        string_match:
                                          exact: supabase_admin_key
                                    - header:
                                        name: ':path'
                                        string_match:
                                          contains: apikey=anon_key
                                    - header:
                                        name: ':path'
                                        string_match:
                                          contains: apikey=service_key
                                    - header:
                                        name: ':path'
                                        string_match:
                                          contains: apikey=supabase_admin_key
                        origin_protection_key_missing:
                          permissions:
                            - any: true
                          principals:
                            - not_id:
                                header:
                                  name: sb-opk
                                  present_match: true
                        origin_protection_key_not_valid:
                          permissions:
                            - any: true
                          principals:
                            - not_id:
                                or_ids:
                                  ids:
                                    - header:
                                        name: sb-opk
                                        string_match:
                                          exact: supabase_origin_protection_key
                - name: envoy.filters.http.lua
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua
                    source_codes:
                      remove_apikey_and_empty_key_query_parameters:
                        inline_string: |-
                          function envoy_on_request(request_handle)
                            local path = request_handle:headers():get(":path")
                            request_handle
                              :headers()
                              :replace(":path", path:gsub("&=[^&]*", ""):gsub("?=[^&]*$", ""):gsub("?=[^&]*&", "?"):gsub("&apikey=[^&]*", ""):gsub("?apikey=[^&]*$", ""):gsub("?apikey=[^&]*&", "?"))
                          end
                      remove_empty_key_query_parameters:
                        inline_string: |-
                          function envoy_on_request(request_handle)
                            local path = request_handle:headers():get(":path")
                            request_handle
                              :headers()
                              :replace(":path", path:gsub("&=[^&]*", ""):gsub("?=[^&]*$", ""):gsub("?=[^&]*&", "?"))
                          end
                - name: envoy.filters.http.compressor.brotli
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.compressor.v3.Compressor
                    response_direction_config:
                      common_config:
                        min_content_length: 100
                        content_type:
                          - application/vnd.pgrst.object+json
                          - application/vnd.pgrst.array+json
                          - application/openapi+json
                          - application/geo+json
                          - text/csv
                          - application/vnd.pgrst.plan
                          - application/vnd.pgrst.object
                          - application/vnd.pgrst.array
                          - application/javascript
                          - application/json
                          - application/xhtml+xml
                          - image/svg+xml
                          - text/css
                          - text/html
                          - text/plain
                          - text/xml
                      disable_on_etag_header: true
                    request_direction_config:
                      common_config:
                        enabled:
                          default_value: false
                          runtime_key: request_compressor_enabled
                    compressor_library:
                      name: text_optimized
                      typed_config:
                        '@type': >-
                          type.googleapis.com/envoy.extensions.compression.brotli.compressor.v3.Brotli
                - name: envoy.filters.http.compressor.gzip
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.compressor.v3.Compressor
                    response_direction_config:
                      common_config:
                        min_content_length: 100
                        content_type:
                          - application/vnd.pgrst.object+json
                          - application/vnd.pgrst.array+json
                          - application/openapi+json
                          - application/geo+json
                          - text/csv
                          - application/vnd.pgrst.plan
                          - application/vnd.pgrst.object
                          - application/vnd.pgrst.array
                          - application/javascript
                          - application/json
                          - application/xhtml+xml
                          - image/svg+xml
                          - text/css
                          - text/html
                          - text/plain
                          - text/xml
                      disable_on_etag_header: true
                    request_direction_config:
                      common_config:
                        enabled:
                          default_value: false
                          runtime_key: request_compressor_enabled
                    compressor_library:
                      name: text_optimized
                      typed_config:
                        '@type': >-
                          type.googleapis.com/envoy.extensions.compression.gzip.compressor.v3.Gzip
                - name: envoy.filters.http.router
                  typed_config:
                    '@type': >-
                      type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
                    dynamic_stats: false
              local_reply_config:
                mappers:
                  - filter:
                      and_filter:
                        filters:
                          - status_code_filter:
                              comparison:
                                value:
                                  default_value: 403
                                  runtime_key: unused
                          - header_filter:
                              header:
                                name: ':path'
                                string_match:
                                  prefix: /customer/v1/privileged/
                    status_code: 401
                    body:
                      inline_string: Unauthorized
                    headers_to_add:
                      - header:
                          key: WWW-Authenticate
                          value: Basic realm="Unknown"
                  - filter:
                      and_filter:
                        filters:
                          - status_code_filter:
                              comparison:
                                value:
                                  default_value: 403
                                  runtime_key: unused
                          - header_filter:
                              header:
                                name: ':path'
                                string_match:
                                  prefix: /metrics/aggregated
                                invert_match: true
                    status_code: 401
                    body_format_override:
                      json_format:
                        message: >-
                          `apikey` request header or query parameter is either
                          missing or invalid. Double check your Supabase `anon`
                          or `service_role` API key.
                        hint: '%RESPONSE_CODE_DETAILS%'
                      json_format_options:
                        sort_properties: false
              merge_slashes: true
              route_config:
                name: route_config_0
                virtual_hosts:
                  - name: virtual_host_0
                    domains:
                      - '*'
                    typed_per_filter_config:
                      envoy.filters.http.cors:
                        '@type': >-
                          type.googleapis.com/envoy.extensions.filters.http.cors.v3.CorsPolicy
                        allow_origin_string_match:
                          - safe_regex:
                              regex: \*
                        allow_methods: GET,HEAD,PUT,PATCH,POST,DELETE,OPTIONS,TRACE,CONNECT
                        allow_headers: apikey,authorization,x-client-info
                        max_age: '3600'
                    routes:
                      - match:
                          path: /health
                        direct_response:
                          status: 200
                          body:
                            inline_string: Healthy
                        typed_per_filter_config: &ref_0
                          envoy.filters.http.rbac:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBACPerRoute
                      - match:
                          safe_regex:
                            google_re2:
                              max_program_size: 150
                            regex: >-
                              /auth/v1/(verify|callback|authorize|sso/saml/(acs|metadata|slo)|\.well-known/(openid-configuration|jwks\.json))
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: gotrue
                          regex_rewrite:
                            pattern:
                              regex: ^/auth/v1
                            substitution: ''
                          retry_policy:
                            num_retries: 3
                            retry_on: 5xx
                          timeout: 35s
                        typed_per_filter_config: *ref_0
                      - match:
                          prefix: /auth/v1/
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: gotrue
                          prefix_rewrite: /
                          timeout: 35s
                      - match:
                          prefix: /rest/v1/
                          query_parameters:
                            - name: apikey
                              present_match: true
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: postgrest
                          prefix_rewrite: /
                          timeout: 125s
                        typed_per_filter_config:
                          envoy.filters.http.lua:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.lua.v3.LuaPerRoute
                            name: remove_apikey_and_empty_key_query_parameters
                      - match:
                          prefix: /rest/v1/
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: postgrest
                          prefix_rewrite: /
                          timeout: 125s
                        typed_per_filter_config:
                          envoy.filters.http.lua:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.lua.v3.LuaPerRoute
                            name: remove_empty_key_query_parameters
                      - match:
                          prefix: /rest-admin/v1/
                          query_parameters:
                            - name: apikey
                              present_match: true
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: postgrest_admin
                          prefix_rewrite: /
                        typed_per_filter_config:
                          envoy.filters.http.lua:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.lua.v3.LuaPerRoute
                            name: remove_apikey_and_empty_key_query_parameters
                      - match:
                          prefix: /rest-admin/v1/
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: postgrest_admin
                          prefix_rewrite: /
                      - match:
                          path: /graphql/v1
                        request_headers_to_add:
                          header:
                            key: Content-Profile
                            value: graphql_public
                        request_headers_to_remove:
                          - apikey
                          - sb-opk
                        route:
                          cluster: postgrest
                          prefix_rewrite: /rpc/graphql
                          timeout: 125s
                      - match:
                          prefix: /admin/v1/
                        request_headers_to_remove:
                          - sb-opk
                        route:
                          cluster: admin_api
                          prefix_rewrite: /
                          timeout: 600s
                      - match:
                          prefix: /customer/v1/privileged/
                        request_headers_to_remove:
                          - sb-opk
                        route:
                          cluster: admin_api
                          prefix_rewrite: /privileged/
                        typed_per_filter_config:
                          envoy.filters.http.rbac:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBACPerRoute
                            rbac:
                              rules:
                                action: DENY
                                policies:
                                  basic_auth:
                                    permissions:
                                      - any: true
                                    principals:
                                      - header:
                                          name: authorization
                                          invert_match: true
                                          string_match:
                                            exact: Basic c2VydmljZV9yb2xlOnNlcnZpY2Vfa2V5
                                          treat_missing_header_as_empty: true
                      - match:
                          prefix: /metrics/aggregated
                        request_headers_to_remove:
                          - sb-opk
                        route:
                          cluster: admin_api
                          prefix_rewrite: /supabase-internal/metrics
                        typed_per_filter_config:
                          envoy.filters.http.rbac:
                            '@type': >-
                              type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBACPerRoute
                            rbac:
                              rules:
                                action: DENY
                                policies:
                                  not_private_ip:
                                    permissions:
                                      - any: true
                                    principals:
                                      - not_id:
                                          direct_remote_ip:
                                            address_prefix: 10.0.0.0
                                            prefix_len: 8
                    include_attempt_count_in_response: true
                    retry_policy:
                      num_retries: 5
                      retry_back_off:
                        base_interval: 0.1s
                        max_interval: 1s
                      retry_on: gateway-error
              stat_prefix: ingress_http
  - '@type': type.googleapis.com/envoy.config.listener.v3.Listener
    name: https_listener
    address:
      socket_address:
        address: '::'
        port_value: 443
        ipv4_compat: true
    filter_chains:
      - filters: *ref_1
        transport_socket:
          name: envoy.transport_sockets.tls
          typed_config:
            '@type': >-
              type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext
            common_tls_context:
              tls_certificates:
                - certificate_chain:
                    filename: /etc/envoy/fullChain.pem
                  private_key:
                    filename: /etc/envoy/privKey.pem


'''
'''--- /postgres/ansible/files/systemd-networkd/systemd-networkd-check-and-fix.service ---
[Unit]
Description=Check if systemd-networkd has broken NDisc routes and fix
Requisite=systemd-networkd.service
After=systemd-networkd.service

[Service]
Type=oneshot
# This needs to be root for the service restart to work
User=root
Group=root
ExecStart=/usr/local/bin/systemd-networkd-check-and-fix.sh

'''
'''--- /postgres/ansible/files/systemd-networkd/systemd-networkd-check-and-fix.sh ---
#!/bin/bash

# Check for occurrences of an NDisc log error
# NOTE: --since timer flag must match the cadence of systemd timer unit. Risk of repeat matches and restart loop
journalctl --no-pager --unit systemd-networkd --since "1 minutes ago" --grep "Could not set NDisc route" >/dev/null
NDISC_ERROR=$?

if systemctl is-active --quiet systemd-networkd.service && [ "${NDISC_ERROR}" == 0 ]; then
  echo "$(date) systemd-network running but NDisc routes are broken. Restarting systemd.networkd.service"
  /usr/bin/systemctl restart systemd-networkd.service
  exit  # no need to check further
fi

# check for routes
ROUTES=$(ip -6 route list)

if ! echo "${ROUTES}" | grep default >/dev/null || ! echo "${ROUTES}" | grep "::1 dev lo">/dev/null; then
  echo "IPv6 routing table messed up. Restarting systemd.networkd.service"
  /usr/bin/systemctl restart systemd-networkd.service
fi

'''
'''--- /postgres/ansible/files/systemd-networkd/systemd-networkd-check-and-fix.timer ---
[Unit]
Description=Check if systemd-networkd has broken NDisc routes and fix

[Timer]
# NOTE: cadence must match that of the journalctl search (--since). Risk of repeat matches and restart loop
OnCalendar=minutely

[Install]
WantedBy=timers.target

'''
'''--- /postgres/ansible/files/walg_helper_scripts/wal_change_ownership.sh ---
#! /usr/bin/env bash

set -euo pipefail

filename=$1

if [[ -z "$filename" ]]; then
	echo "Nothing supplied. Exiting."
	exit 1
fi

full_path=/tmp/wal_fetch_dir/$filename

num_paths=$(readlink -f "$full_path" | wc -l)

# Checks if supplied filename string contains multiple paths
# For example, "correct/path /var/lib/injected/path /var/lib/etc"
if [[ "$num_paths" -gt 1 ]]; then
	echo "Multiple paths supplied. Exiting."
	exit 1
fi

base_dir=$(readlink -f "$full_path" | cut -d'/' -f2)

# Checks if directory/ file to be manipulated 
# is indeed within the /tmp directory
# For example, "/tmp/../var/lib/postgresql/..." 
# will return "var" as the value for $base_dir
if [[ "$base_dir" != "tmp" ]]; then
	echo "Attempt to manipulate a file not in /tmp. Exiting."
	exit 1
fi

# Checks if change of ownership will be applied to a file
# If not, exit
if [[ ! -f $full_path ]]; then
	echo "Either file does not exist or is a directory.  Exiting."
	exit 1
fi

# once valid, proceed to change ownership
chown postgres:postgres "$full_path"

'''
'''--- /postgres/ansible/files/walg_helper_scripts/wal_fetch.sh ---
#! /usr/bin/env bash

set -euo pipefail

# Fetch the WAL file and temporarily store them in /tmp
sudo -u wal-g wal-g wal-fetch "$1" /tmp/wal_fetch_dir/"$1" --config /etc/wal-g/config.json 

# Ensure WAL file is owned by the postgres Linux user
sudo -u root /root/wal_change_ownership.sh "$1"

# Move file to its final destination
mv /tmp/wal_fetch_dir/"$1" /var/lib/postgresql/data/"$2"

'''
'''--- /postgres/ansible/files/kong_config/kong.conf.j2 ---
database = off
declarative_config = /etc/kong/kong.yml

# plugins defined in the dockerfile
plugins = request-transformer,cors,key-auth,http-log

proxy_listen = 0.0.0.0:80 reuseport backlog=16384, 0.0.0.0:443 http2 ssl reuseport backlog=16834, [::]:80 reuseport backlog=16384, [::]:443 http2 ssl reuseport backlog=16384

'''
'''--- /postgres/ansible/files/kong_config/kong.env.j2 ---
KONG_NGINX_HTTP_GZIP=on
KONG_NGINX_HTTP_GZIP_COMP_LEVEL=6
KONG_NGINX_HTTP_GZIP_MIN_LENGTH=256
KONG_NGINX_HTTP_GZIP_PROXIED=any
KONG_NGINX_HTTP_GZIP_VARY=on
KONG_NGINX_HTTP_GZIP_TYPES=text/plain application/xml application/openapi+json application/json
KONG_PROXY_ERROR_LOG=syslog:server=unix:/dev/log
KONG_ADMIN_ERROR_LOG=syslog:server=unix:/dev/log

'''
'''--- /postgres/ansible/files/kong_config/kong.service.j2 ---
[Unit]
Description=Kong server
After=postgrest.service gotrue.service adminapi.service
Wants=postgrest.service gotrue.service adminapi.service
Conflicts=envoy.service

# Ensures that Kong service is stopped before Envoy service is started
Before=envoy.service

[Service]
Type=forking
ExecStart=/usr/local/bin/kong start -c /etc/kong/kong.conf
ExecReload=/usr/local/bin/kong reload -c /etc/kong/kong.conf
ExecStop=/usr/local/bin/kong quit
User=kong
EnvironmentFile=/etc/kong/kong.env
Slice=services.slice
Restart=always
RestartSec=3
LimitNOFILE=100000

# The kong user is unprivileged and thus not permitted to bind on ports < 1024
# Via systemd we grant the process a set of privileges to bind to 80/443
# See http://archive.vn/36zJU
AmbientCapabilities=CAP_NET_BIND_SERVICE

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ansible/files/logrotate_config/logrotate-postgres-csv.conf ---
/var/log/postgresql/postgresql.csv {
    size 50M
    rotate 9
    compress
    delaycompress
    notifempty
    missingok
    postrotate
        sudo -u postgres /usr/lib/postgresql/bin/pg_ctl -D /var/lib/postgresql/data logrotate
    endscript
}

'''
'''--- /postgres/ansible/files/logrotate_config/logrotate-postgres-auth.conf ---
/var/log/postgresql/auth-failures.csv {
    size 10M
    rotate 5
    compress
    delaycompress
    notifempty
    missingok
}

'''
'''--- /postgres/ansible/files/logrotate_config/logrotate-postgres.conf ---
/var/log/postgresql/postgresql.log {
    size 50M
    rotate 3  
    copytruncate
    delaycompress
    compress
    notifempty
    missingok
}

'''
'''--- /postgres/ansible/files/logrotate_config/logrotate-walg.conf ---
/var/log/wal-g/*.log {
    size 50M
    rotate 3  
    copytruncate
    delaycompress
    compress
    notifempty
    missingok
}

'''
'''--- /postgres/ebssurrogate/USAGE.md ---
## Ext4 amd64 AMI creation

`packer build -var "aws_access_key=$AWS_ACCESS_KEY_ID" -var "aws_secret_key=$AWS_SECRET_ACCESS_KEY" -var "region=$AWS_REGION" \
-var "docker_passwd=$DOCKER_PASSWD" -var "docker_user=$DOCKER_USER" -var "docker_image=$DOCKER_IMAGE" -var "docker_image_tag=$DOCKER_IMAGE_TAG" \
amazon-amd64.pkr.hcl`

## Ext4 arm64 AMI creation

`packer build -var "aws_access_key=$AWS_ACCESS_KEY_ID" -var "aws_secret_key=$AWS_SECRET_ACCESS_KEY" -var "region=$AWS_REGION" \
-var "docker_passwd=$DOCKER_PASSWD" -var "docker_user=$DOCKER_USER" -var "docker_image=$DOCKER_IMAGE" -var "docker_image_tag=$DOCKER_IMAGE_TAG" \
amazon-arm64.pkr.hcl`

## Docker Image

	DOCKER_IMAGE is used to store ccache data during build process. This can be any image, you can create your image using:

	```
	docker pull ubuntu
	docker tag ubuntu <username>/ccache
	docker push <username>/ccache
	```

	For ARM64 builds

	```	
	docker pull arm64v8/ubuntu
	docker tag arm64v8/ubuntu:latest <username>/ccache-arm64v8
	docker push <username>/ccache-arm64v8
	```
	
	Now set DOCKER_IMAGE="<username>/ccache" or DOCKER_IMAGE="<username>/ccache-arm64v8" based on your AMI architecture.
	
	
## EBS-Surrogate File layout

```
$ tree ebssurrogate/
ebssurrogate/
 files
  70-ec2-nvme-devices.rules
  cloud.cfg		# cloud.cfg for cloud-init
  ebsnvme-id
  sources-arm64.cfg       # apt/sources.list for arm64
  sources.cfg		# apt/sources.list for amd64
  vector.timer            # systemd-timer to delay vectore execution
  zfs-growpart-root.cfg
 scripts
     chroot-bootstrap.sh    # Installs grub and other required packages for build. Configures target AMI  settings
     surrogate-bootstrap.sh # Formats disk and setups chroot environment. Runs Ansible tasks within chrooted environment.
```

'''
'''--- /postgres/ebssurrogate/scripts/chroot-bootstrap-nix.sh ---
#!/usr/bin/env bash
#
# This script runs inside chrooted environment. It installs grub and its
# Configuration file.
#

set -o errexit
set -o pipefail
set -o xtrace

export DEBIAN_FRONTEND=noninteractive

export APT_OPTIONS="-oAPT::Install-Recommends=false \
		  -oAPT::Install-Suggests=false \
		    -oAcquire::Languages=none"

if [ $(dpkg --print-architecture) = "amd64" ]; 
then 
	ARCH="amd64";
else
	ARCH="arm64";
fi



function update_install_packages {
	source /etc/os-release

	# Update APT with new sources
	cat /etc/apt/sources.list
	apt-get $APT_OPTIONS update && apt-get $APT_OPTIONS --yes dist-upgrade

	# Do not configure grub during package install
	if [ "${ARCH}" = "amd64" ]; then
		echo 'grub-pc grub-pc/install_devices_empty select true' | debconf-set-selections
		echo 'grub-pc grub-pc/install_devices select' | debconf-set-selections
	# Install various packages needed for a booting system
		apt-get install -y \
		linux-aws \
		grub-pc \
		e2fsprogs
	else
		apt-get install -y e2fsprogs
	fi
	# Install standard packages
	apt-get install -y \
		sudo \
		wget \
		cloud-init \
		acpid \
		ec2-hibinit-agent \
		ec2-instance-connect \
		hibagent \
		ncurses-term \
		ssh-import-id \

	# apt upgrade
	apt-get upgrade -y

	# Install OpenSSH and other packages
	sudo add-apt-repository universe
	apt-get update
	apt-get install -y --no-install-recommends \
		openssh-server \
		git \
		ufw \
		cron \
		logrotate \
		fail2ban \
		locales \
		at \
		less \
		python3-systemd

	if [ "${ARCH}" = "arm64" ]; then
		apt-get $APT_OPTIONS --yes install linux-aws initramfs-tools dosfstools
	fi
}

function setup_locale {
cat << EOF >> /etc/locale.gen
en_US.UTF-8 UTF-8
EOF

cat << EOF > /etc/default/locale
LANG="C.UTF-8"
LC_CTYPE="C.UTF-8"
EOF
	locale-gen en_US.UTF-8
}

function setup_postgesql_env {
	    # Create the directory if it doesn't exist
    sudo mkdir -p /etc/environment.d
    
    # Define the contents of the PostgreSQL environment file
    cat <<EOF | sudo tee /etc/environment.d/postgresql.env >/dev/null
LOCALE_ARCHIVE=/usr/lib/locale/locale-archive
LANG="en_US.UTF-8"
LANGUAGE="en_US.UTF-8"
LC_ALL="en_US.UTF-8"
LC_CTYPE="en_US.UTF-8"
EOF
}

function install_packages_for_build {
	apt-get install -y --no-install-recommends linux-libc-dev \
	 acl \
	 magic-wormhole sysstat \
	 build-essential libreadline-dev zlib1g-dev flex bison libxml2-dev libxslt-dev libssl-dev libsystemd-dev libpq-dev libxml2-utils uuid-dev xsltproc ssl-cert \
	 gcc-10 g++-10 \
	 libgeos-dev libproj-dev libgdal-dev libjson-c-dev libboost-all-dev libcgal-dev libmpfr-dev libgmp-dev cmake \
	 libkrb5-dev \
	 maven default-jre default-jdk \
	 curl gpp apt-transport-https cmake libc++-dev libc++abi-dev libc++1 libglib2.0-dev libtinfo5 libc++abi1 ninja-build python \
	 liblzo2-dev

	source /etc/os-release

	apt-get install -y --no-install-recommends llvm-11-dev clang-11
	# Mark llvm as manual to prevent auto removal
	apt-mark manual libllvm11:arm64
}

function setup_apparmor {
	apt-get install -y apparmor apparmor-utils auditd

	# Copy apparmor profiles
	cp -rv /tmp/apparmor_profiles/* /etc/apparmor.d/
}

function setup_grub_conf_arm64 {
cat << EOF > /etc/default/grub
GRUB_DEFAULT=0
GRUB_TIMEOUT=0
GRUB_TIMEOUT_STYLE="hidden"
GRUB_DISTRIBUTOR="Supabase postgresql"
GRUB_CMDLINE_LINUX_DEFAULT="nomodeset console=tty1 console=ttyS0 ipv6.disable=0"
EOF
}

# Install GRUB
function install_configure_grub {
	if [ "${ARCH}" = "arm64" ]; then
		apt-get $APT_OPTIONS --yes install cloud-guest-utils fdisk grub-efi-arm64 efibootmgr
		setup_grub_conf_arm64
		rm -rf /etc/grub.d/30_os-prober
		sleep 1
	fi
	grub-install /dev/xvdf && update-grub
}

# skip fsck for first boot
function disable_fsck {
	touch /fastboot
}

# Don't request hostname during boot but set hostname
function setup_hostname {
	sed -i 's/gethostname()/ubuntu /g' /etc/dhcp/dhclient.conf
	sed -i 's/host-name,//g' /etc/dhcp/dhclient.conf
	echo "ubuntu" > /etc/hostname
	chmod 644 /etc/hostname
}

# Set options for the default interface
function setup_eth0_interface {
cat << EOF > /etc/netplan/eth0.yaml
network:
  version: 2
  ethernets:
    eth0:
      dhcp4: true
EOF
}

function disable_sshd_passwd_auth {
	sed -i -E -e 's/^#?\s*PasswordAuthentication\s+(yes|no)\s*$/PasswordAuthentication no/g' \
	  -e 's/^#?\s*ChallengeResponseAuthentication\s+(yes|no)\s*$/ChallengeResponseAuthentication no/g' \
	 /etc/ssh/sshd_config
}

function create_admin_account {
	groupadd admin
}

# Ensure pgbouncer user and group exist
if ! id pgbouncer >/dev/null 2>&1; then
    adduser --system --no-create-home --group pgbouncer
fi

#Set default target as multi-user
function set_default_target {
	rm -f /etc/systemd/system/default.target
	ln -s /lib/systemd/system/multi-user.target /etc/systemd/system/default.target
}

# Setup ccache
function setup_ccache {
	apt-get install ccache -y
	mkdir -p /tmp/ccache
	export PATH=/usr/lib/ccache:$PATH
	echo "PATH=$PATH" >> /etc/environment
}

# Clear apt caches
function cleanup_cache {
	apt-get clean
}

update_install_packages
setup_locale
setup_postgesql_env
#install_packages_for_build
install_configure_grub
setup_apparmor
setup_hostname
create_admin_account
set_default_target
setup_eth0_interface
disable_sshd_passwd_auth
disable_fsck
#setup_ccache
cleanup_cache

'''
'''--- /postgres/ebssurrogate/scripts/qemu-bootstrap.nex.sh ---
#!/usr/bin/env bash

set -o errexit
set -o pipefail
set -o xtrace

if [ $(dpkg --print-architecture) = "amd64" ]; then
	ARCH="amd64"
else
	ARCH="arm64"
fi

function waitfor_boot_finished {
	export DEBIAN_FRONTEND=noninteractive

	echo "args: ${ARGS}"
	# Wait for cloudinit on the surrogate to complete before making progress
	while [[ ! -f /var/lib/cloud/instance/boot-finished ]]; do
		echo 'Waiting for cloud-init...'
		sleep 1
	done
}

function install_packages {
	apt-get update && sudo apt-get install software-properties-common e2fsprogs -y
	add-apt-repository --yes --update ppa:ansible/ansible && sudo apt-get install ansible -y
	ansible-galaxy collection install community.general
}

function execute_playbook {

	tee /etc/ansible/ansible.cfg <<EOF
[defaults]
callbacks_enabled = timer, profile_tasks, profile_roles
EOF
	# Run Ansible playbook
	export ANSIBLE_LOG_PATH=/tmp/ansible.log && export ANSIBLE_REMOTE_TEMP=/mnt/tmp
	ansible-playbook ./ansible/playbook.yml --extra-vars '{"nixpkg_mode": true, "debpkg_mode": false, "stage2_nix": false}' \
		--extra-vars "postgresql_version=postgresql_${POSTGRES_MAJOR_VERSION}" \
		--extra-vars "postgresql_major_version=${POSTGRES_MAJOR_VERSION}" \
		--extra-vars "postgresql_major=${POSTGRES_MAJOR_VERSION}" \
		--extra-vars "psql_version=psql_${POSTGRES_MAJOR_VERSION}"
}

function setup_postgesql_env {
	# Create the directory if it doesn't exist
	sudo mkdir -p /etc/environment.d

	# Define the contents of the PostgreSQL environment file
	cat <<EOF | sudo tee /etc/environment.d/postgresql.env >/dev/null
LOCALE_ARCHIVE=/usr/lib/locale/locale-archive
LANG="en_US.UTF-8"
LANGUAGE="en_US.UTF-8"
LC_ALL="en_US.UTF-8"
LC_CTYPE="en_US.UTF-8"
EOF
}

function setup_locale {
	cat <<EOF >>/etc/locale.gen
en_US.UTF-8 UTF-8
EOF

	cat <<EOF >/etc/default/locale
LANG="C.UTF-8"
LC_CTYPE="C.UTF-8"
EOF
	locale-gen en_US.UTF-8
}

sed -i 's/- hosts: all/- hosts: localhost/' ansible/playbook.yml

waitfor_boot_finished
install_packages
setup_postgesql_env
setup_locale
execute_playbook

####################
# stage 2 things
####################

function install_nix() {
	sudo su -c "curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install --no-confirm \
    --extra-conf \"substituters = https://cache.nixos.org https://nix-postgres-artifacts.s3.amazonaws.com\" \
    --extra-conf \"trusted-public-keys = nix-postgres-artifacts:dGZlQOvKcNEjvT7QEAJbcV6b6uk7VF/hWMjhYleiaLI=% cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY=\" " -s /bin/bash root
	. /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh

}

function execute_stage2_playbook {
	sudo tee /etc/ansible/ansible.cfg <<EOF
[defaults]
callbacks_enabled = timer, profile_tasks, profile_roles
EOF
	# Run Ansible playbook
	export ANSIBLE_LOG_PATH=/tmp/ansible.log && export ANSIBLE_REMOTE_TEMP=/tmp
	ansible-playbook ./ansible/playbook.yml \
		--extra-vars '{"nixpkg_mode": false, "stage2_nix": true, "debpkg_mode": false, "qemu_mode": true}' \
		--extra-vars "git_commit_sha=${GIT_SHA}" \
		--extra-vars "postgresql_version=postgresql_${POSTGRES_MAJOR_VERSION}" \
		--extra-vars "postgresql_major_version=${POSTGRES_MAJOR_VERSION}" \
		--extra-vars "postgresql_major=${POSTGRES_MAJOR_VERSION}" \
		--extra-vars "psql_version=psql_${POSTGRES_MAJOR_VERSION}"
}

function clean_system {
	# Copy cleanup scripts
	chmod +x /tmp/ansible-playbook/scripts/90-cleanup-qemu.sh
	/tmp/ansible-playbook/scripts/90-cleanup-qemu.sh

	# # Cleanup logs
	rm -rf /var/log/*
	# # https://github.com/fail2ban/fail2ban/issues/1593
	touch /var/log/auth.log

	touch /var/log/pgbouncer.log
	chown pgbouncer:postgres /var/log/pgbouncer.log

	# # Setup postgresql logs
	mkdir -p /var/log/postgresql
	chown postgres:postgres /var/log/postgresql
	# # Setup wal-g logs
	mkdir /var/log/wal-g
	touch /var/log/wal-g/{backup-push.log,backup-fetch.log,wal-push.log,wal-fetch.log,pitr.log}

	# #Creatre Sysstat directory for SAR
	mkdir /var/log/sysstat

	chown -R postgres:postgres /var/log/wal-g
	chmod -R 0300 /var/log/wal-g

	# # audit logs directory for apparmor
	mkdir /var/log/audit

	# # unwanted files
	rm -rf /var/lib/apt/lists/*
	rm -rf /root/.cache
	rm -rf /root/.vpython*
	rm -rf /root/go
	rm -rf /mnt/usr/share/doc
}

install_nix
execute_stage2_playbook
cloud-init clean --logs
'''
'''--- /postgres/ebssurrogate/scripts/surrogate-bootstrap-nix.sh ---
#!/usr/bin/env bash
#
# This script creates filesystem and setups up chrooted
# enviroment for further processing. It also runs
# ansible playbook and finally does system cleanup.
#
# Adapted from: https://github.com/jen20/packer-ubuntu-zfs

set -o errexit
set -o pipefail
set -o xtrace

if [ $(dpkg --print-architecture) = "amd64" ]; 
then 
	ARCH="amd64";
else
        ARCH="arm64";
fi

function waitfor_boot_finished {
	export DEBIAN_FRONTEND=noninteractive

	echo "args: ${ARGS}"
	# Wait for cloudinit on the surrogate to complete before making progress
	while [[ ! -f /var/lib/cloud/instance/boot-finished ]]; do
	    echo 'Waiting for cloud-init...'
	    sleep 1
	done
}

function install_packages {
	# Setup Ansible on host VM
	apt-get update && sudo apt-get install software-properties-common -y
	add-apt-repository --yes --update ppa:ansible/ansible && sudo apt-get install ansible -y
	ansible-galaxy collection install community.general

	# Update apt and install required packages
	apt-get update
	apt-get install -y \
		gdisk \
		e2fsprogs \
		debootstrap \
		nvme-cli
}

# Partition the new root EBS volume
function create_partition_table {

	if [ "${ARCH}" = "arm64" ]; then
		parted --script /dev/xvdf \
			 mklabel gpt \
	                 mkpart UEFI 1MiB 100MiB \
        	         mkpart ROOT 100MiB 100%
			 set 1 esp on \
			 set 1 boot on 
		parted --script /dev/xvdf print
	else
		sgdisk -Zg -n1:0:4095 -t1:EF02 -c1:GRUB -n2:0:0 -t2:8300 -c2:EXT4 /dev/xvdf
	fi

	sleep 2
}

function device_partition_mappings {
	# NVMe EBS launch device mappings (symlinks): /dev/nvme*n* to /dev/xvd*
	declare -A blkdev_mappings
	for blkdev in $(nvme list | awk '/^\/dev/ { print $1 }'); do  # /dev/nvme*n*
	    # Mapping info from disk headers
	    header=$(nvme id-ctrl --raw-binary "${blkdev}" | cut -c3073-3104 | tr -s ' ' | sed 's/ $//g' | sed 's!/dev/!!')
	    mapping="/dev/${header%%[0-9]}"  # normalize sda1 => sda

	    # Create /dev/xvd* device symlink
	    if [[ ! -z "$mapping" ]] && [[ -b "${blkdev}" ]] && [[ ! -L "${mapping}" ]]; then
		ln -s "$blkdev" "$mapping"

		blkdev_mappings["$blkdev"]="$mapping"
	    fi
	done

	create_partition_table

	# NVMe EBS launch device partition mappings (symlinks): /dev/nvme*n*p* to /dev/xvd*[0-9]+
	declare -A partdev_mappings
	for blkdev in "${!blkdev_mappings[@]}"; do  # /dev/nvme*n*
	    mapping="${blkdev_mappings[$blkdev]}"

	    # Create /dev/xvd*[0-9]+ partition device symlink
	    for partdev in "${blkdev}"p*; do
		partnum=${partdev##*p}
		if [[ ! -L "${mapping}${partnum}" ]]; then
		    ln -s "${blkdev}p${partnum}" "${mapping}${partnum}"

		    partdev_mappings["${blkdev}p${partnum}"]="${mapping}${partnum}"
		fi
	    done
	done
}


#Download and install latest e2fsprogs for fast_commit feature,if required.
function format_and_mount_rootfs {
	mkfs.ext4 -m0.1 /dev/xvdf2

	mount -o noatime,nodiratime /dev/xvdf2 /mnt
	if [ "${ARCH}" = "arm64" ]; then
		mkfs.fat -F32 /dev/xvdf1
		mkdir -p /mnt/boot/efi 
		sleep 2
		mount /dev/xvdf1 /mnt/boot/efi
	fi
	
	mkfs.ext4 /dev/xvdh

	# Explicitly reserving 100MiB worth of blocks for the data volume
	RESERVED_DATA_VOLUME_BLOCK_COUNT=$((100 * 1024 * 1024 / 4096))
	tune2fs -r $RESERVED_DATA_VOLUME_BLOCK_COUNT /dev/xvdh

	mkdir -p /mnt/data
	mount -o defaults,discard /dev/xvdh /mnt/data
}

function create_swapfile {
	fallocate -l 1G /mnt/swapfile
	chmod 600 /mnt/swapfile
	mkswap /mnt/swapfile
}

function format_build_partition {
	mkfs.ext4 -O ^has_journal /dev/xvdc
}
function pull_docker {
	apt-get install -y docker.io
	docker run -itd --name ccachedata "${DOCKER_IMAGE}:${DOCKER_IMAGE_TAG}" sh
	docker exec -itd ccachedata mkdir -p /build/ccache
}

# Create fstab
function create_fstab {
	FMT="%-42s %-11s %-5s %-17s %-5s %s"
cat > "/mnt/etc/fstab" << EOF
$(printf "${FMT}" "# DEVICE UUID" "MOUNTPOINT" "TYPE" "OPTIONS" "DUMP" "FSCK")
$(findmnt -no SOURCE /mnt | xargs blkid -o export | awk -v FMT="${FMT}" '/^UUID=/ { printf(FMT, $0, "/", "ext4", "defaults,discard", "0", "1" ) }')
$(findmnt -no SOURCE /mnt/boot/efi | xargs blkid -o export | awk -v FMT="${FMT}" '/^UUID=/ { printf(FMT, $0, "/boot/efi", "vfat", "umask=0077", "0", "1" ) }')
$(findmnt -no SOURCE /mnt/data | xargs blkid -o export | awk -v FMT="${FMT}" '/^UUID=/ { printf(FMT, $0, "/data", "ext4", "defaults,discard", "0", "2" ) }')
$(printf "$FMT" "/swapfile" "none" "swap" "sw" "0" "0")
EOF
	unset FMT
}

function setup_chroot_environment {
	UBUNTU_VERSION=$(lsb_release -cs) # 'focal' for Ubuntu 20.04

	# Bootstrap Ubuntu into /mnt
	debootstrap --arch ${ARCH} --variant=minbase "$UBUNTU_VERSION" /mnt

	# Update ec2-region
	REGION=$(curl --silent --fail http://169.254.169.254/latest/meta-data/placement/availability-zone | sed -E 's|[a-z]+$||g')
	sed -i "s/REGION/${REGION}/g" /tmp/sources.list
	cp /tmp/sources.list /mnt/etc/apt/sources.list

	if [ "${ARCH}" = "arm64" ]; then
		create_fstab
	fi

	# Create mount points and mount the filesystem
	mkdir -p /mnt/{dev,proc,sys}
	mount --rbind /dev /mnt/dev
	mount --rbind /proc /mnt/proc
	mount --rbind /sys /mnt/sys

        # Create build mount point and mount 
	mkdir -p /mnt/tmp
	mount /dev/xvdc /mnt/tmp
	chmod 777 /mnt/tmp

	# Copy apparmor profiles
	chmod 644 /tmp/apparmor_profiles/*
	cp -r /tmp/apparmor_profiles /mnt/tmp/

	# Copy migrations
	cp -r /tmp/migrations /mnt/tmp/

	# Copy unit tests 
	cp -r /tmp/unit-tests /mnt/tmp/

	# Copy the bootstrap script into place and execute inside chroot
	cp /tmp/chroot-bootstrap-nix.sh /mnt/tmp/chroot-bootstrap-nix.sh
	chroot /mnt /tmp/chroot-bootstrap-nix.sh
	rm -f /mnt/tmp/chroot-bootstrap-nix.sh
	echo "${POSTGRES_SUPABASE_VERSION}" > /mnt/root/supabase-release

	# Copy the nvme identification script into /sbin inside the chroot
	mkdir -p /mnt/sbin
	cp /tmp/ebsnvme-id /mnt/sbin/ebsnvme-id
	chmod +x /mnt/sbin/ebsnvme-id

	# Copy the udev rules for identifying nvme devices into the chroot
	mkdir -p /mnt/etc/udev/rules.d
	cp /tmp/70-ec2-nvme-devices.rules \
		/mnt/etc/udev/rules.d/70-ec2-nvme-devices.rules

	#Copy custom cloud-init
	rm -f /mnt/etc/cloud/cloud.cfg
	cp /tmp/cloud.cfg /mnt/etc/cloud/cloud.cfg

	sleep 2
}

function download_ccache {
	docker cp ccachedata:/build/ccache/. /mnt/tmp/ccache
}

function execute_playbook {

tee /etc/ansible/ansible.cfg <<EOF
[defaults]
callbacks_enabled = timer, profile_tasks, profile_roles
EOF
	# Run Ansible playbook
	#export ANSIBLE_LOG_PATH=/tmp/ansible.log && export ANSIBLE_DEBUG=True && export ANSIBLE_REMOTE_TEMP=/mnt/tmp 
	export ANSIBLE_LOG_PATH=/tmp/ansible.log && export ANSIBLE_REMOTE_TEMP=/mnt/tmp
	ansible-playbook -c chroot -i '/mnt,' /tmp/ansible-playbook/ansible/playbook.yml \
	--extra-vars '{"nixpkg_mode": true, "debpkg_mode": false, "stage2_nix": false} ' \
	--extra-vars "psql_version=psql_${POSTGRES_MAJOR_VERSION}" \
	$ARGS
}

function update_systemd_services {
	# Disable vector service and set timer unit.
	cp -v /tmp/vector.timer /mnt/etc/systemd/system/vector.timer
	rm -f /mnt/etc/systemd/system/multi-user.target.wants/vector.service
	ln -s /etc/systemd/system/vector.timer /mnt/etc/systemd/system/multi-user.target.wants/vector.timer

	# Disable services during first boot.
	rm -f /mnt/etc/systemd/system/sysinit.target.wants/apparmor.service
	rm -f /mnt/etc/systemd/system/multi-user.target.wants/postgresql.service
	rm -f /mnt/etc/systemd/system/multi-user.target.wants/salt-minion.service

	# Disable auditd
	rm -f /mnt/etc/systemd/system/multi-user.target.wants/auditd.service
}


function clean_system {
	# Copy cleanup scripts
	cp -v /tmp/ansible-playbook/scripts/90-cleanup.sh /mnt/tmp
	chmod +x /mnt/tmp/90-cleanup.sh
	chroot /mnt /tmp/90-cleanup.sh

	# Cleanup logs
	rm -rf /mnt/var/log/*
	# https://github.com/fail2ban/fail2ban/issues/1593
	touch /mnt/var/log/auth.log

	touch /mnt/var/log/pgbouncer.log
	if [ -f /usr/bin/chown ]; then
		chroot /mnt /usr/bin/chown pgbouncer:postgres /var/log/pgbouncer.log
	fi

	# Setup postgresql logs
	mkdir -p /mnt/var/log/postgresql
	if [ -f /usr/bin/chown ]; then
		chroot /mnt /usr/bin/chown postgres:postgres /var/log/postgresql
	fi

	# Setup wal-g logs
	mkdir /mnt/var/log/wal-g
	touch /mnt/var/log/wal-g/{backup-push.log,backup-fetch.log,wal-push.log,wal-fetch.log,pitr.log}

	#Creatre Sysstat directory for SAR
	mkdir /mnt/var/log/sysstat

	if [ -f /usr/bin/chown ]; then
		chroot /mnt /usr/bin/chown -R postgres:postgres /var/log/wal-g
		chroot /mnt /usr/bin/chmod -R 0300 /var/log/wal-g
	fi

	# audit logs directory for apparmor
	mkdir /mnt/var/log/audit

	# unwanted files
	rm -rf /mnt/var/lib/apt/lists/*
	rm -rf /mnt/root/.cache
	rm -rf /mnt/root/.vpython*
	rm -rf /mnt/root/go
	rm -rf /mnt/usr/share/doc

}

function upload_ccache {
	docker cp /mnt/tmp/ccache/. ccachedata:/build/ccache
	docker stop ccachedata
	docker commit ccachedata "${DOCKER_IMAGE}:${DOCKER_IMAGE_TAG}"
	echo ${DOCKER_PASSWD} | docker login --username ${DOCKER_USER} --password-stdin 
	docker push  "${DOCKER_IMAGE}:${DOCKER_IMAGE_TAG}"
}

# Unmount bind mounts
function umount_reset_mappings {
	umount -l /mnt/dev
	umount -l /mnt/proc
	umount -l /mnt/sys
	umount -l /mnt/tmp
	if [ "${ARCH}" = "arm64" ]; then
		umount /mnt/boot/efi
	fi
	umount /mnt/data
	umount /mnt

	# Reset device mappings
	for dev_link in "${blkdev_mappings[@]}" "${partdev_mappings[@]}"; do
	    if [[ -L "$dev_link" ]]; then
		rm -f "$dev_link"
	    fi
	done
}

waitfor_boot_finished
install_packages
device_partition_mappings
format_and_mount_rootfs
create_swapfile
format_build_partition
#pull_docker
setup_chroot_environment
#download_ccache
execute_playbook
update_systemd_services
#upload_ccache
clean_system
umount_reset_mappings

'''
'''--- /postgres/ebssurrogate/files/70-ec2-nvme-devices.rules ---
# Copyright (C) 2006-2016 Amazon.com, Inc. or its affiliates.
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#    http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS
# OF ANY KIND, either express or implied. See the License for the
# specific language governing permissions and limitations under the
# License.

#nvme-ns-* devices
KERNEL=="nvme[0-9]*n[0-9]*", ENV{DEVTYPE}=="disk", ATTRS{serial}=="?*", ATTRS{model}=="?*", SYMLINK+="disk/by-id/nvme-$attr{model}_$attr{serial}-ns-%n", OPTIONS+="string_escape=replace"

#nvme partitions
KERNEL=="nvme[0-9]*n[0-9]*p[0-9]*", ENV{DEVTYPE}=="partition", ATTRS{serial}=="?*", ATTRS{model}=="?*",  IMPORT{program}="ec2nvme-nsid %k"
KERNEL=="nvme[0-9]*n[0-9]*p[0-9]*", ENV{DEVTYPE}=="partition", ATTRS{serial}=="?*", ATTRS{model}=="?*",  ENV{_NS_ID}=="?*", SYMLINK+="disk/by-id/nvme-$attr{model}_$attr{serial}-ns-$env{_NS_ID}-part%n", OPTIONS+="string_escape=replace"

# ebs nvme devices
KERNEL=="nvme[0-9]*n[0-9]*",        ENV{DEVTYPE}=="disk",      ATTRS{model}=="Amazon Elastic Block Store", PROGRAM="/sbin/ebsnvme-id -u /dev/%k", SYMLINK+="%c"
KERNEL=="nvme[0-9]*n[0-9]*p[0-9]*", ENV{DEVTYPE}=="partition", ATTRS{model}=="Amazon Elastic Block Store", PROGRAM="/sbin/ebsnvme-id -u /dev/%k", SYMLINK+="%c%n"

'''
'''--- /postgres/ebssurrogate/files/sources.cfg ---
deb http://REGION.ec2.archive.ubuntu.com/ubuntu/ focal main restricted
deb http://REGION.ec2.archive.ubuntu.com/ubuntu/ focal-updates main restricted
deb http://REGION.ec2.archive.ubuntu.com/ubuntu/ focal universe
deb http://REGION.ec2.archive.ubuntu.com/ubuntu/ focal-updates universe
deb http://REGION.ec2.archive.ubuntu.com/ubuntu/ focal multiverse
deb http://REGION.ec2.archive.ubuntu.com/ubuntu/ focal-updates multiverse
deb http://REGION.ec2.archive.ubuntu.com/ubuntu/ focal-backports main restricted universe multiverse
deb http://security.ubuntu.com/ubuntu focal-security main restricted
deb http://security.ubuntu.com/ubuntu focal-security universe
deb http://security.ubuntu.com/ubuntu focal-security multiverse

'''
'''--- /postgres/ebssurrogate/files/vector.timer ---
[Unit]
Description= Defer running the vector.service 60secs after boot up

[Timer]
OnBootSec=60s
Unit=vector.service

[Install]
WantedBy=multi-user.target

'''
'''--- /postgres/ebssurrogate/files/cloud.cfg ---
# The top level settings are used as module
# and system configuration.
# A set of users which may be applied and/or used by various modules
# when a 'default' entry is found it will reference the 'default_user'
# from the distro configuration specified below
users:
   - default


# If this is set, 'root' will not be able to ssh in and they
# will get a message to login instead as the default $user
disable_root: true

# This will cause the set+update hostname module to not operate (if true)
preserve_hostname: false

# If you use datasource_list array, keep array items in a single line.
# If you use multi line array, ds-identify script won't read array items.
# Example datasource config
# datasource:
#    Ec2:
#      metadata_urls: [ 'blah.com' ]
#      timeout: 5 # (defaults to 50 seconds)
#      max_wait: 10 # (defaults to 120 seconds)



# The modules that run in the 'init' stage
cloud_init_modules:
# - migrator
# - seed_random
# - bootcmd
 - write-files
# - growpart
# - resizefs
# - disk_setup
# - mounts
 - set_hostname
 - update_hostname
 - update_etc_hosts
# - ca-certs
# - rsyslog
 - users-groups
 - ssh

# The modules that run in the 'config' stage
cloud_config_modules:
# Emit the cloud config ready event
# this can be used by upstart jobs for 'start on cloud-config'.
# - emit_upstart
# - snap
# - ssh-import-id
# - locale
# - set-passwords
# - grub-dpkg
# - apt-pipelining
# - apt-configure
# - ubuntu-advantage
 - ntp
 - timezone
 - disable-ec2-metadata
 - runcmd
# - byobu

# The modules that run in the 'final' stage
cloud_final_modules:
# - package-update-upgrade-install
# - fan
# - landscape
# - lxd
# - ubuntu-drivers
# - puppet
# - chef
# - mcollective
# - salt-minion
 - reset_rmc
 - refresh_rmc_and_interface
# - rightscale_userdata
 - scripts-vendor
 - scripts-per-once
 - scripts-per-boot
 - scripts-per-instance
 - scripts-user
# - ssh-authkey-fingerprints
# - keys-to-console
# - phone-home
 - final-message
 - power-state-change

# System and/or distro specific settings
# (not accessible to handlers/transforms)
system_info:
   # This will affect which distro class gets used
   distro: ubuntu
   # Default user name + that default users groups (if added/used)
   default_user:
     name: ubuntu
     lock_passwd: True
     gecos: Ubuntu
     groups: [adm, audio, cdrom, dialout, dip, floppy, lxd, netdev, plugdev, sudo, video]
     sudo: ["ALL=(ALL) NOPASSWD:ALL"]
     shell: /bin/bash
   network:
     renderers: ['netplan', 'eni', 'sysconfig']
   # Automatically discover the best ntp_client
   ntp_client: auto
   # Other config here will be given to the distro class and/or path classes
   paths:
      cloud_dir: /var/lib/cloud/
      templates_dir: /etc/cloud/templates/
      upstart_dir: /etc/init/
   package_mirrors:
     - arches: [i386, amd64]
       failsafe:
         primary: http://archive.ubuntu.com/ubuntu
         security: http://security.ubuntu.com/ubuntu
       search:
         primary:
           - http://%(ec2_region)s.ec2.archive.ubuntu.com/ubuntu/
           - http://%(availability_zone)s.clouds.archive.ubuntu.com/ubuntu/
           - http://%(region)s.clouds.archive.ubuntu.com/ubuntu/
         security: []
     - arches: [arm64, armel, armhf]
       failsafe:
         primary: http://ports.ubuntu.com/ubuntu-ports
         security: http://ports.ubuntu.com/ubuntu-ports
       search:
         primary:
           - http://%(ec2_region)s.ec2.ports.ubuntu.com/ubuntu-ports/
           - http://%(availability_zone)s.clouds.ports.ubuntu.com/ubuntu-ports/
           - http://%(region)s.clouds.ports.ubuntu.com/ubuntu-ports/
         security: []
     - arches: [default]
       failsafe:
         primary: http://ports.ubuntu.com/ubuntu-ports
         security: http://ports.ubuntu.com/ubuntu-ports
   ssh_svcname: ssh

'''
'''--- /postgres/ebssurrogate/files/sources-arm64.cfg ---
deb http://REGION.clouds.ports.ubuntu.com/ubuntu-ports/ focal main restricted
deb http://REGION.clouds.ports.ubuntu.com/ubuntu-ports/ focal-updates main restricted
deb http://REGION.clouds.ports.ubuntu.com/ubuntu-ports/ focal universe
deb http://REGION.clouds.ports.ubuntu.com/ubuntu-ports/ focal-updates universe
deb http://REGION.clouds.ports.ubuntu.com/ubuntu-ports/ focal multiverse
deb http://REGION.clouds.ports.ubuntu.com/ubuntu-ports/ focal-updates multiverse
deb http://REGION.clouds.ports.ubuntu.com/ubuntu-ports/ focal-backports main restricted universe multiverse
deb http://ports.ubuntu.com/ubuntu-ports focal-security main restricted
deb http://ports.ubuntu.com/ubuntu-ports focal-security universe
deb http://ports.ubuntu.com/ubuntu-ports focal-security multiverse

'''
'''--- /postgres/ebssurrogate/files/ebsnvme-id ---
#!/usr/bin/env python2.7

# Copyright (C) 2017 Amazon.com, Inc. or its affiliates.
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#    http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS
# OF ANY KIND, either express or implied. See the License for the
# specific language governing permissions and limitations under the
# License.

"""
Usage:
Read EBS device information and provide information about
the volume.
"""

import argparse
from ctypes import *
from fcntl import ioctl
import sys

NVME_ADMIN_IDENTIFY = 0x06
NVME_IOCTL_ADMIN_CMD = 0xC0484E41
AMZN_NVME_VID = 0x1D0F
AMZN_NVME_EBS_MN = "Amazon Elastic Block Store"

class nvme_admin_command(Structure):
    _pack_ = 1
    _fields_ = [("opcode", c_uint8),      # op code
                ("flags", c_uint8),       # fused operation
                ("cid", c_uint16),        # command id
                ("nsid", c_uint32),       # namespace id
                ("reserved0", c_uint64),
                ("mptr", c_uint64),       # metadata pointer
                ("addr", c_uint64),       # data pointer
                ("mlen", c_uint32),       # metadata length
                ("alen", c_uint32),       # data length
                ("cdw10", c_uint32),
                ("cdw11", c_uint32),
                ("cdw12", c_uint32),
                ("cdw13", c_uint32),
                ("cdw14", c_uint32),
                ("cdw15", c_uint32),
                ("reserved1", c_uint64)]

class nvme_identify_controller_amzn_vs(Structure):
    _pack_ = 1
    _fields_ = [("bdev", c_char * 32),  # block device name
                ("reserved0", c_char * (1024 - 32))]

class nvme_identify_controller_psd(Structure):
    _pack_ = 1
    _fields_ = [("mp", c_uint16),       # maximum power
                ("reserved0", c_uint16),
                ("enlat", c_uint32),     # entry latency
                ("exlat", c_uint32),     # exit latency
                ("rrt", c_uint8),       # relative read throughput
                ("rrl", c_uint8),       # relative read latency
                ("rwt", c_uint8),       # relative write throughput
                ("rwl", c_uint8),       # relative write latency
                ("reserved1", c_char * 16)]

class nvme_identify_controller(Structure):
    _pack_ = 1
    _fields_ = [("vid", c_uint16),          # PCI Vendor ID
                ("ssvid", c_uint16),        # PCI Subsystem Vendor ID
                ("sn", c_char * 20),        # Serial Number
                ("mn", c_char * 40),        # Module Number
                ("fr", c_char * 8),         # Firmware Revision
                ("rab", c_uint8),           # Recommend Arbitration Burst
                ("ieee", c_uint8 * 3),      # IEEE OUI Identifier
                ("mic", c_uint8),           # Multi-Interface Capabilities
                ("mdts", c_uint8),          # Maximum Data Transfer Size
                ("reserved0", c_uint8 * (256 - 78)),
                ("oacs", c_uint16),         # Optional Admin Command Support
                ("acl", c_uint8),           # Abort Command Limit
                ("aerl", c_uint8),          # Asynchronous Event Request Limit
                ("frmw", c_uint8),          # Firmware Updates
                ("lpa", c_uint8),           # Log Page Attributes
                ("elpe", c_uint8),          # Error Log Page Entries
                ("npss", c_uint8),          # Number of Power States Support
                ("avscc", c_uint8),         # Admin Vendor Specific Command Configuration
                ("reserved1", c_uint8 * (512 - 265)),
                ("sqes", c_uint8),          # Submission Queue Entry Size
                ("cqes", c_uint8),          # Completion Queue Entry Size
                ("reserved2", c_uint16),
                ("nn", c_uint32),            # Number of Namespaces
                ("oncs", c_uint16),         # Optional NVM Command Support
                ("fuses", c_uint16),        # Fused Operation Support
                ("fna", c_uint8),           # Format NVM Attributes
                ("vwc", c_uint8),           # Volatile Write Cache
                ("awun", c_uint16),         # Atomic Write Unit Normal
                ("awupf", c_uint16),        # Atomic Write Unit Power Fail
                ("nvscc", c_uint8),         # NVM Vendor Specific Command Configuration
                ("reserved3", c_uint8 * (704 - 531)),
                ("reserved4", c_uint8 * (2048 - 704)),
                ("psd", nvme_identify_controller_psd * 32),     # Power State Descriptor
                ("vs", nvme_identify_controller_amzn_vs)]  # Vendor Specific

class ebs_nvme_device:
    def __init__(self, device):
        self.device = device
        self.ctrl_identify()

    def _nvme_ioctl(self, id_response, id_len):
        admin_cmd = nvme_admin_command(opcode = NVME_ADMIN_IDENTIFY,
                                       addr = id_response,
                                       alen = id_len,
                                       cdw10 = 1)

        with open(self.device, "rw") as nvme:
            ioctl(nvme, NVME_IOCTL_ADMIN_CMD, admin_cmd)

    def ctrl_identify(self):
        self.id_ctrl = nvme_identify_controller()
        self._nvme_ioctl(addressof(self.id_ctrl), sizeof(self.id_ctrl))

        if self.id_ctrl.vid != AMZN_NVME_VID or self.id_ctrl.mn.strip() != AMZN_NVME_EBS_MN:
            raise TypeError("[ERROR] Not an EBS device: '{0}'".format(self.device))

    def get_volume_id(self):
        vol = self.id_ctrl.sn

        if vol.startswith("vol") and vol[3] != "-":
            vol = "vol-" + vol[3:]

        return vol

    def get_block_device(self, stripped=False):
        dev = self.id_ctrl.vs.bdev.strip()

        if stripped and dev.startswith("/dev/"):
            dev = dev[5:]

        return dev

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Reads EBS information from NVMe devices.")
    parser.add_argument("device", nargs=1, help="Device to query")

    display = parser.add_argument_group("Display Options")
    display.add_argument("-v", "--volume", action="store_true",
            help="Return volume-id")
    display.add_argument("-b", "--block-dev", action="store_true",
            help="Return block device mapping")
    display.add_argument("-u", "--udev", action="store_true",
            help="Output data in format suitable for udev rules")

    if len(sys.argv) < 2:
        parser.print_help()
        sys.exit(1)

    args = parser.parse_args()

    get_all = not (args.udev or args.volume or args.block_dev)

    try:
        dev = ebs_nvme_device(args.device[0])
    except (IOError, TypeError) as err:
        print >> sys.stderr, err
        sys.exit(1)

    if get_all or args.volume:
        print "Volume ID: {0}".format(dev.get_volume_id())
    if get_all or args.block_dev or args.udev:
        print dev.get_block_device(args.udev)

'''
'''--- /postgres/ebssurrogate/files/unit-tests/unit-test-01.sql ---
BEGIN;
CREATE EXTENSION IF NOT EXISTS pgtap;

DO $$ 
DECLARE
    extension_array text[];
    orioledb_available boolean;
BEGIN
    -- Check if orioledb is available
    SELECT EXISTS (
        SELECT 1 FROM pg_available_extensions WHERE name = 'orioledb'
    ) INTO orioledb_available;

    -- If available, create it and add to the expected extensions list
    IF orioledb_available THEN
        CREATE EXTENSION IF NOT EXISTS orioledb;
        extension_array := ARRAY[
            'plpgsql',
            'pg_stat_statements',
            'pgsodium',
            'pgtap',
            'pg_graphql',
            'pgcrypto',
            'pgjwt',
            'uuid-ossp',
            'supabase_vault',
            'orioledb'
        ];
    ELSE
        extension_array := ARRAY[
            'plpgsql',
            'pg_stat_statements',
            'pgsodium',
            'pgtap',
            'pg_graphql',
            'pgcrypto',
            'pgjwt',
            'uuid-ossp',
            'supabase_vault'
        ];
    END IF;

    -- Set the array as a temporary variable to use in the test
    PERFORM set_config('myapp.extensions', array_to_string(extension_array, ','), false);
END $$;

SELECT plan(8);

SELECT extensions_are(
    string_to_array(current_setting('myapp.extensions'), ',')::text[]
);


SELECT has_schema('pg_toast');
SELECT has_schema('pg_catalog');
SELECT has_schema('information_schema');
SELECT has_schema('public');

SELECT function_privs_are('pgsodium', 'crypto_aead_det_decrypt', array['bytea', 'bytea', 'uuid', 'bytea'], 'service_role', array['EXECUTE']);
SELECT function_privs_are('pgsodium', 'crypto_aead_det_encrypt', array['bytea', 'bytea', 'uuid', 'bytea'], 'service_role', array['EXECUTE']);
SELECT function_privs_are('pgsodium', 'crypto_aead_det_keygen', array[]::text[], 'service_role', array['EXECUTE']);

SELECT * FROM finish();
ROLLBACK;
'''
'''--- /postgres/ebssurrogate/files/apparmor_profiles/opt.postgrest ---
#include <tunables/global>

/opt/postgrest {
  #include <abstractions/base>
  #include <abstractions/nameservice>
  #include <abstractions/openssl>

  /etc/gss/mech.d/ r,
  /sys/devices/system/node/ r,
  /sys/devices/system/node/node0/meminfo r,
  owner /etc/postgrest/merged.conf r,
}

'''
'''--- /postgres/ebssurrogate/files/apparmor_profiles/usr.bin.vector ---
#include <tunables/global>

/usr/bin/vector flags=(attach_disconnected) {
  #include <abstractions/base>
  #include <abstractions/bash>
  #include <abstractions/consoles>
  #include <abstractions/dbus-session-strict>
  #include <abstractions/nameservice>
  #include <abstractions/openssl>
  #include <abstractions/ssl_keys>
  #include <abstractions/user-tmp>

  deny @{HOME}/** rwx,
  /etc/machine-id r,
  /etc/vector/** r,
  /proc/*/sched r,
  /proc/cmdline r,
  /proc/sys/kernel/osrelease r,
  /run/log/journal/ r,
  /var/log/journal/** r,
  /run/systemd/notify rw,
  /sys/firmware/efi/efivars/SecureBoot-8be4df61-93ca-11d2-aa0d-00e098032b8c r,
  /sys/fs/cgroup/cpu,cpuacct/cpu.cfs_quota_us r,
  /sys/kernel/mm/transparent_hugepage/enabled r,
  /usr/bin/journalctl mrix,
  /usr/bin/vector mrix,
  /var/lib/vector/** rw,
  /var/log/journal/ r,
  /var/log/postgresql/ r,
  /var/log/postgresql/** rw,
  /var/run/systemd/notify rw,
  owner /proc/*/cgroup r,
  owner /proc/*/mountinfo r,
  owner /proc/*/stat r,
}

'''
'''--- /postgres/ebssurrogate/files/apparmor_profiles/opt.gotrue.gotrue ---
#include <tunables/global>

/opt/gotrue/gotrue {
  #include <abstractions/base>
  #include <abstractions/nameservice>
  #include <abstractions/ssl_keys>

  /opt/gotrue/gotrue r,
  /opt/gotrue/migrations/ r,
  /etc/ssl/certs/java/* r,
  /opt/gotrue/migrations/** rw,
  /proc/sys/net/core/somaxconn r,
  /sys/kernel/mm/transparent_hugepage/hpage_pmd_size r,
  owner /etc/gotrue.env r,
}

'''
'''--- /postgres/ebssurrogate/files/apparmor_profiles/usr.local.bin.pgbouncer ---
#include <tunables/global>
profile /usr/local/bin/pgbouncer flags=(attach_disconnected) {
  #include <abstractions/base>
  #include <abstractions/bash>
  #include <abstractions/consoles>
  #include <abstractions/nameservice>
  #include <abstractions/openssl>
  #include <abstractions/ssl_keys>
  #include <abstractions/user-tmp>

  deny @{HOME}/** rwx,
  /etc/pgbouncer-custom/** r,
  /etc/pgbouncer/** r,
  /proc/sys/kernel/random/uuid r,
  /run/systemd/notify rw,
  /usr/local/bin/pgbouncer mrix,
  /var/log/pgbouncer.log rw,
  /var/run/systemd/notify rw,
  /{,var/}run/pgbouncer/** rw,
}

'''
'''--- /postgres/ebssurrogate/files/apparmor_profiles/usr.lib.postgresql.bin.postgres ---
#include <tunables/global>

profile /usr/lib/postgresql/bin/postgres flags=(attach_disconnected) {
#include <abstractions/base>
#include <abstractions/bash>
#include <abstractions/consoles>
#include <abstractions/nameservice>
#include <abstractions/openssl>
#include <abstractions/ssl_keys>
#include <abstractions/user-tmp>

capability dac_override,
capability dac_read_search,

deny @{HOME}/** rwx,

/data/pgdata/** r,
/dev/shm rw,
/etc/java-11-openjdk/logging.properties r,
/etc/java-11-openjdk/security/default.policy r,
/etc/java-11-openjdk/security/java.policy r,
/etc/java-11-openjdk/security/java.security r,
/etc/mecabrc r,
/etc/postgresql-custom/** r,
/etc/postgresql/** r,
/etc/timezone r,
/etc/wal-g/config.json r,
/run/systemd/notify rw,
/usr/bin/cat rix,
/usr/bin/dash rix,
/usr/bin/mknod rix,
/usr/bin/admin-mgr Ux,
/usr/lib/postgresql/bin/* mrix,
/usr/local/bin/wal-g rix,
/usr/local/lib/groonga/plugins/tokenizers/mecab.so mr,
/usr/local/lib/libSFCGAL.so.* mr,
/usr/local/lib/libgroonga.so.* mr,
/usr/local/pgsql/etc/pljava.policy r,
/usr/share/postgresql/** r,
/var/lib/mecab/** r,
/var/lib/postgresql/** rwl,
/var/log/postgresql/** rw,
/var/log/wal-g/** w,
/var/run/systemd/notify rw,
/{,var/}run/postgresql/** rw,
owner /data/pgdata/ r,
owner /data/pgdata/** rwl,
owner /data/pgdata/pgroonga.log k,
owner /dev/shm/ rw,
owner /dev/shm/PostgreSQL.* rw,
owner /sys/kernel/mm/transparent_hugepage/hpage_pmd_size r,
owner /var/log/wal-g/** rw,
owner @{PROC}/[0-9]*/oom_adj rw,

}

'''
