The below represents the folders and files from the root paths:
- /Users/barneycook/Desktop/code/ProjectRef/postgres/flake.nix
- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests

Each file is separated by '''--- followed by the file path and ending with ---.
File content begins immediately after its path and extends until the next '''---


*Directory: tests*
Total words: 12342

File structure:

tests/
    postgresql.conf.in
    prime.sql
    prime.sql.backup
smoke/
    0000-hello-world.sql
    0001-pg_graphql.sql
    0002-supautils.sql
    0003-pgsodium-vault.sql
    0004-index_advisor.sql
    0005-test_pgroonga_mecab.sql
util/
    pgsodium_getkey.sh
    pgsodium_getkey_arb.sh
expected/
    hypopg.out
    index_advisor.out
    pg-safeupdate.out
    pg_graphql.out
    pg_hashids.out
    pg_jsonschema.out
    pg_net.out
    pg_plan_filter.out
    pg_tle.out
    pgaudit.out
    pgjwt.out
    pgmq.out
    pgrouting.out
    pgsodium.out
    pgtap.out
    plpgsql-check.out
    postgis.out
    vault.out
    wal2json.out
    z_15_pg_stat_monitor.out
    z_15_pgroonga.out
    z_15_pgvector.out
    z_15_plv8.out
    z_15_rum.out
    z_15_timescale.out
    z_17_pg_stat_monitor.out
    z_17_pgvector.out
sql/
    hypopg.sql
    index_advisor.sql
    pg-safeupdate.sql
    pg_graphql.sql
    pg_hashids.sql
    pg_jsonschema.sql
    pg_net.sql
    pg_plan_filter.sql
    pg_tle.sql
    pgaudit.sql
    pgjwt.sql
    pgmq.sql
    pgrouting.sql
    pgsodium.sql
    pgtap.sql
    plpgsql-check.sql
    postgis.sql
    vault.sql
    wal2json.sql
    z_15_ext_interface.sql
    z_15_pg_stat_monitor.sql
    z_15_pgroonga.sql
    z_15_pgvector.sql
    z_15_plv8.sql
    z_15_rum.sql
    z_15_timescale.sql
    z_17_ext_interface.sql
    z_17_pg_stat_monitor.sql
    z_17_pgvector.sql


*File: flake.nix*
Words: 5336

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/prime.sql.backup ---
-- disable notice messages becuase they differ between 15 and 17
set client_min_messages = warning;

create extension if not exists address_standardizer;
create extension if not exists address_standardizer_data_us;
create extension if not exists amcheck;
create extension if not exists autoinc;
create extension if not exists bloom;
create extension if not exists btree_gin;
create extension if not exists btree_gist;
create extension if not exists citext;
create extension if not exists cube;
create extension if not exists dblink;
create extension if not exists dict_int;
create extension if not exists dict_xsyn;
create extension if not exists earthdistance;
create extension if not exists file_fdw;
create extension if not exists fuzzystrmatch;
create extension if not exists http;
create extension if not exists hstore;
create extension if not exists hypopg;
create extension if not exists index_advisor;
create extension if not exists insert_username;
create extension if not exists intagg;
create extension if not exists intarray;
create extension if not exists isn;
create extension if not exists lo;
create extension if not exists ltree;
create extension if not exists moddatetime;
create extension if not exists pageinspect;
create extension if not exists pg_backtrace;
create extension if not exists pg_buffercache;

/*
TODO: Does not enable locally mode
requires a change to postgresql.conf to set
cron.database_name = 'testing'
*/
-- create extension if not exists pg_cron;

create extension if not exists pg_net;
create extension if not exists pg_graphql;
create extension if not exists pg_freespacemap;
create extension if not exists pg_hashids;
create extension if not exists pg_prewarm;
create extension if not exists pgmq;
create extension if not exists pg_jsonschema;
create extension if not exists pg_repack;
create extension if not exists pg_stat_monitor;
create extension if not exists pg_stat_statements;
create extension if not exists pg_surgery;
create extension if not exists pg_tle;
create extension if not exists pg_trgm;
create extension if not exists pg_visibility;
create extension if not exists pg_walinspect;
create extension if not exists pgaudit;
create extension if not exists pgcrypto;
create extension if not exists pgtap;
create extension if not exists pgjwt;
create extension if not exists pgroonga;
create extension if not exists pgroonga_database;
create extension if not exists pgsodium;
create extension if not exists pgrowlocks;
create extension if not exists pgstattuple;
create extension if not exists plpgsql_check;
create extension if not exists postgis;
create extension if not exists postgis_raster;
create extension if not exists postgis_sfcgal;
create extension if not exists postgis_topology;
create extension if not exists pgrouting; -- requires postgis
create extension if not exists postgres_fdw;
create extension if not exists rum;
create extension if not exists refint;
create extension if not exists seg;
create extension if not exists sslinfo;
create extension if not exists supabase_vault;
create extension if not exists tablefunc;
create extension if not exists tcn;
create extension if not exists tsm_system_rows;
-- create extension if not exists tsm_system_time; not supported in apache license
create extension if not exists unaccent;
create extension if not exists "uuid-ossp";
create extension if not exists vector;
create extension if not exists wrappers;
create extension if not exists xml2;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/postgresql.conf.in ---
# -----------------------------
# PostgreSQL configuration file

# -----------------------------
#
# This file consists of lines of the form:
#
#   name = value
#
# (The "=" is optional.)  Whitespace may be used.  Comments are introduced with
# "#" anywhere on a line.  The complete list of parameter names and allowed
# values can be found in the PostgreSQL documentation.
#
# The commented-out settings shown in this file represent the default values.
# Re-commenting a setting is NOT sufficient to revert it to the default value;
# you need to reload the server.
#
# This file is read on server startup and when the server receives a SIGHUP
# signal.  If you edit the file on a running system, you have to SIGHUP the
# server for the changes to take effect, run "pg_ctl reload", or execute
# "SELECT pg_reload_conf()".  Some parameters, which are marked below,
# require a server shutdown and restart to take effect.
#
# Any parameter can also be given as a command-line option to the server, e.g.,
# "postgres -c log_connections=on".  Some parameters can be changed at run time
# with the "SET" SQL command.
#
# Memory units:  B  = bytes            Time units:  us  = microseconds
#                kB = kilobytes                     ms  = milliseconds
#                MB = megabytes                     s   = seconds
#                GB = gigabytes                     min = minutes
#                TB = terabytes                     h   = hours
#                                                   d   = days


#------------------------------------------------------------------------------
# FILE LOCATIONS
#------------------------------------------------------------------------------

# The default values of these variables are driven from the -D command-line
# option or PGDATA environment variable, represented here as ConfigDir.

#data_directory = 'ConfigDir'		# use data in another directory
					# (change requires restart)
#hba_file = 'ConfigDir/pg_hba.conf'	# host-based authentication file
					# (change requires restart)
#ident_file = 'ConfigDir/pg_ident.conf'	# ident configuration file
					# (change requires restart)

# If external_pid_file is not explicitly set, no extra PID file is written.
#external_pid_file = ''			# write an extra PID file
					# (change requires restart)


#------------------------------------------------------------------------------
# CONNECTIONS AND AUTHENTICATION
#------------------------------------------------------------------------------

# - Connection Settings -

listen_addresses = '*'		# what IP address(es) to listen on;
#port = @PGSQL_DEFAULT_PORT@				# (change requires restart)
max_connections = 100			# (change requires restart)
#superuser_reserved_connections = 3	# (change requires restart)
unix_socket_directories = '/tmp'	# comma-separated list of directories
					# (change requires restart)
#unix_socket_group = ''			# (change requires restart)
#unix_socket_permissions = 0777		# begin with 0 to use octal notation
					# (change requires restart)
#bonjour = off				# advertise server via Bonjour
					# (change requires restart)
#bonjour_name = ''			# defaults to the computer name
					# (change requires restart)

# - TCP settings -
# see "man tcp" for details

#tcp_keepalives_idle = 0		# TCP_KEEPIDLE, in seconds;
					# 0 selects the system default
#tcp_keepalives_interval = 0		# TCP_KEEPINTVL, in seconds;
					# 0 selects the system default
#tcp_keepalives_count = 0		# TCP_KEEPCNT;
					# 0 selects the system default
#tcp_user_timeout = 0			# TCP_USER_TIMEOUT, in milliseconds;
					# 0 selects the system default

#client_connection_check_interval = 0	# time between checks for client
					# disconnection while running queries;
					# 0 for never

# - Authentication -

#authentication_timeout = 1min		# 1s-600s
#password_encryption = scram-sha-256	# scram-sha-256 or md5
#db_user_namespace = off

# GSSAPI using Kerberos
#krb_server_keyfile = 'FILE:${sysconfdir}/krb5.keytab'
#krb_caseins_users = off

# - SSL -

#ssl = off
#ssl_ca_file = ''
#ssl_cert_file = 'server.crt'
#ssl_crl_file = ''
#ssl_crl_dir = ''
#ssl_key_file = 'server.key'
#ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # allowed SSL ciphers
#ssl_prefer_server_ciphers = on
#ssl_ecdh_curve = 'prime256v1'
#ssl_min_protocol_version = 'TLSv1.2'
#ssl_max_protocol_version = ''
#ssl_dh_params_file = ''
#ssl_passphrase_command = ''
#ssl_passphrase_command_supports_reload = off


#------------------------------------------------------------------------------
# RESOURCE USAGE (except WAL)
#------------------------------------------------------------------------------

# - Memory -

shared_buffers = 128MB			# min 128kB
					# (change requires restart)
#huge_pages = try			# on, off, or try
					# (change requires restart)
#huge_page_size = 0			# zero for system default
					# (change requires restart)
#temp_buffers = 8MB			# min 800kB
#max_prepared_transactions = 0		# zero disables the feature
					# (change requires restart)
# Caution: it is not advisable to set max_prepared_transactions nonzero unless
# you actively intend to use prepared transactions.
#work_mem = 4MB				# min 64kB
#hash_mem_multiplier = 1.0		# 1-1000.0 multiplier on hash table work_mem
#maintenance_work_mem = 64MB		# min 1MB
#autovacuum_work_mem = -1		# min 1MB, or -1 to use maintenance_work_mem
#logical_decoding_work_mem = 64MB	# min 64kB
#max_stack_depth = 2MB			# min 100kB
#shared_memory_type = mmap		# the default is the first option
					# supported by the operating system:
					#   mmap
					#   sysv
					#   windows
					# (change requires restart)
dynamic_shared_memory_type = posix	# the default is the first option
					# supported by the operating system:
					#   posix
					#   sysv
					#   windows
					#   mmap
					# (change requires restart)
#min_dynamic_shared_memory = 0MB	# (change requires restart)

# - Disk -

#temp_file_limit = -1			# limits per-process temp file space
					# in kilobytes, or -1 for no limit

# - Kernel Resources -

#max_files_per_process = 1000		# min 64
					# (change requires restart)

# - Cost-Based Vacuum Delay -

#vacuum_cost_delay = 0			# 0-100 milliseconds (0 disables)
#vacuum_cost_page_hit = 1		# 0-10000 credits
#vacuum_cost_page_miss = 2		# 0-10000 credits
#vacuum_cost_page_dirty = 20		# 0-10000 credits
#vacuum_cost_limit = 200		# 1-10000 credits

# - Background Writer -

#bgwriter_delay = 200ms			# 10-10000ms between rounds
#bgwriter_lru_maxpages = 100		# max buffers written/round, 0 disables
#bgwriter_lru_multiplier = 2.0		# 0-10.0 multiplier on buffers scanned/round
#bgwriter_flush_after = 512kB		# measured in pages, 0 disables

# - Asynchronous Behavior -

#backend_flush_after = 0		# measured in pages, 0 disables
#effective_io_concurrency = 1		# 1-1000; 0 disables prefetching
#maintenance_io_concurrency = 10	# 1-1000; 0 disables prefetching
#max_worker_processes = 8		# (change requires restart)
#max_parallel_workers_per_gather = 2	# taken from max_parallel_workers
#max_parallel_maintenance_workers = 2	# taken from max_parallel_workers
#max_parallel_workers = 8		# maximum number of max_worker_processes that
					# can be used in parallel operations
#parallel_leader_participation = on
#old_snapshot_threshold = -1		# 1min-60d; -1 disables; 0 is immediate
					# (change requires restart)


#------------------------------------------------------------------------------
# WRITE-AHEAD LOG
#------------------------------------------------------------------------------

# - Settings -

wal_level = logical			# minimal, replica, or logical
					# (change requires restart)
#fsync = on				# flush data to disk for crash safety
					# (turning this off can cause
					# unrecoverable data corruption)
#synchronous_commit = on		# synchronization level;
					# off, local, remote_write, remote_apply, or on
#wal_sync_method = fsync		# the default is the first option
					# supported by the operating system:
					#   open_datasync
					#   fdatasync (default on Linux and FreeBSD)
					#   fsync
					#   fsync_writethrough
					#   open_sync
#full_page_writes = on			# recover from partial page writes
wal_log_hints = on			# also do full page writes of non-critical updates
					# (change requires restart)
#wal_compression = off			# enable compression of full-page writes
#wal_init_zero = on			# zero-fill new WAL files
#wal_recycle = on			# recycle WAL files
#wal_buffers = -1			# min 32kB, -1 sets based on shared_buffers
					# (change requires restart)
#wal_writer_delay = 200ms		# 1-10000 milliseconds
#wal_writer_flush_after = 1MB		# measured in pages, 0 disables
#wal_skip_threshold = 2MB

#commit_delay = 0			# range 0-100000, in microseconds
#commit_siblings = 5			# range 1-1000

# - Checkpoints -

#checkpoint_timeout = 5min		# range 30s-1d
#checkpoint_completion_target = 0.9	# checkpoint target duration, 0.0 - 1.0
#checkpoint_flush_after = 256kB		# measured in pages, 0 disables
#checkpoint_warning = 30s		# 0 disables
max_wal_size = 1GB
min_wal_size = 80MB

# - Archiving -

#archive_mode = off		# enables archiving; off, on, or always
				# (change requires restart)
#archive_command = ''		# command to use to archive a logfile segment
				# placeholders: %p = path of file to archive
				#               %f = file name only
				# e.g. 'test ! -f /mnt/server/archivedir/%f && cp %p /mnt/server/archivedir/%f'
#archive_timeout = 0		# force a logfile segment switch after this
				# number of seconds; 0 disables

# - Archive Recovery -

# These are only used in recovery mode.

#restore_command = ''		# command to use to restore an archived logfile segment
				# placeholders: %p = path of file to restore
				#               %f = file name only
				# e.g. 'cp /mnt/server/archivedir/%f %p'
#archive_cleanup_command = ''	# command to execute at every restartpoint
#recovery_end_command = ''	# command to execute at completion of recovery

# - Recovery Target -

# Set these only when performing a targeted recovery.

#recovery_target = ''		# 'immediate' to end recovery as soon as a
                                # consistent state is reached
				# (change requires restart)
#recovery_target_name = ''	# the named restore point to which recovery will proceed
				# (change requires restart)
#recovery_target_time = ''	# the time stamp up to which recovery will proceed
				# (change requires restart)
#recovery_target_xid = ''	# the transaction ID up to which recovery will proceed
				# (change requires restart)
#recovery_target_lsn = ''	# the WAL LSN up to which recovery will proceed
				# (change requires restart)
#recovery_target_inclusive = on # Specifies whether to stop:
				# just after the specified recovery target (on)
				# just before the recovery target (off)
				# (change requires restart)
#recovery_target_timeline = 'latest'	# 'current', 'latest', or timeline ID
				# (change requires restart)
#recovery_target_action = 'pause'	# 'pause', 'promote', 'shutdown'
				# (change requires restart)


#------------------------------------------------------------------------------
# REPLICATION
#------------------------------------------------------------------------------

# - Sending Servers -

# Set these on the primary and on any standby that will send replication data.

#max_wal_senders = 10		# max number of walsender processes
				# (change requires restart)
#max_replication_slots = 10	# max number of replication slots
				# (change requires restart)
#wal_keep_size = 0		# in megabytes; 0 disables
#max_slot_wal_keep_size = -1	# in megabytes; -1 disables
#wal_sender_timeout = 60s	# in milliseconds; 0 disables
#track_commit_timestamp = off	# collect timestamp of transaction commit
				# (change requires restart)

# - Primary Server -

# These settings are ignored on a standby server.

#synchronous_standby_names = ''	# standby servers that provide sync rep
				# method to choose sync standbys, number of sync standbys,
				# and comma-separated list of application_name
				# from standby(s); '*' = all
#vacuum_defer_cleanup_age = 0	# number of xacts by which cleanup is delayed

# - Standby Servers -

# These settings are ignored on a primary server.

#primary_conninfo = ''			# connection string to sending server
#primary_slot_name = ''			# replication slot on sending server
#promote_trigger_file = ''		# file name whose presence ends recovery
#hot_standby = on			# "off" disallows queries during recovery
					# (change requires restart)
#max_standby_archive_delay = 30s	# max delay before canceling queries
					# when reading WAL from archive;
					# -1 allows indefinite delay
#max_standby_streaming_delay = 30s	# max delay before canceling queries
					# when reading streaming WAL;
					# -1 allows indefinite delay
#wal_receiver_create_temp_slot = off	# create temp slot if primary_slot_name
					# is not set
#wal_receiver_status_interval = 10s	# send replies at least this often
					# 0 disables
#hot_standby_feedback = off		# send info from standby to prevent
					# query conflicts
#wal_receiver_timeout = 60s		# time that receiver waits for
					# communication from primary
					# in milliseconds; 0 disables
#wal_retrieve_retry_interval = 5s	# time to wait before retrying to
					# retrieve WAL after a failed attempt
#recovery_min_apply_delay = 0		# minimum delay for applying changes during recovery

# - Subscribers -

# These settings are ignored on a publisher.

#max_logical_replication_workers = 4	# taken from max_worker_processes
					# (change requires restart)
#max_sync_workers_per_subscription = 2	# taken from max_logical_replication_workers


#------------------------------------------------------------------------------
# QUERY TUNING
#------------------------------------------------------------------------------

# - Planner Method Configuration -

#enable_async_append = on
#enable_bitmapscan = on
#enable_gathermerge = on
#enable_hashagg = on
#enable_hashjoin = on
#enable_incremental_sort = on
#enable_indexscan = on
#enable_indexonlyscan = on
#enable_material = on
#enable_memoize = on
#enable_mergejoin = on
#enable_nestloop = on
#enable_parallel_append = on
#enable_parallel_hash = on
#enable_partition_pruning = on
#enable_partitionwise_join = off
#enable_partitionwise_aggregate = off
#enable_seqscan = on
#enable_sort = on
#enable_tidscan = on

# - Planner Cost Constants -

#seq_page_cost = 1.0			# measured on an arbitrary scale
#random_page_cost = 4.0			# same scale as above
#cpu_tuple_cost = 0.01			# same scale as above
#cpu_index_tuple_cost = 0.005		# same scale as above
#cpu_operator_cost = 0.0025		# same scale as above
#parallel_setup_cost = 1000.0	# same scale as above
#parallel_tuple_cost = 0.1		# same scale as above
#min_parallel_table_scan_size = 8MB
#min_parallel_index_scan_size = 512kB
#effective_cache_size = 4GB

#jit_above_cost = 100000		# perform JIT compilation if available
					# and query more expensive than this;
					# -1 disables
#jit_inline_above_cost = 500000		# inline small functions if query is
					# more expensive than this; -1 disables
#jit_optimize_above_cost = 500000	# use expensive JIT optimizations if
					# query is more expensive than this;
					# -1 disables

# - Genetic Query Optimizer -

#geqo = on
#geqo_threshold = 12
#geqo_effort = 5			# range 1-10
#geqo_pool_size = 0			# selects default based on effort
#geqo_generations = 0			# selects default based on effort
#geqo_selection_bias = 2.0		# range 1.5-2.0
#geqo_seed = 0.0			# range 0.0-1.0

# - Other Planner Options -

#default_statistics_target = 100	# range 1-10000
#constraint_exclusion = partition	# on, off, or partition
#cursor_tuple_fraction = 0.1		# range 0.0-1.0
#from_collapse_limit = 8
#jit = on				# allow JIT compilation
#join_collapse_limit = 8		# 1 disables collapsing of explicit
					# JOIN clauses
#plan_cache_mode = auto			# auto, force_generic_plan or
					# force_custom_plan


#------------------------------------------------------------------------------
# REPORTING AND LOGGING
#------------------------------------------------------------------------------

# - Where to Log -

#log_destination = 'stderr'		# Valid values are combinations of
					# stderr, csvlog, syslog, and eventlog,
					# depending on platform.  csvlog
					# requires logging_collector to be on.

# This is used when logging to stderr:
#logging_collector = off		# Enable capturing of stderr and csvlog
					# into log files. Required to be on for
					# csvlogs.
					# (change requires restart)

# These are only used if logging_collector is on:
#log_directory = 'log'			# directory where log files are written,
					# can be absolute or relative to PGDATA
#log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'	# log file name pattern,
					# can include strftime() escapes
#log_file_mode = 0600			# creation mode for log files,
					# begin with 0 to use octal notation
#log_rotation_age = 1d			# Automatic rotation of logfiles will
					# happen after that time.  0 disables.
#log_rotation_size = 10MB		# Automatic rotation of logfiles will
					# happen after that much log output.
					# 0 disables.
#log_truncate_on_rotation = off		# If on, an existing log file with the
					# same name as the new log file will be
					# truncated rather than appended to.
					# But such truncation only occurs on
					# time-driven rotation, not on restarts
					# or size-driven rotation.  Default is
					# off, meaning append to existing files
					# in all cases.

# These are relevant when logging to syslog:
#syslog_facility = 'LOCAL0'
#syslog_ident = 'postgres'
#syslog_sequence_numbers = on
#syslog_split_messages = on

# This is only relevant when logging to eventlog (Windows):
# (change requires restart)
#event_source = 'PostgreSQL'

# - When to Log -

#log_min_messages = warning		# values in order of decreasing detail:
					#   debug5
					#   debug4
					#   debug3
					#   debug2
					#   debug1
					#   info
					#   notice
					#   warning
					#   error
					#   log
					#   fatal
					#   panic

#log_min_error_statement = error	# values in order of decreasing detail:
					#   debug5
					#   debug4
					#   debug3
					#   debug2
					#   debug1
					#   info
					#   notice
					#   warning
					#   error
					#   log
					#   fatal
					#   panic (effectively off)

#log_min_duration_statement = -1	# -1 is disabled, 0 logs all statements
					# and their durations, > 0 logs only
					# statements running at least this number
					# of milliseconds

#log_min_duration_sample = -1		# -1 is disabled, 0 logs a sample of statements
					# and their durations, > 0 logs only a sample of
					# statements running at least this number
					# of milliseconds;
					# sample fraction is determined by log_statement_sample_rate

#log_statement_sample_rate = 1.0	# fraction of logged statements exceeding
					# log_min_duration_sample to be logged;
					# 1.0 logs all such statements, 0.0 never logs


#log_transaction_sample_rate = 0.0	# fraction of transactions whose statements
					# are logged regardless of their duration; 1.0 logs all
					# statements from all transactions, 0.0 never logs

# - What to Log -

#debug_print_parse = off
#debug_print_rewritten = off
#debug_print_plan = off
#debug_pretty_print = on
#log_autovacuum_min_duration = -1	# log autovacuum activity;
					# -1 disables, 0 logs all actions and
					# their durations, > 0 logs only
					# actions running at least this number
					# of milliseconds.
#log_checkpoints = off
#log_connections = off
#log_disconnections = off
#log_duration = off
#log_error_verbosity = default		# terse, default, or verbose messages
#log_hostname = off
#log_line_prefix = '%m [%p] '		# special values:
					#   %a = application name
					#   %u = user name
					#   %d = database name
					#   %r = remote host and port
					#   %h = remote host
					#   %b = backend type
					#   %p = process ID
					#   %P = process ID of parallel group leader
					#   %t = timestamp without milliseconds
					#   %m = timestamp with milliseconds
					#   %n = timestamp with milliseconds (as a Unix epoch)
					#   %Q = query ID (0 if none or not computed)
					#   %i = command tag
					#   %e = SQL state
					#   %c = session ID
					#   %l = session line number
					#   %s = session start timestamp
					#   %v = virtual transaction ID
					#   %x = transaction ID (0 if none)
					#   %q = stop here in non-session
					#        processes
					#   %% = '%'
					# e.g. '<%u%%%d> '
#log_lock_waits = off			# log lock waits >= deadlock_timeout
#log_recovery_conflict_waits = off	# log standby recovery conflict waits
					# >= deadlock_timeout
#log_parameter_max_length = -1		# when logging statements, limit logged
					# bind-parameter values to N bytes;
					# -1 means print in full, 0 disables
#log_parameter_max_length_on_error = 0	# when logging an error, limit logged
					# bind-parameter values to N bytes;
					# -1 means print in full, 0 disables
#log_statement = 'none'			# none, ddl, mod, all
#log_replication_commands = off
#log_temp_files = -1			# log temporary files equal or larger
					# than the specified size in kilobytes;
					# -1 disables, 0 logs all temp files
log_timezone = 'America/Chicago'


#------------------------------------------------------------------------------
# PROCESS TITLE
#------------------------------------------------------------------------------

#cluster_name = ''			# added to process titles if nonempty
					# (change requires restart)
#update_process_title = on


#------------------------------------------------------------------------------
# STATISTICS
#------------------------------------------------------------------------------

# - Query and Index Statistics Collector -

#track_activities = on
#track_activity_query_size = 1024	# (change requires restart)
#track_counts = on
#track_io_timing = off
#track_wal_io_timing = off
#track_functions = none			# none, pl, all
#stats_temp_directory = 'pg_stat_tmp'


# - Monitoring -

#compute_query_id = auto
#log_statement_stats = off
#log_parser_stats = off
#log_planner_stats = off
#log_executor_stats = off


#------------------------------------------------------------------------------
# AUTOVACUUM
#------------------------------------------------------------------------------

#autovacuum = on			# Enable autovacuum subprocess?  'on'
					# requires track_counts to also be on.
#autovacuum_max_workers = 3		# max number of autovacuum subprocesses
					# (change requires restart)
#autovacuum_naptime = 1min		# time between autovacuum runs
#autovacuum_vacuum_threshold = 50	# min number of row updates before
					# vacuum
#autovacuum_vacuum_insert_threshold = 1000	# min number of row inserts
					# before vacuum; -1 disables insert
					# vacuums
#autovacuum_analyze_threshold = 50	# min number of row updates before
					# analyze
#autovacuum_vacuum_scale_factor = 0.2	# fraction of table size before vacuum
#autovacuum_vacuum_insert_scale_factor = 0.2	# fraction of inserts over table
					# size before insert vacuum
#autovacuum_analyze_scale_factor = 0.1	# fraction of table size before analyze
#autovacuum_freeze_max_age = 200000000	# maximum XID age before forced vacuum
					# (change requires restart)
#autovacuum_multixact_freeze_max_age = 400000000	# maximum multixact age
					# before forced vacuum
					# (change requires restart)
#autovacuum_vacuum_cost_delay = 2ms	# default vacuum cost delay for
					# autovacuum, in milliseconds;
					# -1 means use vacuum_cost_delay
#autovacuum_vacuum_cost_limit = -1	# default vacuum cost limit for
					# autovacuum, -1 means use
					# vacuum_cost_limit


#------------------------------------------------------------------------------
# CLIENT CONNECTION DEFAULTS
#------------------------------------------------------------------------------

# - Statement Behavior -

#client_min_messages = notice		# values in order of decreasing detail:
					#   debug5
					#   debug4
					#   debug3
					#   debug2
					#   debug1
					#   log
					#   notice
					#   warning
					#   error
#search_path = '"$user", public'	# schema names
#row_security = on
#default_table_access_method = 'heap'
#default_tablespace = ''		# a tablespace name, '' uses the default
#default_toast_compression = 'pglz'	# 'pglz' or 'lz4'
#temp_tablespaces = ''			# a list of tablespace names, '' uses
					# only default tablespace
#check_function_bodies = on
#default_transaction_isolation = 'read committed'
#default_transaction_read_only = off
#default_transaction_deferrable = off
#session_replication_role = 'origin'
#statement_timeout = 0			# in milliseconds, 0 is disabled
#lock_timeout = 0			# in milliseconds, 0 is disabled
#idle_in_transaction_session_timeout = 0	# in milliseconds, 0 is disabled
#idle_session_timeout = 0		# in milliseconds, 0 is disabled
#vacuum_freeze_table_age = 150000000
#vacuum_freeze_min_age = 50000000
#vacuum_failsafe_age = 1600000000
#vacuum_multixact_freeze_table_age = 150000000
#vacuum_multixact_freeze_min_age = 5000000
#vacuum_multixact_failsafe_age = 1600000000
#bytea_output = 'hex'			# hex, escape
#xmlbinary = 'base64'
#xmloption = 'content'
#gin_pending_list_limit = 4MB

# - Locale and Formatting -

datestyle = 'iso, mdy'
#intervalstyle = 'postgres'
timezone = 'America/Chicago'
#timezone_abbreviations = 'Default'     # Select the set of available time zone
					# abbreviations.  Currently, there are
					#   Default
					#   Australia (historical usage)
					#   India
					# You can create your own file in
					# share/timezonesets/.
#extra_float_digits = 1			# min -15, max 3; any value >0 actually
					# selects precise output mode
#client_encoding = sql_ascii		# actually, defaults to database
					# encoding

# These settings are initialized by initdb, but they can be changed.
lc_messages = 'C'			# locale for system error message
					# strings
lc_monetary = 'C'			# locale for monetary formatting
lc_numeric = 'C'			# locale for number formatting
lc_time = 'C'				# locale for time formatting

# default configuration for text search
default_text_search_config = 'pg_catalog.english'

# - Shared Library Preloading -

#local_preload_libraries = ''
#session_preload_libraries = ''
shared_preload_libraries = 'pg_stat_statements, pgaudit, plpgsql, plpgsql_check, pg_cron, pg_net, pgsodium, timescaledb, auto_explain, pg_tle, plan_filter, pg_backtrace'	# (change requires restart)
jit_provider = 'llvmjit'		# JIT library to use


# - Other Defaults -

#dynamic_library_path = '$libdir'
#gin_fuzzy_search_limit = 0


#------------------------------------------------------------------------------
# LOCK MANAGEMENT
#------------------------------------------------------------------------------

#deadlock_timeout = 1s
#max_locks_per_transaction = 64		# min 10
					# (change requires restart)
#max_pred_locks_per_transaction = 64	# min 10
					# (change requires restart)
#max_pred_locks_per_relation = -2	# negative values mean
					# (max_pred_locks_per_transaction
					#  / -max_pred_locks_per_relation) - 1
#max_pred_locks_per_page = 2            # min 0


#------------------------------------------------------------------------------
# VERSION AND PLATFORM COMPATIBILITY
#------------------------------------------------------------------------------

# - Previous PostgreSQL Versions -

#array_nulls = on
#backslash_quote = safe_encoding	# on, off, or safe_encoding
#escape_string_warning = on
#lo_compat_privileges = off
#quote_all_identifiers = off
#standard_conforming_strings = on
#synchronize_seqscans = on

# - Other Platforms and Clients -

#transform_null_equals = off


#------------------------------------------------------------------------------
# ERROR HANDLING
#------------------------------------------------------------------------------

#exit_on_error = off			# terminate session on any error?
#restart_after_crash = on		# reinitialize after backend crash?
#data_sync_retry = off			# retry or panic on failure to fsync
					# data?
					# (change requires restart)
#recovery_init_sync_method = fsync	# fsync, syncfs (Linux 5.8+)


#------------------------------------------------------------------------------
# CONFIG FILE INCLUDES
#------------------------------------------------------------------------------

# These options allow settings to be loaded from files other than the
# default postgresql.conf.  Note that these are directives, not variable
# assignments, so they can usefully be given more than once.

#include_dir = '...'			# include files ending in '.conf' from
					# a directory, e.g., 'conf.d'
#include_if_exists = '...'		# include file only if it exists
#include = '...'			# include file


#------------------------------------------------------------------------------
# CUSTOMIZED OPTIONS
#------------------------------------------------------------------------------

# Add settings for extensions here

pgsodium.getkey_script = '@PGSODIUM_GETKEY_SCRIPT@'

auto_explain.log_min_duration = 10s
cron.database_name = 'postgres'

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/prime.sql ---
-- disable notice messages becuase they differ between 15 and 17
set client_min_messages = warning;

create extension if not exists address_standardizer;
create extension if not exists address_standardizer_data_us;
create extension if not exists amcheck;
create extension if not exists autoinc;
create extension if not exists bloom;
create extension if not exists btree_gin;
create extension if not exists btree_gist;
create extension if not exists citext;
create extension if not exists cube;
create extension if not exists dblink;
create extension if not exists dict_int;
create extension if not exists dict_xsyn;
create extension if not exists earthdistance;
create extension if not exists file_fdw;
create extension if not exists fuzzystrmatch;
create extension if not exists http;
create extension if not exists hstore;
create extension if not exists hypopg;
create extension if not exists index_advisor;
create extension if not exists insert_username;
create extension if not exists intagg;
create extension if not exists intarray;
create extension if not exists isn;
create extension if not exists lo;
create extension if not exists ltree;
create extension if not exists moddatetime;
create extension if not exists pageinspect;
create extension if not exists pg_backtrace;
create extension if not exists age;
create extension if not exists pg_buffercache;

/*
TODO: Does not enable locally mode
requires a change to postgresql.conf to set
cron.database_name = 'testing'
*/
-- create extension if not exists pg_cron;

create extension if not exists pg_net;
create extension if not exists pg_graphql;
create extension if not exists pg_freespacemap;
create extension if not exists pg_hashids;
create extension if not exists pg_prewarm;
create extension if not exists pgmq;
create extension if not exists pg_jsonschema;
create extension if not exists pg_repack;
create extension if not exists pg_stat_monitor;
create extension if not exists pg_stat_statements;
create extension if not exists pg_surgery;
create extension if not exists pg_tle;
create extension if not exists pg_trgm;
create extension if not exists pg_visibility;
create extension if not exists pg_walinspect;
create extension if not exists pgaudit;
create extension if not exists pgcrypto;
create extension if not exists pgtap;
create extension if not exists pgjwt;
create extension if not exists pgroonga;
create extension if not exists pgroonga_database;
create extension if not exists pgsodium;
create extension if not exists pgrowlocks;
create extension if not exists pgstattuple;
create extension if not exists plpgsql_check;
create extension if not exists postgis;
create extension if not exists postgis_raster;
create extension if not exists postgis_sfcgal;
create extension if not exists postgis_topology;
create extension if not exists pgrouting; -- requires postgis
create extension if not exists postgres_fdw;
create extension if not exists rum;
create extension if not exists refint;
create extension if not exists seg;
create extension if not exists sslinfo;
create extension if not exists supabase_vault;
create extension if not exists tablefunc;
create extension if not exists tcn;
create extension if not exists tsm_system_rows;
-- create extension if not exists tsm_system_time; not supported in apache license
create extension if not exists unaccent;
create extension if not exists "uuid-ossp";
create extension if not exists vector;
create extension if not exists wrappers;
create extension if not exists xml2;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/smoke/0000-hello-world.sql ---
-- Start transaction and plan the tests.
BEGIN;
SELECT plan(1);

-- Run the tests.
SELECT pass( 'My test passed, w00t!' );

-- Finish the tests and clean up.
SELECT * FROM finish();
ROLLBACK;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/smoke/0005-test_pgroonga_mecab.sql ---
-- File: 0005-test_pgroonga_revised.sql

begin;
    -- Plan for 3 tests: extension, table, and index
    select plan(3);
    
    -- Create the PGroonga extension
    create extension if not exists pgroonga;
    
    -- -- Test 1: Check if PGroonga extension exists
    select has_extension('pgroonga', 'The pgroonga extension should exist.');
    
    -- Create the table
    create table notes(
        id integer primary key,
        content text
    );
    
    -- Test 2: Check if the table was created
    SELECT has_table('public', 'notes', 'The notes table should exist.');    
    -- Create the PGroonga index
    CREATE INDEX pgroonga_content_index
            ON notes
         USING pgroonga (content)
          WITH (tokenizer='TokenMecab');
    
    -- -- Test 3: Check if the index was created
    SELECT has_index('public', 'notes', 'pgroonga_content_index', 'The pgroonga_content_index should exist.');
    
    -- -- Cleanup (this won't affect the test results as they've already been checked)
    DROP INDEX IF EXISTS pgroonga_content_index;
    DROP TABLE IF EXISTS notes;
    
    -- Finish the test plan
    select * from finish();
rollback;
'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/smoke/0004-index_advisor.sql ---
-- Start transaction and plan the tests.
begin;
    select plan(1);

    create extension if not exists index_advisor;

    create table account(
        id int primary key,
        is_verified bool
    );

    select is(
      (select count(1) from index_advisor('select id from public.account where is_verified;'))::int,
      1,
      'index_advisor returns 1 row'
    );

    select * from finish();
rollback;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/smoke/0001-pg_graphql.sql ---
-- Start transaction and plan the tests.
begin;
    select plan(1);

    create extension if not exists pg_graphql;

    create table account(
        id int primary key,
        is_verified bool,
        name text,
        phone text
    );

    insert into public.account(id, is_verified, name, phone)
    values
        (1, true, 'foo', '1111111111'),
        (2, true, 'bar', null),
        (3, false, 'baz', '33333333333');

    select is(
      graphql.resolve($$
        {
        accountCollection {
	        edges {
	          node {
		        id
	          }
	        }
          }
        }
        $$),
        '{
           "data": {
             "accountCollection": {
               "edges": [
                 {
                   "node": {
                     "id": 1
                    }
                 },
                 {
                   "node": {
                     "id": 2
                   }
                 },
                 {
                   "node": {
                     "id": 3
                   }
                 }
               ]
             }
           }
        }'::jsonb
    );


    select * from finish();
rollback;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/smoke/0003-pgsodium-vault.sql ---
BEGIN;

select plan(3);

select id as test_new_key_id from pgsodium.create_key(name:='test_new_key') \gset

select vault.create_secret (
	's3kr3t_k3y', 'a_name', 'this is the foo secret key') test_secret_id \gset

select vault.create_secret (
	's3kr3t_k3y_2', 'another_name', 'this is another foo key',
	(select id from pgsodium.key where name = 'test_new_key')) test_secret_id_2 \gset

SELECT results_eq(
    $$
    SELECT decrypted_secret = 's3kr3t_k3y', description = 'this is the foo secret key'
    FROM vault.decrypted_secrets WHERE name = 'a_name';
    $$,
    $$VALUES (true, true)$$,
    'can select from masking view with custom key');

SELECT results_eq(
    $$
    SELECT decrypted_secret = 's3kr3t_k3y_2', description = 'this is another foo key'
    FROM vault.decrypted_secrets WHERE key_id = (select id from pgsodium.key where name = 'test_new_key');
    $$,
    $$VALUES (true, true)$$,
    'can select from masking view');

SELECT lives_ok(
	format($test$
	select vault.update_secret(
	    %L::uuid, new_name:='a_new_name',
	    new_secret:='new_s3kr3t_k3y', new_description:='this is the bar key')
	$test$, :'test_secret_id'),
	'can update name, secret and description'
	);

SELECT * FROM finish();
ROLLBACK;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/smoke/0002-supautils.sql ---
BEGIN;
SELECT plan(2);

-- the setting doesn't exist when supautils is not loaded
SELECT throws_ok($$
  select current_setting('supautils.privileged_extensions', false)
$$);

LOAD 'supautils';

-- now it does
SELECT ok(
  current_setting('supautils.privileged_extensions', false) = ''
);

SELECT * FROM finish();
ROLLBACK;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/util/pgsodium_getkey_arb.sh ---
echo -n 8359dafbba5c05568799c1c24eb6c2fbff497654bc6aa5e9a791c666768875a1
'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/util/pgsodium_getkey.sh ---
#!/usr/bin/env bash

set -euo pipefail

KEY_FILE="${1:-/tmp/pgsodium.key}"

if [[ ! -f "$KEY_FILE" ]]; then
    head -c 32 /dev/urandom | od -A n -t x1 | tr -d ' \n' > "$KEY_FILE"
fi
cat $KEY_FILE

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/z_15_rum.out ---
/*
This extension is excluded from oriole-17 because it uses an unsupported index type
*/
create schema v;
create table v.test_rum(
  t text,
  a tsvector
);
create trigger tsvectorupdate
  before update or insert on v.test_rum
  for each row
  execute procedure
    tsvector_update_trigger(
      'a',
      'pg_catalog.english',
      't'
    );
insert into v.test_rum(t)
values
  ('the situation is most beautiful'),
  ('it is a beautiful'),
  ('it looks like a beautiful place');
create index rumidx on v.test_rum using rum (a rum_tsvector_ops);
select
  t,
  round(a <=> to_tsquery('english', 'beautiful | place')) as rank
from
  v.test_rum
where
  a @@ to_tsquery('english', 'beautiful | place')
order by
  a <=> to_tsquery('english', 'beautiful | place');
                t                | rank 
---------------------------------+------
 it looks like a beautiful place |    8
 the situation is most beautiful |   16
 it is a beautiful               |   16
(3 rows)

drop schema v cascade;
NOTICE:  drop cascades to table v.test_rum

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pgjwt.out ---
select
  sign(
    payload   := '{"sub":"1234567890","name":"John Doe","iat":1516239022}',
    secret    := 'secret',
    algorithm := 'HS256'
  );
                                                                            sign                                                                             
-------------------------------------------------------------------------------------------------------------------------------------------------------------
 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.XbPfbIHMI6arZ3Y922BhjWgQzWXcXNrz0ogtVhfEd2o
(1 row)

select
  verify(
    token := 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiRm9vIn0.Q8hKjuadCEhnCPuqIj9bfLhTh_9QSxshTRsA5Aq4IuM',
    secret    := 'secret',
    algorithm := 'HS256'
  );
                             verify                             
----------------------------------------------------------------
 ("{""alg"":""HS256"",""typ"":""JWT""}","{""name"":""Foo""}",t)
(1 row)


'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/plpgsql-check.out ---
create schema v;
create table v.t1(
  a int,
  b int
);
create or replace function v.f1()
  returns void
  language plpgsql
as $$
declare r record;
begin
  for r in select * from v.t1
  loop
    raise notice '%', r.c; -- there is bug - table t1 missing "c" column
  end loop;
end;
$$;
select * from v.f1();
 f1 
----
 
(1 row)

-- use plpgsql_check_function to check the function for errors
select * from plpgsql_check_function('v.f1()');
             plpgsql_check_function              
-------------------------------------------------
 error:42703:6:RAISE:record "r" has no field "c"
 Context: SQL expression "r.c"
(2 rows)

drop schema v cascade;
NOTICE:  drop cascades to 2 other objects
DETAIL:  drop cascades to table v.t1
drop cascades to function v.f1()

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/vault.out ---
select
  1
from
  vault.create_secret('my_s3kre3t');
 ?column? 
----------
        1
(1 row)

select
  1
from
  vault.create_secret(
    'another_s3kre3t',
    'unique_name',
    'This is the description'
  );
 ?column? 
----------
        1
(1 row)

insert into vault.secrets (secret)
values
  ('s3kre3t_k3y');
select
  name,
  description
from
  vault.decrypted_secrets 
order by
  created_at desc 
limit
  3;
    name     |       description       
-------------+-------------------------
             | 
 unique_name | This is the description
             | 
(3 rows)

 

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/z_17_pgvector.out ---
/*
This test excludes indexes shipped with pgvector because orioledb doesn't support them yet
*/
create schema v;
create table v.items(
  id serial primary key,
  embedding vector(3),
  half_embedding halfvec(3),
  bit_embedding bit(3),
  sparse_embedding sparsevec(3)
);
-- Populate some records
insert into v.items(
    embedding,
    half_embedding,
    bit_embedding,
    sparse_embedding
)
values
  ('[1,2,3]', '[1,2,3]', '101', '{1:4}/3'),
  ('[2,3,4]', '[2,3,4]', '010', '{1:7,3:0}/3');
-- Test op types
select
  *
from
  v.items
order by
  embedding <-> '[2,3,5]',
  embedding <=> '[2,3,5]',
  embedding <+> '[2,3,5]',
  embedding <#> '[2,3,5]',
  half_embedding <-> '[2,3,5]',
  half_embedding <=> '[2,3,5]',
  half_embedding <+> '[2,3,5]',
  half_embedding <#> '[2,3,5]',
  sparse_embedding <-> '{2:4,3:1}/3',
  sparse_embedding <=> '{2:4,3:1}/3',
  sparse_embedding <+> '{2:4,3:1}/3',
  sparse_embedding <#> '{2:4,3:1}/3',
  bit_embedding <~> '011';
 id | embedding | half_embedding | bit_embedding | sparse_embedding 
----+-----------+----------------+---------------+------------------
  2 | [2,3,4]   | [2,3,4]        | 010           | {1:7}/3
  1 | [1,2,3]   | [1,2,3]        | 101           | {1:4}/3
(2 rows)

select
  avg(embedding),
  avg(half_embedding)
from
  v.items;
      avg      |      avg      
---------------+---------------
 [1.5,2.5,3.5] | [1.5,2.5,3.5]
(1 row)

-- Cleanup
drop schema v cascade;
NOTICE:  drop cascades to table v.items

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/z_15_pg_stat_monitor.out ---
select
  *
from
  pg_stat_monitor
where
  false;
 bucket | bucket_start_time | userid | username | dbid | datname | client_ip | pgsm_query_id | queryid | toplevel | top_queryid | query | comments | planid | query_plan | top_query | application_name | relations | cmd_type | cmd_type_text | elevel | sqlcode | message | calls | total_exec_time | min_exec_time | max_exec_time | mean_exec_time | stddev_exec_time | rows | shared_blks_hit | shared_blks_read | shared_blks_dirtied | shared_blks_written | local_blks_hit | local_blks_read | local_blks_dirtied | local_blks_written | temp_blks_read | temp_blks_written | blk_read_time | blk_write_time | temp_blk_read_time | temp_blk_write_time | resp_calls | cpu_user_time | cpu_sys_time | wal_records | wal_fpi | wal_bytes | bucket_done | plans | total_plan_time | min_plan_time | max_plan_time | mean_plan_time | stddev_plan_time | jit_functions | jit_generation_time | jit_inlining_count | jit_inlining_time | jit_optimization_count | jit_optimization_time | jit_emission_count | jit_emission_time 
--------+-------------------+--------+----------+------+---------+-----------+---------------+---------+----------+-------------+-------+----------+--------+------------+-----------+------------------+-----------+----------+---------------+--------+---------+---------+-------+-----------------+---------------+---------------+----------------+------------------+------+-----------------+------------------+---------------------+---------------------+----------------+-----------------+--------------------+--------------------+----------------+-------------------+---------------+----------------+--------------------+---------------------+------------+---------------+--------------+-------------+---------+-----------+-------------+-------+-----------------+---------------+---------------+----------------+------------------+---------------+---------------------+--------------------+-------------------+------------------------+-----------------------+--------------------+-------------------
(0 rows)


'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pg_net.out ---
-- This is a very basic test because you can't get the value returned
-- by a pg_net request in the same transaction that created it;
select
  net.http_get (
    'https://postman-echo.com/get?foo1=bar1&foo2=bar2'
  ) as request_id;
 request_id 
------------
          1
(1 row)


'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/z_17_pg_stat_monitor.out ---
select
  *
from
  pg_stat_monitor
where
  false;
 bucket | bucket_start_time | userid | username | dbid | datname | client_ip | pgsm_query_id | queryid | toplevel | top_queryid | query | comments | planid | query_plan | top_query | application_name | relations | cmd_type | cmd_type_text | elevel | sqlcode | message | calls | total_exec_time | min_exec_time | max_exec_time | mean_exec_time | stddev_exec_time | rows | shared_blks_hit | shared_blks_read | shared_blks_dirtied | shared_blks_written | local_blks_hit | local_blks_read | local_blks_dirtied | local_blks_written | temp_blks_read | temp_blks_written | shared_blk_read_time | shared_blk_write_time | local_blk_read_time | local_blk_write_time | temp_blk_read_time | temp_blk_write_time | resp_calls | cpu_user_time | cpu_sys_time | wal_records | wal_fpi | wal_bytes | bucket_done | plans | total_plan_time | min_plan_time | max_plan_time | mean_plan_time | stddev_plan_time | jit_functions | jit_generation_time | jit_inlining_count | jit_inlining_time | jit_optimization_count | jit_optimization_time | jit_emission_count | jit_emission_time | jit_deform_count | jit_deform_time | stats_since | minmax_stats_since 
--------+-------------------+--------+----------+------+---------+-----------+---------------+---------+----------+-------------+-------+----------+--------+------------+-----------+------------------+-----------+----------+---------------+--------+---------+---------+-------+-----------------+---------------+---------------+----------------+------------------+------+-----------------+------------------+---------------------+---------------------+----------------+-----------------+--------------------+--------------------+----------------+-------------------+----------------------+-----------------------+---------------------+----------------------+--------------------+---------------------+------------+---------------+--------------+-------------+---------+-----------+-------------+-------+-----------------+---------------+---------------+----------------+------------------+---------------+---------------------+--------------------+-------------------+------------------------+-----------------------+--------------------+-------------------+------------------+-----------------+-------------+--------------------
(0 rows)


'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pg_plan_filter.out ---
begin;
  load 'plan_filter';
  create schema v;
  -- create a sample table
  create table v.test_table (
    id serial primary key,
    data text
  );
  -- insert some test data
  insert into v.test_table (data)
  values ('sample1'), ('sample2'), ('sample3');
  set local plan_filter.statement_cost_limit = 0.001;
  select * from v.test_table;
ERROR:  plan cost limit exceeded
HINT:  The plan for your query shows that it would probably have an excessive run time. This may be due to a logic error in the SQL, or it maybe just a very costly query. Rewrite your query or increase the configuration parameter "plan_filter.statement_cost_limit".
rollback;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pg_hashids.out ---
select id_encode(1001); -- Result: jNl
 id_encode 
-----------
 jNl
(1 row)

select id_encode(1234567, 'This is my salt'); -- Result: Pdzxp
 id_encode 
-----------
 Pdzxp
(1 row)

select id_encode(1234567, 'This is my salt', 10); -- Result: PlRPdzxpR7
 id_encode  
------------
 PlRPdzxpR7
(1 row)

select id_encode(1234567, 'This is my salt', 10, 'abcdefghijABCDxFGHIJ1234567890'); -- Result: 3GJ956J9B9
 id_encode  
------------
 3GJ956J9B9
(1 row)

select id_decode('PlRPdzxpR7', 'This is my salt', 10); -- Result: 1234567
 id_decode 
-----------
 {1234567}
(1 row)

select id_decode('3GJ956J9B9', 'This is my salt', 10, 'abcdefghijABCDxFGHIJ1234567890'); -- Result: 1234567
 id_decode 
-----------
 {1234567}
(1 row)


'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/postgis.out ---
create schema v;
-- create a table to store geographic points
create table v.places (
    id serial primary key,
    name text,
    geom geometry(point, 4326)  -- using WGS 84 coordinate system
);
-- insert some sample geographic points into the places table
insert into v.places (name, geom)
values
  ('place_a', st_setsrid(st_makepoint(-73.9857, 40.7484), 4326)),  -- latitude and longitude for a location
  ('place_b', st_setsrid(st_makepoint(-74.0060, 40.7128), 4326)),  -- another location
  ('place_c', st_setsrid(st_makepoint(-73.9687, 40.7851), 4326));  -- yet another location
-- calculate the distance between two points (in meters)
select
  a.name as place_a,
  b.name as place_b,
  st_distance(a.geom::geography, b.geom::geography) as distance_meters
from
  v.places a,
  v.places b
where
  a.name = 'place_a'
  and b.name = 'place_b';
 place_a | place_b | distance_meters 
---------+---------+-----------------
 place_a | place_b |   4309.25283351
(1 row)

-- find all places within a 5km radius of 'place_a'
select
  name,
  st_distance(
    geom::geography,
    (
      select
        geom
      from
        v.places
      where
        name = 'place_a'
    )::geography) as distance_meters
from
  v.places
where
  st_dwithin(
    geom::geography,
    (select geom from v.places where name = 'place_a')::geography,
    5000
  )
  and name != 'place_a';
  name   | distance_meters 
---------+-----------------
 place_b |   4309.25283351
 place_c |    4320.8765634
(2 rows)

drop schema v cascade;
NOTICE:  drop cascades to table v.places

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/wal2json.out ---
create schema v;
create table v.foo(
  id int primary key
);
select
  1
from
  pg_create_logical_replication_slot('reg_test', 'wal2json', false);
 ?column? 
----------
        1
(1 row)

insert into v.foo(id) values (1);
select
  data
from
  pg_logical_slot_get_changes(
	'reg_test',
    null,
    null,
	'include-pk', '1',
	'include-transaction', 'false',
	'include-timestamp', 'false',
	'include-type-oids', 'false',
	'format-version', '2',
	'actions', 'insert,update,delete'
  ) x;
                                                                 data                                                                 
--------------------------------------------------------------------------------------------------------------------------------------
 {"action":"I","schema":"v","table":"foo","columns":[{"name":"id","type":"integer","value":1}],"pk":[{"name":"id","type":"integer"}]}
(1 row)

select
  pg_drop_replication_slot('reg_test');
 pg_drop_replication_slot 
--------------------------
 
(1 row)

drop schema v cascade;
NOTICE:  drop cascades to table v.foo

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/z_15_pgroonga.out ---
create schema v;
create table v.roon(
  id serial primary key,
  content text
);
with tokenizers as (
  select
    x
  from
    jsonb_array_elements(
      (select pgroonga_command('tokenizer_list'))::jsonb
    ) x(val)
  limit
    1
  offset
    1 -- first record is unrelated and not stable
)
select
  t.x::jsonb ->> 'name'
from
  jsonb_array_elements((select * from tokenizers)) t(x)
order by
  t.x::jsonb ->> 'name';
                  ?column?                   
---------------------------------------------
 TokenBigram
 TokenBigramIgnoreBlank
 TokenBigramIgnoreBlankSplitSymbol
 TokenBigramIgnoreBlankSplitSymbolAlpha
 TokenBigramIgnoreBlankSplitSymbolAlphaDigit
 TokenBigramSplitSymbol
 TokenBigramSplitSymbolAlpha
 TokenBigramSplitSymbolAlphaDigit
 TokenDelimit
 TokenDelimitNull
 TokenDocumentVectorBM25
 TokenDocumentVectorTFIDF
 TokenMecab
 TokenNgram
 TokenPattern
 TokenRegexp
 TokenTable
 TokenTrigram
 TokenUnigram
(19 rows)

insert into v.roon (content)
values
  ('Hello World'),
  ('PostgreSQL with PGroonga is a thing'),
  ('This is a full-text search test'),
  ('PGroonga supports various languages');
-- Create default index
create index pgroonga_index on v.roon using pgroonga (content);
-- Create mecab tokenizer index since we had a bug with this one once
create index pgroonga_index_mecab on v.roon using pgroonga (content) with (tokenizer='TokenMecab');
-- Run some queries to test the index
select * from v.roon where content &@~ 'Hello';
 id |   content   
----+-------------
  1 | Hello World
(1 row)

select * from v.roon where content &@~ 'powerful';
 id | content 
----+---------
(0 rows)

select * from v.roon where content &@~ 'supports';
 id |               content               
----+-------------------------------------
  4 | PGroonga supports various languages
(1 row)

drop schema v cascade;
NOTICE:  drop cascades to table v.roon

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pg-safeupdate.out ---
load 'safeupdate';
set safeupdate.enabled=1;
create schema v;
create table v.foo(
  id int,
  val text
);
update v.foo
  set val = 'bar';
ERROR:  UPDATE requires a WHERE clause
drop schema v cascade;
NOTICE:  drop cascades to table v.foo

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/hypopg.out ---
create schema v;
create table v.samp(
  id int
);
select 1 from hypopg_create_index($$
  create index on v.samp(id)
$$);
 ?column? 
----------
        1
(1 row)

drop schema v cascade;
NOTICE:  drop cascades to table v.samp

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pgtap.out ---
begin;
select plan(1);
 plan 
------
 1..1
(1 row)

-- Run the tests.
select pass( 'My test passed, w00t!' );
             pass             
------------------------------
 ok 1 - My test passed, w00t!
(1 row)

-- Finish the tests and clean up.
select * from finish();
 finish 
--------
(0 rows)

rollback;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pgsodium.out ---
select
  status
from
  pgsodium.create_key();
 status 
--------
 valid
(1 row)


'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pgaudit.out ---
-- Note: there is no test that the logs were correctly output. Only checking for exceptions
set pgaudit.log = 'write, ddl';
set pgaudit.log_relation = on;
set pgaudit.log_level = notice;
create schema v;
create table v.account(
  id int,
  name text,
  password text,
  description text
);
insert into v.account (id, name, password, description)
values (1, 'user1', 'HASH1', 'blah, blah');
select
  *
from
  v.account;
 id | name  | password | description 
----+-------+----------+-------------
  1 | user1 | HASH1    | blah, blah
(1 row)

drop schema v cascade;
NOTICE:  drop cascades to table v.account

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pg_graphql.out ---
begin;
    comment on schema public is '@graphql({"inflect_names": true})';
    create table account(
        id serial primary key,
        email varchar(255) not null,
        priority int,
        status text default 'active'
    );
    create table blog(
        id serial primary key,
        owner_id integer not null references account(id)
    );
    comment on table blog is e'@graphql({"totalCount": {"enabled": true}})';
    -- Make sure functions still work
    create function _echo_email(account)
        returns text
        language sql
    as $$ select $1.email $$;
    /*
        Literals
    */
    select graphql.resolve($$
    mutation {
      insertIntoAccountCollection(objects: [
        { email: "foo@barsley.com", priority: 1 },
        { email: "bar@foosworth.com" }
      ]) {
        affectedCount
        records {
          id
          status
          echoEmail
          blogCollection {
            totalCount
          }
        }
      }
    }
    $$);
                                                                                                                                        resolve                                                                                                                                         
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 {"data": {"insertIntoAccountCollection": {"records": [{"id": 1, "status": "active", "echoEmail": "foo@barsley.com", "blogCollection": {"totalCount": 0}}, {"id": 2, "status": "active", "echoEmail": "bar@foosworth.com", "blogCollection": {"totalCount": 0}}], "affectedCount": 2}}}
(1 row)

    select graphql.resolve($$
    mutation {
      insertIntoBlogCollection(objects: [{
        ownerId: 1
      }]) {
        records {
          id
          owner {
            id
          }
        }
      }
    }
    $$);
                                       resolve                                        
--------------------------------------------------------------------------------------
 {"data": {"insertIntoBlogCollection": {"records": [{"id": 1, "owner": {"id": 1}}]}}}
(1 row)

    -- Override a default on status with null
    select graphql.resolve($$
    mutation {
      insertIntoAccountCollection(objects: [
        { email: "baz@baz.com", status: null },
      ]) {
        affectedCount
        records {
          email
          status
        }
      }
    }
    $$);
                                                        resolve                                                         
------------------------------------------------------------------------------------------------------------------------
 {"data": {"insertIntoAccountCollection": {"records": [{"email": "baz@baz.com", "status": null}], "affectedCount": 1}}}
(1 row)

    /*
        Variables
    */
    select graphql.resolve($$
    mutation newAccount($emailAddress: String) {
       xyz: insertIntoAccountCollection(objects: [
        { email: $emailAddress },
        { email: "other@email.com" }
       ]) {
        affectedCount
        records {
          id
          email
        }
      }
    }
    $$,
    variables := '{"emailAddress": "foo@bar.com"}'::jsonb
    );
                                                            resolve                                                             
--------------------------------------------------------------------------------------------------------------------------------
 {"data": {"xyz": {"records": [{"id": 4, "email": "foo@bar.com"}, {"id": 5, "email": "other@email.com"}], "affectedCount": 2}}}
(1 row)

    -- Variable override of default with null results in null
    select graphql.resolve($$
    mutation newAccount($status: String) {
       xyz: insertIntoAccountCollection(objects: [
        { email: "1@email.com", status: $status}
       ]) {
        affectedCount
        records {
          email
          status
        }
      }
    }
    $$,
    variables := '{"status": null}'::jsonb
    );
                                            resolve                                             
------------------------------------------------------------------------------------------------
 {"data": {"xyz": {"records": [{"email": "1@email.com", "status": null}], "affectedCount": 1}}}
(1 row)

    -- Skipping variable override of default results in default
    select graphql.resolve($$
    mutation newAccount($status: String) {
       xyz: insertIntoAccountCollection(objects: [
        { email: "x@y.com", status: $status},
       ]) {
        affectedCount
        records {
          email
          status
        }
      }
    }
    $$,
    variables := '{}'::jsonb
    );
                                            resolve                                             
------------------------------------------------------------------------------------------------
 {"data": {"xyz": {"records": [{"email": "x@y.com", "status": "active"}], "affectedCount": 1}}}
(1 row)

    select graphql.resolve($$
    mutation newAccount($acc: AccountInsertInput!) {
       insertIntoAccountCollection(objects: [$acc]) {
        affectedCount
        records {
          id
          email
        }
      }
    }
    $$,
    variables := '{"acc": {"email": "bar@foo.com"}}'::jsonb
    );
                                                     resolve                                                     
-----------------------------------------------------------------------------------------------------------------
 {"data": {"insertIntoAccountCollection": {"records": [{"id": 8, "email": "bar@foo.com"}], "affectedCount": 1}}}
(1 row)

    select graphql.resolve($$
    mutation newAccounts($acc: [AccountInsertInput!]!) {
       insertIntoAccountCollection(objects: $accs) {
        affectedCount
        records {
          id
          email
        }
      }
    }
    $$,
    variables := '{"accs": [{"email": "bar@foo.com"}]}'::jsonb
    );
                                                     resolve                                                     
-----------------------------------------------------------------------------------------------------------------
 {"data": {"insertIntoAccountCollection": {"records": [{"id": 9, "email": "bar@foo.com"}], "affectedCount": 1}}}
(1 row)

    -- Single object coerces to a list
    select graphql.resolve($$
    mutation {
      insertIntoBlogCollection(objects: {ownerId: 1}) {
        affectedCount
      }
    }
    $$);
                           resolve                            
--------------------------------------------------------------
 {"data": {"insertIntoBlogCollection": {"affectedCount": 1}}}
(1 row)

    /*
        Errors
    */
    -- Field does not exist
    select graphql.resolve($$
    mutation createAccount($acc: AccountInsertInput) {
       insertIntoAccountCollection(objects: [$acc]) {
        affectedCount
        records {
          id
          email
        }
      }
    }
    $$,
    variables := '{"acc": {"doesNotExist": "other"}}'::jsonb
    );
                                                       resolve                                                       
---------------------------------------------------------------------------------------------------------------------
 {"data": null, "errors": [{"message": "Input for type AccountInsertInput contains extra keys [\"doesNotExist\"]"}]}
(1 row)

    -- Wrong input type (list of string, not list of object)
    select graphql.resolve($$
    mutation {
      insertIntoBlogCollection(objects: ["not an object"]) {
        affectedCount
      }
    }
    $$);
                                      resolve                                      
-----------------------------------------------------------------------------------
 {"data": null, "errors": [{"message": "Invalid input for BlogInsertInput type"}]}
(1 row)

    -- objects argument is missing
    select graphql.resolve($$
    mutation {
      insertIntoBlogCollection {
        affectedCount
      }
    }
    $$);
                                  resolve                                  
---------------------------------------------------------------------------
 {"data": null, "errors": [{"message": "Invalid input for NonNull type"}]}
(1 row)

    -- Empty call
    select graphql.resolve($$
    mutation {
      insertIntoBlogCollection(objects: []) {
        affectedCount
      }
    }
    $$);
                                          resolve                                           
--------------------------------------------------------------------------------------------
 {"data": null, "errors": [{"message": "At least one record must be provided to objects"}]}
(1 row)

rollback;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pg_jsonschema.out ---
begin;
-- Test json_matches_schema
create table customer(
    id serial primary key,
    metadata json,
    check (
        json_matches_schema(
            '{
                "type": "object",
                "properties": {
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "maxLength": 16
                        }
                    }
                }
            }',
            metadata
        )
    )
);
insert into customer(metadata)
values ('{"tags": ["vip", "darkmode-ui"]}');
-- Test jsonb_matches_schema
select
  jsonb_matches_schema(
  '{
    "type": "object",
    "properties": {
	  "tags": {
        "type": "array",
        "items": {
          "type": "string",
          "maxLength": 16
        }
      }
      }
  }',
  '{"tags": ["vip", "darkmode-ui"]}'::jsonb
);
 jsonb_matches_schema 
----------------------
 t
(1 row)

-- Test jsonschema_is_valid
select
  jsonschema_is_valid(
  '{
    "type": "object",
    "properties": {
	  "tags": {
        "type": "array",
        "items": {
          "type": "string",
          "maxLength": 16
        }
      }
    }
  }');
 jsonschema_is_valid 
---------------------
 t
(1 row)

-- Test invalid payload
insert into customer(metadata)
values ('{"tags": [1, 3]}');
ERROR:  new row for relation "customer" violates check constraint "customer_metadata_check"
DETAIL:  Failing row contains (2, {"tags": [1, 3]}).
rollback;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pgrouting.out ---
create schema v;
-- create the roads table
create table v.roads (
  id serial primary key,
  source integer,
  target integer,
  cost double precision
);
-- insert sample data into roads table
insert into v.roads (source, target, cost) values
(1, 2, 1.0),
(2, 3, 1.0),
(3, 4, 1.0),
(1, 3, 2.5),
(3, 5, 2.0);
-- create a function to use pgRouting to find the shortest path
select * from pgr_dijkstra(
  'select id, source, target, cost from v.roads',
  1, -- start node
  4  -- end node
);
 seq | path_seq | node | edge | cost | agg_cost 
-----+----------+------+------+------+----------
   1 |        1 |    1 |    1 |    1 |        0
   2 |        2 |    2 |    2 |    1 |        1
   3 |        3 |    3 |    3 |    1 |        2
   4 |        4 |    4 |   -1 |    0 |        3
(4 rows)

drop schema v cascade;
NOTICE:  drop cascades to table v.roads

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/z_15_plv8.out ---
/*
This test is excluded from the Postgres 17 suite because it does not ship
with the Supabase PG17 image
*/
create extension if not exists plv8;
NOTICE:  extension "plv8" already exists, skipping
create schema v;
-- create a function to perform some JavaScript operations
create function v.multiply_numbers(a integer, b integer)
  returns integer
  language plv8
as $$
  return a * b;
$$;
select
  v.multiply_numbers(3, 4);
 multiply_numbers 
------------------
               12
(1 row)

drop schema v cascade;
NOTICE:  drop cascades to function v.multiply_numbers(integer,integer)

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/z_15_timescale.out ---
/*
This test is excluded from the Postgres 17 suite because it does not ship
with the Supabase PG17 image
*/
create extension if not exists timescaledb;
NOTICE:  extension "timescaledb" already exists, skipping
-- Confirm we're running the apache version
show timescaledb.license;
 timescaledb.license 
---------------------
 apache
(1 row)

-- Create schema v
create schema v;
-- Create a table in the v schema
create table v.sensor_data (
  time timestamptz not null,
  sensor_id int not null,
  temperature double precision not null,
  humidity double precision not null
);
-- Convert the table to a hypertable
select create_hypertable('v.sensor_data', 'time');
  create_hypertable  
---------------------
 (1,v,sensor_data,t)
(1 row)

-- Insert some data into the hypertable
insert into v.sensor_data (time, sensor_id, temperature, humidity)
values 
  ('2024-08-09', 1, 22.5, 60.2),
  ('2024-08-08', 1, 23.0, 59.1),
  ('2024-08-07', 2, 21.7, 63.3);
-- Select data from the hypertable
select
  *
from
  v.sensor_data;
             time             | sensor_id | temperature | humidity 
------------------------------+-----------+-------------+----------
 Fri Aug 09 00:00:00 2024 PDT |         1 |        22.5 |     60.2
 Thu Aug 08 00:00:00 2024 PDT |         1 |          23 |     59.1
 Wed Aug 07 00:00:00 2024 PDT |         2 |        21.7 |     63.3
(3 rows)

-- Drop schema v and all its entities
drop schema v cascade;
NOTICE:  drop cascades to 3 other objects
DETAIL:  drop cascades to table v.sensor_data
drop cascades to table _timescaledb_internal._hyper_1_1_chunk
drop cascades to table _timescaledb_internal._hyper_1_2_chunk

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/index_advisor.out ---
create schema v;
create table v.book(
  id int primary key,
  title text not null
);
select
  index_statements, errors
from
  index_advisor('select id from v.book where title = $1');
                index_statements                | errors 
------------------------------------------------+--------
 {"CREATE INDEX ON v.book USING btree (title)"} | {}
(1 row)

drop schema v cascade;
NOTICE:  drop cascades to table v.book

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/z_15_pgvector.out ---
create schema v;
create table v.items(
  id serial primary key,
  embedding vector(3),
  half_embedding halfvec(3),
  bit_embedding bit(3),
  sparse_embedding sparsevec(3)
);
-- vector ops
create index on v.items using hnsw (embedding vector_l2_ops);
create index on v.items using hnsw (embedding vector_cosine_ops);
create index on v.items using hnsw (embedding vector_l1_ops);
create index on v.items using ivfflat (embedding vector_l2_ops);
NOTICE:  ivfflat index created with little data
DETAIL:  This will cause low recall.
HINT:  Drop the index until the table has more data.
create index on v.items using ivfflat (embedding vector_cosine_ops);
NOTICE:  ivfflat index created with little data
DETAIL:  This will cause low recall.
HINT:  Drop the index until the table has more data.
-- halfvec ops
create index on v.items using hnsw (half_embedding halfvec_l2_ops);
create index on v.items using hnsw (half_embedding halfvec_cosine_ops);
create index on v.items using hnsw (half_embedding halfvec_l1_ops);
create index on v.items using ivfflat (half_embedding halfvec_l2_ops);
NOTICE:  ivfflat index created with little data
DETAIL:  This will cause low recall.
HINT:  Drop the index until the table has more data.
create index on v.items using ivfflat (half_embedding halfvec_cosine_ops);
NOTICE:  ivfflat index created with little data
DETAIL:  This will cause low recall.
HINT:  Drop the index until the table has more data.
-- sparsevec
create index on v.items using hnsw (sparse_embedding sparsevec_l2_ops);
create index on v.items using hnsw (sparse_embedding sparsevec_cosine_ops);
create index on v.items using hnsw (sparse_embedding sparsevec_l1_ops);
-- bit ops
create index on v.items using hnsw (bit_embedding bit_hamming_ops);
create index on v.items using ivfflat (bit_embedding bit_hamming_ops);
NOTICE:  ivfflat index created with little data
DETAIL:  This will cause low recall.
HINT:  Drop the index until the table has more data.
-- Populate some records
insert into v.items(
    embedding,
    half_embedding,
    bit_embedding,
    sparse_embedding
)
values
  ('[1,2,3]', '[1,2,3]', '101', '{1:4}/3'),
  ('[2,3,4]', '[2,3,4]', '010', '{1:7,3:0}/3');
-- Test op types
select
  *
from
  v.items
order by
  embedding <-> '[2,3,5]',
  embedding <=> '[2,3,5]',
  embedding <+> '[2,3,5]',
  embedding <#> '[2,3,5]',
  half_embedding <-> '[2,3,5]',
  half_embedding <=> '[2,3,5]',
  half_embedding <+> '[2,3,5]',
  half_embedding <#> '[2,3,5]',
  sparse_embedding <-> '{2:4,3:1}/3',
  sparse_embedding <=> '{2:4,3:1}/3',
  sparse_embedding <+> '{2:4,3:1}/3',
  sparse_embedding <#> '{2:4,3:1}/3',
  bit_embedding <~> '011';
 id | embedding | half_embedding | bit_embedding | sparse_embedding 
----+-----------+----------------+---------------+------------------
  2 | [2,3,4]   | [2,3,4]        | 010           | {1:7}/3
  1 | [1,2,3]   | [1,2,3]        | 101           | {1:4}/3
(2 rows)

select
  avg(embedding),
  avg(half_embedding)
from
  v.items;
      avg      |      avg      
---------------+---------------
 [1.5,2.5,3.5] | [1.5,2.5,3.5]
(1 row)

-- Cleanup
drop schema v cascade;
NOTICE:  drop cascades to table v.items

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pgmq.out ---
-- Test the standard flow
select
  pgmq.create('Foo');
 create 
--------
 
(1 row)

select
  *
from
  pgmq.send(
    queue_name:='Foo',
    msg:='{"foo": "bar1"}'
  );
 send 
------
    1
(1 row)

-- Test queue is not case sensitive
select
  *
from
  pgmq.send(
    queue_name:='foo', -- note: lowercase useage
    msg:='{"foo": "bar2"}',
    delay:=5
  );
 send 
------
    2
(1 row)

select
  msg_id,
  read_ct,
  message
from
  pgmq.read(
    queue_name:='Foo',
    vt:=30,
    qty:=2
  );
 msg_id | read_ct |     message     
--------+---------+-----------------
      1 |       1 | {"foo": "bar1"}
(1 row)

select
  msg_id,
  read_ct,
  message
from 
  pgmq.pop('Foo');
 msg_id | read_ct | message 
--------+---------+---------
(0 rows)

-- Archive message with msg_id=2.
select
  pgmq.archive(
    queue_name:='Foo',
    msg_id:=2
  );
 archive 
---------
 t
(1 row)

select
  pgmq.create('my_queue');
 create 
--------
 
(1 row)

select
  pgmq.send_batch(
  queue_name:='my_queue',
  msgs:=array['{"foo": "bar3"}','{"foo": "bar4"}','{"foo": "bar5"}']::jsonb[]
);
 send_batch 
------------
          1
          2
          3
(3 rows)

select
  pgmq.archive(
    queue_name:='my_queue',
    msg_ids:=array[3, 4, 5]
  );
 archive 
---------
       3
(1 row)

select
  pgmq.delete('my_queue', 6);
 delete 
--------
 f
(1 row)

select
  pgmq.drop_queue('my_queue');
 drop_queue 
------------
 t
(1 row)

/*
-- Disabled until pg_partman goes back into the image
select
  pgmq.create_partitioned(
    'my_partitioned_queue',
    '5 seconds',
    '10 seconds'
);
*/
-- Make sure SQLI enabling characters are blocked
select pgmq.create('F--oo');
ERROR:  queue name contains invalid characters: $, ;, --, or \'
CONTEXT:  PL/pgSQL function pgmq.format_table_name(text,text) line 5 at RAISE
PL/pgSQL function pgmq.create_non_partitioned(text) line 3 during statement block local variable initialization
SQL statement "SELECT pgmq.create_non_partitioned(queue_name)"
PL/pgSQL function pgmq."create"(text) line 3 at PERFORM
select pgmq.create('F$oo');
ERROR:  queue name contains invalid characters: $, ;, --, or \'
CONTEXT:  PL/pgSQL function pgmq.format_table_name(text,text) line 5 at RAISE
PL/pgSQL function pgmq.create_non_partitioned(text) line 3 during statement block local variable initialization
SQL statement "SELECT pgmq.create_non_partitioned(queue_name)"
PL/pgSQL function pgmq."create"(text) line 3 at PERFORM
select pgmq.create($$F'oo$$);
ERROR:  queue name contains invalid characters: $, ;, --, or \'
CONTEXT:  PL/pgSQL function pgmq.format_table_name(text,text) line 5 at RAISE
PL/pgSQL function pgmq.create_non_partitioned(text) line 3 during statement block local variable initialization
SQL statement "SELECT pgmq.create_non_partitioned(queue_name)"
PL/pgSQL function pgmq."create"(text) line 3 at PERFORM

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/expected/pg_tle.out ---
set client_min_messages = warning;
select
  pgtle.install_extension(
    'pg_distance',
    '0.1',
    'Distance functions for two points',
    $_pg_tle_$
      CREATE FUNCTION dist(x1 float8, y1 float8, x2 float8, y2 float8, norm int)
      RETURNS float8
      AS $$
        SELECT (abs(x2 - x1) ^ norm + abs(y2 - y1) ^ norm) ^ (1::float8 / norm);
      $$ LANGUAGE SQL;

      CREATE FUNCTION manhattan_dist(x1 float8, y1 float8, x2 float8, y2 float8)
      RETURNS float8
      AS $$
        SELECT dist(x1, y1, x2, y2, 1);
      $$ LANGUAGE SQL;

      CREATE FUNCTION euclidean_dist(x1 float8, y1 float8, x2 float8, y2 float8)
      RETURNS float8
      AS $$
        SELECT dist(x1, y1, x2, y2, 2);
      $$ LANGUAGE SQL;
    $_pg_tle_$
  );
 install_extension 
-------------------
 t
(1 row)

create extension pg_distance;
select manhattan_dist(1, 1, 5, 5)::numeric(10,2);
 manhattan_dist 
----------------
           8.00
(1 row)

select euclidean_dist(1, 1, 5, 5)::numeric(10,2);
 euclidean_dist 
----------------
           5.66
(1 row)

SELECT pgtle.install_update_path(
  'pg_distance',
  '0.1',
  '0.2',
  $_pg_tle_$
    CREATE OR REPLACE FUNCTION dist(x1 float8, y1 float8, x2 float8, y2 float8, norm int)
    RETURNS float8
    AS $$
      SELECT (abs(x2 - x1) ^ norm + abs(y2 - y1) ^ norm) ^ (1::float8 / norm);
    $$ LANGUAGE SQL IMMUTABLE PARALLEL SAFE;

    CREATE OR REPLACE FUNCTION manhattan_dist(x1 float8, y1 float8, x2 float8, y2 float8)
    RETURNS float8
    AS $$
      SELECT dist(x1, y1, x2, y2, 1);
    $$ LANGUAGE SQL IMMUTABLE PARALLEL SAFE;

    CREATE OR REPLACE FUNCTION euclidean_dist(x1 float8, y1 float8, x2 float8, y2 float8)
    RETURNS float8
    AS $$
      SELECT dist(x1, y1, x2, y2, 2);
    $$ LANGUAGE SQL IMMUTABLE PARALLEL SAFE;
  $_pg_tle_$
  );
 install_update_path 
---------------------
 t
(1 row)

select
  pgtle.set_default_version('pg_distance', '0.2');
 set_default_version 
---------------------
 t
(1 row)

alter extension pg_distance update;
drop extension pg_distance;
select
  pgtle.uninstall_extension('pg_distance');
 uninstall_extension 
---------------------
 t
(1 row)

-- Restore original state if any of the above fails
drop extension pg_tle cascade;
create extension pg_tle;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pg_jsonschema.sql ---
begin;

-- Test json_matches_schema
create table customer(
    id serial primary key,
    metadata json,

    check (
        json_matches_schema(
            '{
                "type": "object",
                "properties": {
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "maxLength": 16
                        }
                    }
                }
            }',
            metadata
        )
    )
);

insert into customer(metadata)
values ('{"tags": ["vip", "darkmode-ui"]}');

-- Test jsonb_matches_schema
select
  jsonb_matches_schema(
  '{
    "type": "object",
    "properties": {
	  "tags": {
        "type": "array",
        "items": {
          "type": "string",
          "maxLength": 16
        }
      }
      }
  }',
  '{"tags": ["vip", "darkmode-ui"]}'::jsonb
);

-- Test jsonschema_is_valid
select
  jsonschema_is_valid(
  '{
    "type": "object",
    "properties": {
	  "tags": {
        "type": "array",
        "items": {
          "type": "string",
          "maxLength": 16
        }
      }
    }
  }');

-- Test invalid payload
insert into customer(metadata)
values ('{"tags": [1, 3]}');

rollback;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/z_17_ext_interface.sql ---
/*

The purpose of this test is to monitor the SQL interface exposed
by Postgres extensions so we have to manually review/approve any difference
that emerge as versions change.

*/


/*

List all extensions that are not enabled
If a new entry shows up in this list, that means a new extension has been
added and you should `create extension ...` to enable it in ./nix/tests/prime

*/

select
  name
from 
  pg_available_extensions
where
  installed_version is null
order by
  name asc;


/*

Monitor relocatability and config of each extension
- lesson learned from pg_cron

*/

select
  extname as extension_name,
  extrelocatable as is_relocatable
from
  pg_extension
order by
  extname asc;


/*

Monitor extension public function interface

*/

select
  e.extname as extension_name,
  n.nspname as schema_name,
  p.proname as function_name,
  pg_catalog.pg_get_function_identity_arguments(p.oid) as argument_types,
  pg_catalog.pg_get_function_result(p.oid) as return_type
from
  pg_catalog.pg_proc p
  join pg_catalog.pg_namespace n
    on n.oid = p.pronamespace
  join pg_catalog.pg_depend d
    on d.objid = p.oid
  join pg_catalog.pg_extension e
    on e.oid = d.refobjid
where
  d.deptype = 'e'
  -- Filter out changes between pg15 and pg16 from extensions that ship with postgres
  -- new in pg16
  and not (e.extname = 'fuzzystrmatch' and p.proname = 'daitch_mokotoff')
  and not (e.extname = 'pageinspect' and p.proname = 'bt_multi_page_stats')
  and not (e.extname = 'pg_buffercache' and p.proname = 'pg_buffercache_summary')
  and not (e.extname = 'pg_buffercache' and p.proname = 'pg_buffercache_usage_counts')
  and not (e.extname = 'pg_walinspect' and p.proname = 'pg_get_wal_block_info')
  -- removed in pg16
  and not (e.extname = 'pg_walinspect' and p.proname = 'pg_get_wal_records_info_till_end_of_wal')
  and not (e.extname = 'pg_walinspect' and p.proname = 'pg_get_wal_stats_till_end_of_wal')
  -- changed in pg16 - output signature added a column
  and not (e.extname = 'pageinspect' and p.proname = 'brin_page_items')
order by
  e.extname,
  n.nspname,
  p.proname,
  pg_catalog.pg_get_function_identity_arguments(p.oid);

/*

Monitor extension public table/view/matview/index interface

*/

select
  e.extname as extension_name,
  n.nspname as schema_name,
  pc.relname as entity_name,
  pa.attname
from
  pg_catalog.pg_class pc
  join pg_catalog.pg_namespace n
    on n.oid = pc.relnamespace
  join pg_catalog.pg_depend d
    on d.objid = pc.oid
  join pg_catalog.pg_extension e
    on e.oid = d.refobjid
  left join pg_catalog.pg_attribute pa
    on pa.attrelid = pc.oid
    and pa.attnum > 0
    and not pa.attisdropped
where
  d.deptype = 'e'
  and pc.relkind in ('r', 'v', 'm', 'i')
order by
  e.extname,
  n.nspname,
  pc.relname,
  pa.attname;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pgrouting.sql ---
create schema v;

-- create the roads table
create table v.roads (
  id serial primary key,
  source integer,
  target integer,
  cost double precision
);

-- insert sample data into roads table
insert into v.roads (source, target, cost) values
(1, 2, 1.0),
(2, 3, 1.0),
(3, 4, 1.0),
(1, 3, 2.5),
(3, 5, 2.0);

-- create a function to use pgRouting to find the shortest path
select * from pgr_dijkstra(
  'select id, source, target, cost from v.roads',
  1, -- start node
  4  -- end node
);

drop schema v cascade;


'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/z_15_plv8.sql ---
/*
This test is excluded from the Postgres 17 suite because it does not ship
with the Supabase PG17 image
*/
create extension if not exists plv8;

create schema v;

-- create a function to perform some JavaScript operations
create function v.multiply_numbers(a integer, b integer)
  returns integer
  language plv8
as $$
  return a * b;
$$;

select
  v.multiply_numbers(3, 4);

drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/z_15_timescale.sql ---
/*
This test is excluded from the Postgres 17 suite because it does not ship
with the Supabase PG17 image
*/
create extension if not exists timescaledb;

-- Confirm we're running the apache version
show timescaledb.license;

-- Create schema v
create schema v;

-- Create a table in the v schema
create table v.sensor_data (
  time timestamptz not null,
  sensor_id int not null,
  temperature double precision not null,
  humidity double precision not null
);

-- Convert the table to a hypertable
select create_hypertable('v.sensor_data', 'time');

-- Insert some data into the hypertable
insert into v.sensor_data (time, sensor_id, temperature, humidity)
values 
  ('2024-08-09', 1, 22.5, 60.2),
  ('2024-08-08', 1, 23.0, 59.1),
  ('2024-08-07', 2, 21.7, 63.3);

-- Select data from the hypertable
select
  *
from
  v.sensor_data;

-- Drop schema v and all its entities
drop schema v cascade;


'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/index_advisor.sql ---
create schema v;

create table v.book(
  id int primary key,
  title text not null
);

select
  index_statements, errors
from
  index_advisor('select id from v.book where title = $1');

drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/z_15_pgvector.sql ---
create schema v;

create table v.items(
  id serial primary key,
  embedding vector(3),
  half_embedding halfvec(3),
  bit_embedding bit(3),
  sparse_embedding sparsevec(3)
);

-- vector ops
create index on v.items using hnsw (embedding vector_l2_ops);
create index on v.items using hnsw (embedding vector_cosine_ops);
create index on v.items using hnsw (embedding vector_l1_ops);
create index on v.items using ivfflat (embedding vector_l2_ops);
create index on v.items using ivfflat (embedding vector_cosine_ops);

-- halfvec ops
create index on v.items using hnsw (half_embedding halfvec_l2_ops);
create index on v.items using hnsw (half_embedding halfvec_cosine_ops);
create index on v.items using hnsw (half_embedding halfvec_l1_ops);
create index on v.items using ivfflat (half_embedding halfvec_l2_ops);
create index on v.items using ivfflat (half_embedding halfvec_cosine_ops);

-- sparsevec
create index on v.items using hnsw (sparse_embedding sparsevec_l2_ops);
create index on v.items using hnsw (sparse_embedding sparsevec_cosine_ops);
create index on v.items using hnsw (sparse_embedding sparsevec_l1_ops);

-- bit ops
create index on v.items using hnsw (bit_embedding bit_hamming_ops);
create index on v.items using ivfflat (bit_embedding bit_hamming_ops);

-- Populate some records
insert into v.items(
    embedding,
    half_embedding,
    bit_embedding,
    sparse_embedding
)
values
  ('[1,2,3]', '[1,2,3]', '101', '{1:4}/3'),
  ('[2,3,4]', '[2,3,4]', '010', '{1:7,3:0}/3');

-- Test op types
select
  *
from
  v.items
order by
  embedding <-> '[2,3,5]',
  embedding <=> '[2,3,5]',
  embedding <+> '[2,3,5]',
  embedding <#> '[2,3,5]',
  half_embedding <-> '[2,3,5]',
  half_embedding <=> '[2,3,5]',
  half_embedding <+> '[2,3,5]',
  half_embedding <#> '[2,3,5]',
  sparse_embedding <-> '{2:4,3:1}/3',
  sparse_embedding <=> '{2:4,3:1}/3',
  sparse_embedding <+> '{2:4,3:1}/3',
  sparse_embedding <#> '{2:4,3:1}/3',
  bit_embedding <~> '011';

select
  avg(embedding),
  avg(half_embedding)
from
  v.items;

-- Cleanup
drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pgmq.sql ---
-- Test the standard flow
select
  pgmq.create('Foo');

select
  *
from
  pgmq.send(
    queue_name:='Foo',
    msg:='{"foo": "bar1"}'
  );

-- Test queue is not case sensitive
select
  *
from
  pgmq.send(
    queue_name:='foo', -- note: lowercase useage
    msg:='{"foo": "bar2"}',
    delay:=5
  );

select
  msg_id,
  read_ct,
  message
from
  pgmq.read(
    queue_name:='Foo',
    vt:=30,
    qty:=2
  );

select
  msg_id,
  read_ct,
  message
from 
  pgmq.pop('Foo');


-- Archive message with msg_id=2.
select
  pgmq.archive(
    queue_name:='Foo',
    msg_id:=2
  );


select
  pgmq.create('my_queue');

select
  pgmq.send_batch(
  queue_name:='my_queue',
  msgs:=array['{"foo": "bar3"}','{"foo": "bar4"}','{"foo": "bar5"}']::jsonb[]
);

select
  pgmq.archive(
    queue_name:='my_queue',
    msg_ids:=array[3, 4, 5]
  );

select
  pgmq.delete('my_queue', 6);


select
  pgmq.drop_queue('my_queue');

/*
-- Disabled until pg_partman goes back into the image
select
  pgmq.create_partitioned(
    'my_partitioned_queue',
    '5 seconds',
    '10 seconds'
);
*/


-- Make sure SQLI enabling characters are blocked
select pgmq.create('F--oo');
select pgmq.create('F$oo');
select pgmq.create($$F'oo$$);





'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pg_tle.sql ---
set client_min_messages = warning;

select
  pgtle.install_extension(
    'pg_distance',
    '0.1',
    'Distance functions for two points',
    $_pg_tle_$
      CREATE FUNCTION dist(x1 float8, y1 float8, x2 float8, y2 float8, norm int)
      RETURNS float8
      AS $$
        SELECT (abs(x2 - x1) ^ norm + abs(y2 - y1) ^ norm) ^ (1::float8 / norm);
      $$ LANGUAGE SQL;

      CREATE FUNCTION manhattan_dist(x1 float8, y1 float8, x2 float8, y2 float8)
      RETURNS float8
      AS $$
        SELECT dist(x1, y1, x2, y2, 1);
      $$ LANGUAGE SQL;

      CREATE FUNCTION euclidean_dist(x1 float8, y1 float8, x2 float8, y2 float8)
      RETURNS float8
      AS $$
        SELECT dist(x1, y1, x2, y2, 2);
      $$ LANGUAGE SQL;
    $_pg_tle_$
  );

create extension pg_distance;

select manhattan_dist(1, 1, 5, 5)::numeric(10,2);
select euclidean_dist(1, 1, 5, 5)::numeric(10,2);

SELECT pgtle.install_update_path(
  'pg_distance',
  '0.1',
  '0.2',
  $_pg_tle_$
    CREATE OR REPLACE FUNCTION dist(x1 float8, y1 float8, x2 float8, y2 float8, norm int)
    RETURNS float8
    AS $$
      SELECT (abs(x2 - x1) ^ norm + abs(y2 - y1) ^ norm) ^ (1::float8 / norm);
    $$ LANGUAGE SQL IMMUTABLE PARALLEL SAFE;

    CREATE OR REPLACE FUNCTION manhattan_dist(x1 float8, y1 float8, x2 float8, y2 float8)
    RETURNS float8
    AS $$
      SELECT dist(x1, y1, x2, y2, 1);
    $$ LANGUAGE SQL IMMUTABLE PARALLEL SAFE;

    CREATE OR REPLACE FUNCTION euclidean_dist(x1 float8, y1 float8, x2 float8, y2 float8)
    RETURNS float8
    AS $$
      SELECT dist(x1, y1, x2, y2, 2);
    $$ LANGUAGE SQL IMMUTABLE PARALLEL SAFE;
  $_pg_tle_$
  );


select
  pgtle.set_default_version('pg_distance', '0.2');

alter extension pg_distance update;

drop extension pg_distance;

select
  pgtle.uninstall_extension('pg_distance');

-- Restore original state if any of the above fails
drop extension pg_tle cascade;

create extension pg_tle;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/z_15_ext_interface.sql ---
/*

The purpose of this test is to monitor the SQL interface exposed
by Postgres extensions so we have to manually review/approve any difference
that emerge as versions change.

*/


/*

List all extensions that are not enabled
If a new entry shows up in this list, that means a new extension has been
added and you should `create extension ...` to enable it in ./nix/tests/prime

*/
create extension if not exists adminpack;
create extension if not exists plv8;
create extension if not exists plcoffee;
create extension if not exists plls;
create extension if not exists old_snapshot;
create extension if not exists timescaledb;
create extension if not exists postgis_tiger_geocoder;


select
  name
from 
  pg_available_extensions
where
  installed_version is null
order by
  name asc;


/*

Monitor relocatability and config of each extension
- lesson learned from pg_cron

*/

select
  extname as extension_name,
  extrelocatable as is_relocatable
from
  pg_extension
order by
  extname asc;


/*

Monitor extension public function interface

*/

select
  e.extname as extension_name,
  n.nspname as schema_name,
  p.proname as function_name,
  pg_catalog.pg_get_function_identity_arguments(p.oid) as argument_types,
  pg_catalog.pg_get_function_result(p.oid) as return_type
from
  pg_catalog.pg_proc p
  join pg_catalog.pg_namespace n
    on n.oid = p.pronamespace
  join pg_catalog.pg_depend d
    on d.objid = p.oid
  join pg_catalog.pg_extension e
    on e.oid = d.refobjid
where
  d.deptype = 'e'
  -- Filter out changes between pg15 and pg16 from extensions that ship with postgres
  -- new in pg16
  and not (e.extname = 'fuzzystrmatch' and p.proname = 'daitch_mokotoff')
  and not (e.extname = 'pageinspect' and p.proname = 'bt_multi_page_stats')
  and not (e.extname = 'pg_buffercache' and p.proname = 'pg_buffercache_summary')
  and not (e.extname = 'pg_buffercache' and p.proname = 'pg_buffercache_usage_counts')
  and not (e.extname = 'pg_walinspect' and p.proname = 'pg_get_wal_block_info')
  -- removed in pg16
  and not (e.extname = 'pg_walinspect' and p.proname = 'pg_get_wal_records_info_till_end_of_wal')
  and not (e.extname = 'pg_walinspect' and p.proname = 'pg_get_wal_stats_till_end_of_wal')
  -- changed in pg16 - output signature added a column
  and not (e.extname = 'pageinspect' and p.proname = 'brin_page_items')
order by
  e.extname,
  n.nspname,
  p.proname,
  md5(pg_catalog.pg_get_function_identity_arguments(p.oid));

/*

Monitor extension public table/view/matview/index interface

*/

select
  e.extname as extension_name,
  n.nspname as schema_name,
  pc.relname as entity_name,
  pa.attname
from
  pg_catalog.pg_class pc
  join pg_catalog.pg_namespace n
    on n.oid = pc.relnamespace
  join pg_catalog.pg_depend d
    on d.objid = pc.oid
  join pg_catalog.pg_extension e
    on e.oid = d.refobjid
  left join pg_catalog.pg_attribute pa
    on pa.attrelid = pc.oid
    and pa.attnum > 0
    and not pa.attisdropped
where
  d.deptype = 'e'
  and pc.relkind in ('r', 'v', 'm', 'i')
order by
  e.extname,
  n.nspname,
  pc.relname,
  pa.attname;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/z_15_pgroonga.sql ---
create schema v;

create table v.roon(
  id serial primary key,
  content text
);


with tokenizers as (
  select
    x
  from
    jsonb_array_elements(
      (select pgroonga_command('tokenizer_list'))::jsonb
    ) x(val)
  limit
    1
  offset
    1 -- first record is unrelated and not stable
)
select
  t.x::jsonb ->> 'name'
from
  jsonb_array_elements((select * from tokenizers)) t(x)
order by
  t.x::jsonb ->> 'name';


insert into v.roon (content)
values
  ('Hello World'),
  ('PostgreSQL with PGroonga is a thing'),
  ('This is a full-text search test'),
  ('PGroonga supports various languages');

-- Create default index
create index pgroonga_index on v.roon using pgroonga (content);

-- Create mecab tokenizer index since we had a bug with this one once
create index pgroonga_index_mecab on v.roon using pgroonga (content) with (tokenizer='TokenMecab');

-- Run some queries to test the index
select * from v.roon where content &@~ 'Hello';
select * from v.roon where content &@~ 'powerful';
select * from v.roon where content &@~ 'supports';


drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/wal2json.sql ---
create schema v;

create table v.foo(
  id int primary key
);

select
  1
from
  pg_create_logical_replication_slot('reg_test', 'wal2json', false);

insert into v.foo(id) values (1);

select
  data
from
  pg_logical_slot_get_changes(
	'reg_test',
    null,
    null,
	'include-pk', '1',
	'include-transaction', 'false',
	'include-timestamp', 'false',
	'include-type-oids', 'false',
	'format-version', '2',
	'actions', 'insert,update,delete'
  ) x;

select
  pg_drop_replication_slot('reg_test');

drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pg-safeupdate.sql ---
load 'safeupdate';

set safeupdate.enabled=1;

create schema v;

create table v.foo(
  id int,
  val text
);

update v.foo
  set val = 'bar';

drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/hypopg.sql ---
create schema v;

create table v.samp(
  id int
);

select 1 from hypopg_create_index($$
  create index on v.samp(id)
$$);

drop schema v cascade;



'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pgsodium.sql ---
select
  status
from
  pgsodium.create_key();

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pgtap.sql ---
begin;

select plan(1);

-- Run the tests.
select pass( 'My test passed, w00t!' );

-- Finish the tests and clean up.
select * from finish();

rollback;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pgaudit.sql ---
-- Note: there is no test that the logs were correctly output. Only checking for exceptions
set pgaudit.log = 'write, ddl';
set pgaudit.log_relation = on;
set pgaudit.log_level = notice;

create schema v;

create table v.account(
  id int,
  name text,
  password text,
  description text
);

insert into v.account (id, name, password, description)
values (1, 'user1', 'HASH1', 'blah, blah');

select
  *
from
  v.account;

drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pg_graphql.sql ---
begin;
    comment on schema public is '@graphql({"inflect_names": true})';

    create table account(
        id serial primary key,
        email varchar(255) not null,
        priority int,
        status text default 'active'
    );

    create table blog(
        id serial primary key,
        owner_id integer not null references account(id)
    );
    comment on table blog is e'@graphql({"totalCount": {"enabled": true}})';

    -- Make sure functions still work
    create function _echo_email(account)
        returns text
        language sql
    as $$ select $1.email $$;

    /*
        Literals
    */

    select graphql.resolve($$
    mutation {
      insertIntoAccountCollection(objects: [
        { email: "foo@barsley.com", priority: 1 },
        { email: "bar@foosworth.com" }
      ]) {
        affectedCount
        records {
          id
          status
          echoEmail
          blogCollection {
            totalCount
          }
        }
      }
    }
    $$);

    select graphql.resolve($$
    mutation {
      insertIntoBlogCollection(objects: [{
        ownerId: 1
      }]) {
        records {
          id
          owner {
            id
          }
        }
      }
    }
    $$);


    -- Override a default on status with null
    select graphql.resolve($$
    mutation {
      insertIntoAccountCollection(objects: [
        { email: "baz@baz.com", status: null },
      ]) {
        affectedCount
        records {
          email
          status
        }
      }
    }
    $$);


    /*
        Variables
    */

    select graphql.resolve($$
    mutation newAccount($emailAddress: String) {
       xyz: insertIntoAccountCollection(objects: [
        { email: $emailAddress },
        { email: "other@email.com" }
       ]) {
        affectedCount
        records {
          id
          email
        }
      }
    }
    $$,
    variables := '{"emailAddress": "foo@bar.com"}'::jsonb
    );


    -- Variable override of default with null results in null
    select graphql.resolve($$
    mutation newAccount($status: String) {
       xyz: insertIntoAccountCollection(objects: [
        { email: "1@email.com", status: $status}
       ]) {
        affectedCount
        records {
          email
          status
        }
      }
    }
    $$,
    variables := '{"status": null}'::jsonb
    );

    -- Skipping variable override of default results in default
    select graphql.resolve($$
    mutation newAccount($status: String) {
       xyz: insertIntoAccountCollection(objects: [
        { email: "x@y.com", status: $status},
       ]) {
        affectedCount
        records {
          email
          status
        }
      }
    }
    $$,
    variables := '{}'::jsonb
    );


    select graphql.resolve($$
    mutation newAccount($acc: AccountInsertInput!) {
       insertIntoAccountCollection(objects: [$acc]) {
        affectedCount
        records {
          id
          email
        }
      }
    }
    $$,
    variables := '{"acc": {"email": "bar@foo.com"}}'::jsonb
    );

    select graphql.resolve($$
    mutation newAccounts($acc: [AccountInsertInput!]!) {
       insertIntoAccountCollection(objects: $accs) {
        affectedCount
        records {
          id
          email
        }
      }
    }
    $$,
    variables := '{"accs": [{"email": "bar@foo.com"}]}'::jsonb
    );

    -- Single object coerces to a list
    select graphql.resolve($$
    mutation {
      insertIntoBlogCollection(objects: {ownerId: 1}) {
        affectedCount
      }
    }
    $$);


    /*
        Errors
    */

    -- Field does not exist
    select graphql.resolve($$
    mutation createAccount($acc: AccountInsertInput) {
       insertIntoAccountCollection(objects: [$acc]) {
        affectedCount
        records {
          id
          email
        }
      }
    }
    $$,
    variables := '{"acc": {"doesNotExist": "other"}}'::jsonb
    );

    -- Wrong input type (list of string, not list of object)
    select graphql.resolve($$
    mutation {
      insertIntoBlogCollection(objects: ["not an object"]) {
        affectedCount
      }
    }
    $$);

    -- objects argument is missing
    select graphql.resolve($$
    mutation {
      insertIntoBlogCollection {
        affectedCount
      }
    }
    $$);

    -- Empty call
    select graphql.resolve($$
    mutation {
      insertIntoBlogCollection(objects: []) {
        affectedCount
      }
    }
    $$);

rollback;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pg_plan_filter.sql ---
begin;
  load 'plan_filter';

  create schema v;

  -- create a sample table
  create table v.test_table (
    id serial primary key,
    data text
  );

  -- insert some test data
  insert into v.test_table (data)
  values ('sample1'), ('sample2'), ('sample3');

  set local plan_filter.statement_cost_limit = 0.001;

  select * from v.test_table;

rollback;



'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pg_hashids.sql ---
select id_encode(1001); -- Result: jNl
select id_encode(1234567, 'This is my salt'); -- Result: Pdzxp
select id_encode(1234567, 'This is my salt', 10); -- Result: PlRPdzxpR7
select id_encode(1234567, 'This is my salt', 10, 'abcdefghijABCDxFGHIJ1234567890'); -- Result: 3GJ956J9B9
select id_decode('PlRPdzxpR7', 'This is my salt', 10); -- Result: 1234567
select id_decode('3GJ956J9B9', 'This is my salt', 10, 'abcdefghijABCDxFGHIJ1234567890'); -- Result: 1234567

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/postgis.sql ---
create schema v;

-- create a table to store geographic points
create table v.places (
    id serial primary key,
    name text,
    geom geometry(point, 4326)  -- using WGS 84 coordinate system
);

-- insert some sample geographic points into the places table
insert into v.places (name, geom)
values
  ('place_a', st_setsrid(st_makepoint(-73.9857, 40.7484), 4326)),  -- latitude and longitude for a location
  ('place_b', st_setsrid(st_makepoint(-74.0060, 40.7128), 4326)),  -- another location
  ('place_c', st_setsrid(st_makepoint(-73.9687, 40.7851), 4326));  -- yet another location

-- calculate the distance between two points (in meters)
select
  a.name as place_a,
  b.name as place_b,
  st_distance(a.geom::geography, b.geom::geography) as distance_meters
from
  v.places a,
  v.places b
where
  a.name = 'place_a'
  and b.name = 'place_b';

-- find all places within a 5km radius of 'place_a'
select
  name,
  st_distance(
    geom::geography,
    (
      select
        geom
      from
        v.places
      where
        name = 'place_a'
    )::geography) as distance_meters
from
  v.places
where
  st_dwithin(
    geom::geography,
    (select geom from v.places where name = 'place_a')::geography,
    5000
  )
  and name != 'place_a';

drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pgjwt.sql ---
select
  sign(
    payload   := '{"sub":"1234567890","name":"John Doe","iat":1516239022}',
    secret    := 'secret',
    algorithm := 'HS256'
  );

select
  verify(
    token := 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiRm9vIn0.Q8hKjuadCEhnCPuqIj9bfLhTh_9QSxshTRsA5Aq4IuM',
    secret    := 'secret',
    algorithm := 'HS256'
  );

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/plpgsql-check.sql ---
create schema v;

create table v.t1(
  a int,
  b int
);

create or replace function v.f1()
  returns void
  language plpgsql
as $$
declare r record;
begin
  for r in select * from v.t1
  loop
    raise notice '%', r.c; -- there is bug - table t1 missing "c" column
  end loop;
end;
$$;

select * from v.f1();

-- use plpgsql_check_function to check the function for errors
select * from plpgsql_check_function('v.f1()');

drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/z_15_rum.sql ---
/*
This extension is excluded from oriole-17 because it uses an unsupported index type
*/
create schema v;

create table v.test_rum(
  t text,
  a tsvector
);

create trigger tsvectorupdate
  before update or insert on v.test_rum
  for each row
  execute procedure
    tsvector_update_trigger(
      'a',
      'pg_catalog.english',
      't'
    );

insert into v.test_rum(t)
values
  ('the situation is most beautiful'),
  ('it is a beautiful'),
  ('it looks like a beautiful place');

create index rumidx on v.test_rum using rum (a rum_tsvector_ops);

select
  t,
  round(a <=> to_tsquery('english', 'beautiful | place')) as rank
from
  v.test_rum
where
  a @@ to_tsquery('english', 'beautiful | place')
order by
  a <=> to_tsquery('english', 'beautiful | place');


drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/vault.sql ---
select
  1
from
  vault.create_secret('my_s3kre3t');

select
  1
from
  vault.create_secret(
    'another_s3kre3t',
    'unique_name',
    'This is the description'
  );

insert into vault.secrets (secret)
values
  ('s3kre3t_k3y');

select
  name,
  description
from
  vault.decrypted_secrets 
order by
  created_at desc 
limit
  3;
 



'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/z_17_pgvector.sql ---
/*
This test excludes indexes shipped with pgvector because orioledb doesn't support them yet
*/
create schema v;

create table v.items(
  id serial primary key,
  embedding vector(3),
  half_embedding halfvec(3),
  bit_embedding bit(3),
  sparse_embedding sparsevec(3)
);

-- Populate some records
insert into v.items(
    embedding,
    half_embedding,
    bit_embedding,
    sparse_embedding
)
values
  ('[1,2,3]', '[1,2,3]', '101', '{1:4}/3'),
  ('[2,3,4]', '[2,3,4]', '010', '{1:7,3:0}/3');

-- Test op types
select
  *
from
  v.items
order by
  embedding <-> '[2,3,5]',
  embedding <=> '[2,3,5]',
  embedding <+> '[2,3,5]',
  embedding <#> '[2,3,5]',
  half_embedding <-> '[2,3,5]',
  half_embedding <=> '[2,3,5]',
  half_embedding <+> '[2,3,5]',
  half_embedding <#> '[2,3,5]',
  sparse_embedding <-> '{2:4,3:1}/3',
  sparse_embedding <=> '{2:4,3:1}/3',
  sparse_embedding <+> '{2:4,3:1}/3',
  sparse_embedding <#> '{2:4,3:1}/3',
  bit_embedding <~> '011';

select
  avg(embedding),
  avg(half_embedding)
from
  v.items;

-- Cleanup
drop schema v cascade;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/z_15_pg_stat_monitor.sql ---
select
  *
from
  pg_stat_monitor
where
  false;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/z_17_pg_stat_monitor.sql ---
select
  *
from
  pg_stat_monitor
where
  false;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/nix/tests/sql/pg_net.sql ---
-- This is a very basic test because you can't get the value returned
-- by a pg_net request in the same transaction that created it;

select
  net.http_get (
    'https://postman-echo.com/get?foo1=bar1&foo2=bar2'
  ) as request_id;

'''

'''--- /Users/barneycook/Desktop/code/ProjectRef/postgres/flake.nix ---
{
  description = "Prototype tooling for deploying PostgreSQL";

  inputs = {
    nixpkgs.url = "github:nixos/nixpkgs/nixpkgs-unstable";
    flake-utils.url = "github:numtide/flake-utils";
    nix2container.url = "github:nlewo/nix2container";
    nix-editor.url = "github:snowfallorg/nix-editor";
    rust-overlay.url = "github:oxalica/rust-overlay";
  };

  outputs = { self, nixpkgs, flake-utils, nix-editor, rust-overlay, nix2container, ... }:
    let
      gitRev = "vcs=${self.shortRev or "dirty"}+${builtins.substring 0 8 (self.lastModifiedDate or self.lastModified or "19700101")}";

      ourSystems = with flake-utils.lib; [
        system.x86_64-linux
        system.aarch64-linux
        system.aarch64-darwin
      ];
    in
    flake-utils.lib.eachSystem ourSystems (system:
      let
        pgsqlDefaultPort = "5435";
        pgsqlDefaultHost = "localhost";
        pgsqlSuperuser = "supabase_admin";

        pkgs = import nixpkgs {
          config = {
            allowUnfree = true;
            permittedInsecurePackages = [
              "v8-9.7.106.18"
            ];
          };
          inherit system;
          overlays = [
            # NOTE: add any needed overlays here. in theory we could
            # pull them from the overlays/ directory automatically, but we don't
            # want to have an arbitrary order, since it might matter. being
            # explicit is better.
            (final: prev: {
              xmrig = throw "The xmrig package has been explicitly disabled in this flake.";
            })
            (import rust-overlay)
            (final: prev: {
              cargo-pgrx = final.callPackage ./nix/cargo-pgrx/default.nix {
                inherit (final) lib;
                inherit (final) darwin;
                inherit (final) fetchCrate;
                inherit (final) openssl;
                inherit (final) pkg-config;
                inherit (final) makeRustPlatform;
                inherit (final) stdenv;
                inherit (final) rust-bin;
              };

              buildPgrxExtension = final.callPackage ./nix/cargo-pgrx/buildPgrxExtension.nix {
                inherit (final) cargo-pgrx;
                inherit (final) lib;
                inherit (final) Security;
                inherit (final) pkg-config;
                inherit (final) makeRustPlatform;
                inherit (final) stdenv;
                inherit (final) writeShellScriptBin;
              };

              buildPgrxExtension_0_11_3 = prev.buildPgrxExtension.override {
                cargo-pgrx = final.cargo-pgrx.cargo-pgrx_0_11_3;
              };

              buildPgrxExtension_0_12_6 = prev.buildPgrxExtension.override {
                cargo-pgrx = final.cargo-pgrx.cargo-pgrx_0_12_6;
              };

              buildPgrxExtension_0_12_9 = prev.buildPgrxExtension.override {
                cargo-pgrx = final.cargo-pgrx.cargo-pgrx_0_12_9;
              };

              buildPgrxExtension_0_14_3 = prev.buildPgrxExtension.override {
                cargo-pgrx = final.cargo-pgrx.cargo-pgrx_0_14_3;
              };

            })
            (final: prev: {
              postgresql = final.callPackage ./nix/postgresql/default.nix {
                inherit (final) lib stdenv fetchurl makeWrapper callPackage buildEnv newScope;
              };
            })
          ];
        };
        # Define pythonEnv here
        pythonEnv = pkgs.python3.withPackages (ps: with ps; [
          boto3
          docker
          pytest
          pytest-testinfra
          requests
          ec2instanceconnectcli
          paramiko
        ]);

        # Add this new definition
        nixFastBuild = pkgs.nix-fast-build or null;

        sfcgal = pkgs.callPackage ./nix/ext/sfcgal/sfcgal.nix { };
        supabase-groonga = pkgs.callPackage ./nix/supabase-groonga.nix { };
        mecab-naist-jdic = pkgs.callPackage ./nix/ext/mecab-naist-jdic/default.nix { };
        inherit (pkgs.callPackage ./nix/wal-g.nix { }) wal-g-2 wal-g-3;
        # Our list of PostgreSQL extensions which come from upstream Nixpkgs.
        # These are maintained upstream and can easily be used here just by
        # listing their name. Anytime the version of nixpkgs is upgraded, these
        # may also bring in new versions of the extensions.
        psqlExtensions = [
          /* pljava */
          /*"postgis"*/
        ];

        #FIXME for now, timescaledb is not included in the orioledb version of supabase extensions, as there is an issue
        # with building timescaledb with the orioledb patched version of postgresql
        orioledbPsqlExtensions = [
          /* pljava */
          /*"timescaledb"*/
        ];

        # Custom extensions that exist in our repository. These aren't upstream
        # either because nobody has done the work, maintaining them here is
        # easier and more expedient, or because they may not be suitable, or are
        # too niche/one-off.
        #
        # Ideally, most of these should have copies upstream for third party
        # use, but even if they did, keeping our own copies means that we can
        # rollout new versions of these critical things easier without having to
        # go through the upstream release engineering process.
        ourExtensions = [
          ./nix/ext/rum.nix
          ./nix/ext/timescaledb.nix
          ./nix/ext/timescaledb-2.9.1.nix
          ./nix/ext/pgroonga.nix
          ./nix/ext/index_advisor.nix
          ./nix/ext/wal2json.nix
          ./nix/ext/pgmq.nix
          ./nix/ext/pg_repack.nix
          ./nix/ext/pg-safeupdate.nix
          ./nix/ext/plpgsql-check.nix
          ./nix/ext/pgjwt.nix
          ./nix/ext/pgaudit.nix
          ./nix/ext/postgis.nix
          ./nix/ext/pgrouting.nix
          ./nix/ext/pgtap.nix
          ./nix/ext/pg_cron.nix
          ./nix/ext/pgsql-http.nix
          ./nix/ext/pg_plan_filter.nix
          ./nix/ext/pg_net.nix
          ./nix/ext/pg_hashids.nix
          ./nix/ext/pgsodium.nix
          ./nix/ext/pg_graphql.nix
          ./nix/ext/pg_stat_monitor.nix
          ./nix/ext/pg_jsonschema.nix
          ./nix/ext/pgvector.nix
          ./nix/ext/vault.nix
          ./nix/ext/hypopg.nix
          ./nix/ext/pg_tle.nix
          ./nix/ext/wrappers/default.nix
          ./nix/ext/supautils.nix
          ./nix/ext/plv8.nix
          ./nix/ext/age.nix   
          ./nix/ext/pg_backtrace.nix 
        ];

        #Where we import and build the orioledb extension, we add on our custom extensions
        # plus the orioledb option
        #we're not using timescaledb or plv8 in the orioledb-17 version or pg 17 of supabase extensions
        orioleFilteredExtensions = builtins.filter
          (
            x:
            x != ./nix/ext/timescaledb.nix &&
            x != ./nix/ext/timescaledb-2.9.1.nix &&
            x != ./nix/ext/plv8.nix &&
            x != ./nix/ext/age.nix &&
            x != ./nix/ext/pgroonga.nix
        ) ourExtensions;

        orioledbExtensions = orioleFilteredExtensions ++ [ ./nix/ext/orioledb.nix ];
        dbExtensions17 = builtins.filter
        (x:
          x != ./nix/ext/timescaledb.nix &&
          x != ./nix/ext/timescaledb-2.9.1.nix &&
          x != ./nix/ext/plv8.nix &&
          x != ./nix/ext/age.nix &&      
          x != ./nix/ext/pgroonga.nix
        ) ourExtensions;

        getPostgresqlPackage = version:
          pkgs.postgresql."postgresql_${version}";
        # Create a 'receipt' file for a given postgresql package. This is a way
        # of adding a bit of metadata to the package, which can be used by other
        # tools to inspect what the contents of the install are: the PSQL
        # version, the installed extensions, et cetera.
        #
        # This takes two arguments:
        #  - pgbin: the postgresql package we are building on top of
        #    not a list of packages, but an attrset containing extension names
        #    mapped to versions.
        #  - ourExts: the list of extensions from upstream nixpkgs. This is not
        #    a list of packages, but an attrset containing extension names
        #    mapped to versions.
        #
        # The output is a package containing the receipt.json file, which can be
        # merged with the PostgreSQL installation using 'symlinkJoin'.
        makeReceipt = pgbin: ourExts: pkgs.writeTextFile {
          name = "receipt";
          destination = "/receipt.json";
          text = builtins.toJSON {
            revision = gitRev;
            psql-version = pgbin.version;
            nixpkgs = {
              revision = nixpkgs.rev;
            };
            extensions = ourExts;

            # NOTE this field can be used to do cache busting (e.g.
            # force a rebuild of the psql packages) but also to helpfully inform
            # tools what version of the schema is being used, for forwards and
            # backwards compatibility
            receipt-version = "1";
          };
        };

        makeOurPostgresPkgs = version:
          let
            postgresql = getPostgresqlPackage version;
            extensionsToUse =
              if version == "15" then ourExtensions  # Include all extensions for PG15
              else if (builtins.elem version [ "orioledb-17" ]) then orioledbExtensions
              else if (builtins.elem version [ "17" ]) then dbExtensions17
              else ourExtensions;  # Default fallback
          in
          map (path: pkgs.callPackage path { inherit postgresql; }) extensionsToUse;


        # Create an attrset that contains all the extensions included in a server.
        makeOurPostgresPkgsSet = version:
          (builtins.listToAttrs (map
            (drv:
              { name = drv.pname; value = drv; }
            )
            (makeOurPostgresPkgs version)))
          // { recurseForDerivations = true; };


        # Create a binary distribution of PostgreSQL, given a version.
        #
        # NOTE: The version here does NOT refer to the exact PostgreSQL version;
        # it refers to the *major number only*, which is used to select the
        # correct version of the package from nixpkgs. This is because we want
        # to be able to do so in an open ended way. As an example, the version
        # "15" passed in will use the nixpkgs package "postgresql_15" as the
        # basis for building extensions, etc.
        makePostgresBin = version:
          let
            postgresql = getPostgresqlPackage version;
            ourExts = map (ext: { name = ext.pname; version = ext.version; }) (makeOurPostgresPkgs version);

            pgbin = postgresql.withPackages (ps:
              makeOurPostgresPkgs version
            );
          in
          pkgs.symlinkJoin {
            inherit (pgbin) name version;
            paths = [ pgbin (makeReceipt pgbin ourExts) ];
          };

        # Create an attribute set, containing all the relevant packages for a
        # PostgreSQL install, wrapped up with a bow on top. There are three
        # packages:
        #
        #  - bin: the postgresql package itself, with all the extensions
        #    installed, and a receipt.json file containing metadata about the
        #    install.
        #  - exts: an attrset containing all the extensions, mapped to their
        #    package names.
        makePostgres = version: rec {
          bin = makePostgresBin version;
          exts = makeOurPostgresPkgsSet version;
          recurseForDerivations = true;
        };

        makePostgresDevSetup = { pkgs, name, extraSubstitutions ? { } }:
          let
            paths = {
              migrationsDir = builtins.path {
                name = "migrations";
                path = ./migrations/db;
              };
              postgresqlSchemaSql = builtins.path {
                name = "postgresql-schema";
                path = ./nix/tools/postgresql_schema.sql;
              };
              pgbouncerAuthSchemaSql = builtins.path {
                name = "pgbouncer-auth-schema";
                path = ./ansible/files/pgbouncer_config/pgbouncer_auth_schema.sql;
              };
              statExtensionSql = builtins.path {
                name = "stat-extension";
                path = ./ansible/files/stat_extension.sql;
              };
              pgconfigFile = builtins.path {
                name = "postgresql.conf";
                path = ./ansible/files/postgresql_config/postgresql.conf.j2;
              };
              supautilsConfigFile = builtins.path {
                name = "supautils.conf";
                path = ./ansible/files/postgresql_config/supautils.conf.j2;
              };
              loggingConfigFile = builtins.path {
                name = "logging.conf";
                path = ./ansible/files/postgresql_config/postgresql-csvlog.conf;
              };
              readReplicaConfigFile = builtins.path {
                name = "readreplica.conf";
                path = ./ansible/files/postgresql_config/custom_read_replica.conf.j2;
              };
              pgHbaConfigFile = builtins.path {
                name = "pg_hba.conf";
                path = ./ansible/files/postgresql_config/pg_hba.conf.j2;
              };
              pgIdentConfigFile = builtins.path {
                name = "pg_ident.conf";
                path = ./ansible/files/postgresql_config/pg_ident.conf.j2;
              };
              postgresqlExtensionCustomScriptsPath = builtins.path {
                name = "extension-custom-scripts";
                path = ./ansible/files/postgresql_extension_custom_scripts;
              };
              getkeyScript = builtins.path {
                name = "pgsodium_getkey.sh";
                path = ./nix/tests/util/pgsodium_getkey.sh;
              };
            };

            localeArchive =
              if pkgs.stdenv.isDarwin
              then "${pkgs.darwin.locale}/share/locale"
              else "${pkgs.glibcLocales}/lib/locale/locale-archive";

            substitutions = {
              SHELL_PATH = "${pkgs.bash}/bin/bash";
              PGSQL_DEFAULT_PORT = "${pgsqlDefaultPort}";
              PGSQL_SUPERUSER = "${pgsqlSuperuser}";
              PSQL15_BINDIR = "${basePackages.psql_15.bin}";
              PSQL17_BINDIR = "${basePackages.psql_17.bin}";
              PSQL_CONF_FILE = "${paths.pgconfigFile}";
              PSQLORIOLEDB17_BINDIR = "${basePackages.psql_orioledb-17.bin}";
              PGSODIUM_GETKEY = "${paths.getkeyScript}";
              READREPL_CONF_FILE = "${paths.readReplicaConfigFile}";
              LOGGING_CONF_FILE = "${paths.loggingConfigFile}";
              SUPAUTILS_CONF_FILE = "${paths.supautilsConfigFile}";
              PG_HBA = "${paths.pgHbaConfigFile}";
              PG_IDENT = "${paths.pgIdentConfigFile}";
              LOCALES = "${localeArchive}";
              EXTENSION_CUSTOM_SCRIPTS_DIR = "${paths.postgresqlExtensionCustomScriptsPath}";
              MECAB_LIB = "${basePackages.psql_15.exts.pgroonga}/lib/groonga/plugins/tokenizers/tokenizer_mecab.so";
              GROONGA_DIR = "${supabase-groonga}";
              MIGRATIONS_DIR = "${paths.migrationsDir}";
              POSTGRESQL_SCHEMA_SQL = "${paths.postgresqlSchemaSql}";
              PGBOUNCER_AUTH_SCHEMA_SQL = "${paths.pgbouncerAuthSchemaSql}";
              STAT_EXTENSION_SQL = "${paths.statExtensionSql}";
              CURRENT_SYSTEM = "${system}";
            } // extraSubstitutions; # Merge in any extra substitutions
          in
          pkgs.runCommand name
            {
              inherit (paths) migrationsDir postgresqlSchemaSql pgbouncerAuthSchemaSql statExtensionSql;
            } ''
            mkdir -p $out/bin $out/etc/postgresql-custom $out/etc/postgresql $out/extension-custom-scripts

            # Copy config files with error handling
            cp ${paths.supautilsConfigFile} $out/etc/postgresql-custom/supautils.conf || { echo "Failed to copy supautils.conf"; exit 1; }
            cp ${paths.pgconfigFile} $out/etc/postgresql/postgresql.conf || { echo "Failed to copy postgresql.conf"; exit 1; }
            cp ${paths.loggingConfigFile} $out/etc/postgresql-custom/logging.conf || { echo "Failed to copy logging.conf"; exit 1; }
            cp ${paths.readReplicaConfigFile} $out/etc/postgresql-custom/read-replica.conf || { echo "Failed to copy read-replica.conf"; exit 1; }
            cp ${paths.pgHbaConfigFile} $out/etc/postgresql/pg_hba.conf || { echo "Failed to copy pg_hba.conf"; exit 1; }
            cp ${paths.pgIdentConfigFile} $out/etc/postgresql/pg_ident.conf || { echo "Failed to copy pg_ident.conf"; exit 1; }
            cp -r ${paths.postgresqlExtensionCustomScriptsPath}/* $out/extension-custom-scripts/ || { echo "Failed to copy custom scripts"; exit 1; }

            echo "Copy operation completed"
            chmod 644 $out/etc/postgresql-custom/supautils.conf
            chmod 644 $out/etc/postgresql/postgresql.conf
            chmod 644 $out/etc/postgresql-custom/logging.conf
            chmod 644 $out/etc/postgresql/pg_hba.conf

            substitute ${./nix/tools/run-server.sh.in} $out/bin/start-postgres-server \
              ${builtins.concatStringsSep " " (builtins.attrValues (builtins.mapAttrs
                (name: value: "--subst-var-by '${name}' '${value}'")
                substitutions
              ))}
            chmod +x $out/bin/start-postgres-server
          '';

        # The base set of packages that we export from this Nix Flake, that can
        # be used with 'nix build'. Don't use the names listed below; check the
        # name in 'nix flake show' in order to make sure exactly what name you
        # want.
        basePackages =
          let
            # Function to get the PostgreSQL version from the attribute name
            getVersion = name:
              let
                match = builtins.match "psql_([0-9]+)" name;
              in
              if match == null then null else builtins.head match;

            # Define the available PostgreSQL versions
            postgresVersions = {
              psql_15 = makePostgres "15";
              psql_17 = makePostgres "17";
              psql_orioledb-17 = makePostgres "orioledb-17";
            };

            # Find the active PostgreSQL version
            activeVersion = getVersion (builtins.head (builtins.attrNames postgresVersions));

            # Function to create the pg_regress package
            makePgRegress = version:
              let
                postgresqlPackage = pkgs."postgresql_${version}";
              in
              pkgs.callPackage ./nix/ext/pg_regress.nix {
                postgresql = postgresqlPackage;
              };
            postgresql_15 = getPostgresqlPackage "15";
            postgresql_17 = getPostgresqlPackage "17";
            postgresql_orioledb-17 = getPostgresqlPackage "orioledb-17";
          in
          postgresVersions // {
            supabase-groonga = supabase-groonga;
            cargo-pgrx_0_11_3 = pkgs.cargo-pgrx.cargo-pgrx_0_11_3;
            cargo-pgrx_0_12_6 = pkgs.cargo-pgrx.cargo-pgrx_0_12_6;
            cargo-pgrx_0_12_9 = pkgs.cargo-pgrx.cargo-pgrx_0_12_9;
            cargo-pgrx_0_14_3 = pkgs.cargo-pgrx.cargo-pgrx_0_14_3;
            # PostgreSQL versions.
            psql_15 = postgresVersions.psql_15;
            psql_17 = postgresVersions.psql_17;
            psql_orioledb-17 = postgresVersions.psql_orioledb-17;
            wal-g-2 = wal-g-2;
            wal-g-3 = wal-g-3;
            sfcgal = sfcgal;
            pg_prove = pkgs.perlPackages.TAPParserSourceHandlerpgTAP;
            inherit postgresql_15 postgresql_17 postgresql_orioledb-17;
            postgresql_15_debug = if pkgs.stdenv.isLinux then postgresql_15.debug else null;
            postgresql_17_debug = if pkgs.stdenv.isLinux then postgresql_17.debug else null;
            postgresql_orioledb-17_debug = if pkgs.stdenv.isLinux then postgresql_orioledb-17.debug else null;
            postgresql_15_src = pkgs.stdenv.mkDerivation {
              pname = "postgresql-15-src";
              version = postgresql_15.version;

              src = postgresql_15.src;

              nativeBuildInputs = [ pkgs.bzip2 ];

              phases = [ "unpackPhase" "installPhase" ];

              installPhase = ''
                mkdir -p $out
                cp -r . $out
              '';

              meta = with pkgs.lib; {
                description = "PostgreSQL 15 source files";
                homepage = "https://www.postgresql.org/";
                license = licenses.postgresql;
                platforms = platforms.all;
              };
            };
            postgresql_17_src = pkgs.stdenv.mkDerivation {
              pname = "postgresql-17-src";
              version = postgresql_17.version;
              src = postgresql_17.src;

              nativeBuildInputs = [ pkgs.bzip2 ];

              phases = [ "unpackPhase" "installPhase" ];

              installPhase = ''
                mkdir -p $out
                cp -r . $out
              '';
              meta = with pkgs.lib; {
                description = "PostgreSQL 17 source files";
                homepage = "https://www.postgresql.org/";
                license = licenses.postgresql;
                platforms = platforms.all;
              };
            };
            postgresql_orioledb-17_src = pkgs.stdenv.mkDerivation {
              pname = "postgresql-17-src";
              version = postgresql_orioledb-17.version;

              src = postgresql_orioledb-17.src;

              nativeBuildInputs = [ pkgs.bzip2 ];

              phases = [ "unpackPhase" "installPhase" ];

              installPhase = ''
                mkdir -p $out
                cp -r . $out
              '';

              meta = with pkgs.lib; {
                description = "PostgreSQL 15 source files";
                homepage = "https://www.postgresql.org/";
                license = licenses.postgresql;
                platforms = platforms.all;
              };
            };
            mecab_naist_jdic = mecab-naist-jdic;
            supabase_groonga = supabase-groonga;
            pg_regress = makePgRegress activeVersion;
            # Start a version of the server.
            start-server = makePostgresDevSetup {
              inherit pkgs;
              name = "start-postgres-server";
            };

            # Start a version of the client and runs migrations script on server.
            start-client =
              let
                migrationsDir = ./migrations/db;
                postgresqlSchemaSql = ./nix/tools/postgresql_schema.sql;
                pgbouncerAuthSchemaSql = ./ansible/files/pgbouncer_config/pgbouncer_auth_schema.sql;
                statExtensionSql = ./ansible/files/stat_extension.sql;
              in
              pkgs.runCommand "start-postgres-client" { } ''
                mkdir -p $out/bin
                substitute ${./nix/tools/run-client.sh.in} $out/bin/start-postgres-client \
                  --subst-var-by 'PGSQL_DEFAULT_PORT' '${pgsqlDefaultPort}' \
                  --subst-var-by 'PGSQL_SUPERUSER' '${pgsqlSuperuser}' \
                  --subst-var-by 'PSQL15_BINDIR' '${basePackages.psql_15.bin}' \
                  --subst-var-by 'PSQL17_BINDIR' '${basePackages.psql_17.bin}' \
                  --subst-var-by 'PSQLORIOLEDB17_BINDIR' '${basePackages.psql_orioledb-17.bin}' \
                  --subst-var-by 'MIGRATIONS_DIR' '${migrationsDir}' \
                  --subst-var-by 'POSTGRESQL_SCHEMA_SQL' '${postgresqlSchemaSql}' \
                  --subst-var-by 'PGBOUNCER_AUTH_SCHEMA_SQL' '${pgbouncerAuthSchemaSql}' \
                  --subst-var-by 'STAT_EXTENSION_SQL' '${statExtensionSql}'
                chmod +x $out/bin/start-postgres-client
              '';

            # Migrate between two data directories.
            migrate-tool =
              let
                configFile = ./nix/tests/postgresql.conf.in;
                getkeyScript = ./nix/tests/util/pgsodium_getkey.sh;
                primingScript = ./nix/tests/prime.sql;
                migrationData = ./nix/tests/migrations/data.sql;
              in
              pkgs.runCommand "migrate-postgres" { } ''
                mkdir -p $out/bin
                substitute ${./nix/tools/migrate-tool.sh.in} $out/bin/migrate-postgres \
                  --subst-var-by 'PSQL15_BINDIR' '${basePackages.psql_15.bin}' \
                  --subst-var-by 'PSQL_CONF_FILE' '${configFile}' \
                  --subst-var-by 'PGSODIUM_GETKEY' '${getkeyScript}' \
                  --subst-var-by 'PRIMING_SCRIPT' '${primingScript}' \
                  --subst-var-by 'MIGRATION_DATA' '${migrationData}'

                chmod +x $out/bin/migrate-postgres
              '';

            start-replica = pkgs.runCommand "start-postgres-replica" { } ''
              mkdir -p $out/bin
              substitute ${./nix/tools/run-replica.sh.in} $out/bin/start-postgres-replica \
                --subst-var-by 'PGSQL_SUPERUSER' '${pgsqlSuperuser}' \
                --subst-var-by 'PSQL15_BINDIR' '${basePackages.psql_15.bin}'
              chmod +x $out/bin/start-postgres-replica
            '';
            pg-restore =
              pkgs.runCommand "run-pg-restore" { } ''
                mkdir -p $out/bin
                substitute ${./nix/tools/run-restore.sh.in} $out/bin/pg-restore \
                  --subst-var-by PSQL15_BINDIR '${basePackages.psql_15.bin}'
                chmod +x $out/bin/pg-restore
              '';
            sync-exts-versions = pkgs.runCommand "sync-exts-versions" { } ''
              mkdir -p $out/bin
              substitute ${./nix/tools/sync-exts-versions.sh.in} $out/bin/sync-exts-versions \
                --subst-var-by 'YQ' '${pkgs.yq}/bin/yq' \
                --subst-var-by 'JQ' '${pkgs.jq}/bin/jq' \
                --subst-var-by 'NIX_EDITOR' '${nix-editor.packages.${system}.nix-editor}/bin/nix-editor' \
                --subst-var-by 'NIXPREFETCHURL' '${pkgs.nixVersions.nix_2_20}/bin/nix-prefetch-url' \
                --subst-var-by 'NIX' '${pkgs.nixVersions.nix_2_20}/bin/nix'
              chmod +x $out/bin/sync-exts-versions
            '';

            local-infra-bootstrap = pkgs.runCommand "local-infra-bootstrap" { } ''
              mkdir -p $out/bin
              substitute ${./nix/tools/local-infra-bootstrap.sh.in} $out/bin/local-infra-bootstrap
              chmod +x $out/bin/local-infra-bootstrap
            '';
            dbmate-tool =
              let
                migrationsDir = ./migrations/db;
                ansibleVars = ./ansible/vars.yml;
                pgbouncerAuthSchemaSql = ./ansible/files/pgbouncer_config/pgbouncer_auth_schema.sql;
                statExtensionSql = ./ansible/files/stat_extension.sql;
              in
              pkgs.runCommand "dbmate-tool"
                {
                  buildInputs = with pkgs; [
                    overmind
                    dbmate
                    nix
                    jq
                    yq
                  ];
                  nativeBuildInputs = with pkgs; [
                    makeWrapper
                  ];
                } ''
                mkdir -p $out/bin $out/migrations
                cp -r ${migrationsDir}/* $out
                substitute ${./nix/tools/dbmate-tool.sh.in} $out/bin/dbmate-tool \
                  --subst-var-by 'PGSQL_DEFAULT_PORT' '${pgsqlDefaultPort}' \
                  --subst-var-by 'MIGRATIONS_DIR' $out \
                  --subst-var-by 'PGSQL_SUPERUSER' '${pgsqlSuperuser}' \
                  --subst-var-by 'ANSIBLE_VARS' ${ansibleVars} \
                  --subst-var-by 'CURRENT_SYSTEM' '${system}' \
                  --subst-var-by 'PGBOUNCER_AUTH_SCHEMA_SQL' '${pgbouncerAuthSchemaSql}' \
                  --subst-var-by 'STAT_EXTENSION_SQL' '${statExtensionSql}'
                chmod +x $out/bin/dbmate-tool
                wrapProgram $out/bin/dbmate-tool \
                  --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.overmind pkgs.dbmate pkgs.nix pkgs.jq pkgs.yq ]}
              '';
            show-commands = pkgs.runCommand "show-commands"
              {
                nativeBuildInputs = [ pkgs.makeWrapper ];
                buildInputs = [ pkgs.nushell ];
              } ''
              mkdir -p $out/bin
              cat > $out/bin/show-commands << 'EOF'
              #!${pkgs.nushell}/bin/nu
              let json_output = (nix flake show --json --quiet --all-systems | from json)
              let apps = ($json_output | get apps.${system})
              $apps | transpose name info | select name | each { |it| echo $"Run this app with: nix run .#($it.name)" }
              EOF
              chmod +x $out/bin/show-commands
              wrapProgram $out/bin/show-commands \
                --prefix PATH : ${pkgs.nushell}/bin
            '';
            update-readme = pkgs.runCommand "update-readme"
              {
                nativeBuildInputs = [ pkgs.makeWrapper ];
                buildInputs = [ pkgs.nushell ];
              } ''
              mkdir -p $out/bin
              cp ${./nix/tools/update_readme.nu} $out/bin/update-readme
              chmod +x $out/bin/update-readme
              wrapProgram $out/bin/update-readme \
                --prefix PATH : ${pkgs.nushell}/bin
            '';
            # Script to run the AMI build and tests locally
            build-test-ami = pkgs.runCommand "build-test-ami"
              {
                buildInputs = with pkgs; [
                  packer
                  awscli2
                  yq
                  jq
                  openssl
                  git
                  coreutils
                  aws-vault
                ];
              } ''
                mkdir -p $out/bin
                cat > $out/bin/build-test-ami << 'EOL'
                #!/usr/bin/env bash
                set -euo pipefail

                show_help() {
                  cat << EOF
                Usage: build-test-ami [--help] <postgres-version>

                Build AMI images for PostgreSQL testing.

                This script will:
                1. Check for required tools and AWS authentication
                2. Build two AMI stages using Packer
                3. Clean up any temporary instances
                4. Output the final AMI name for use with run-testinfra

                Arguments:
                  postgres-version    PostgreSQL major version to build (required)

                Options:
                  --help    Show this help message and exit

                Requirements:
                  - AWS Vault profile must be set in AWS_VAULT environment variable
                  - Packer, AWS CLI, yq, jq, and OpenSSL must be installed
                  - Must be run from a git repository

                Example:
                  aws-vault exec <profile-name> -- nix run .#build-test-ami 15
                EOF
                }

                # Handle help flag
                if [[ "$#" -gt 0 && "$1" == "--help" ]]; then
                  show_help
                  exit 0
                fi

                export PATH="${pkgs.lib.makeBinPath (with pkgs; [
                  packer
                  awscli2
                  yq
                  jq
                  openssl
                  git
                  coreutils
                  aws-vault
                ])}:$PATH"

                # Check for required tools
                for cmd in packer aws-vault yq jq openssl; do
                  if ! command -v $cmd &> /dev/null; then
                    echo "Error: $cmd is required but not found"
                    exit 1
                  fi
                done

                # Check AWS Vault profile
                if [ -z "''${AWS_VAULT:-}" ]; then
                  echo "Error: AWS_VAULT environment variable must be set with the profile name"
                  echo "Usage: aws-vault exec <profile-name> -- nix run .#build-test-ami <postgres-version>"
                  exit 1
                fi

                # Set values
                REGION="ap-southeast-1"
                POSTGRES_VERSION="$1"
                RANDOM_STRING=$(openssl rand -hex 8)
                GIT_SHA=$(git rev-parse HEAD)
                RUN_ID=$(date +%s)

                # Generate common-nix.vars.pkr.hcl
                PG_VERSION=$(yq -r ".postgres_release[\"postgres$POSTGRES_VERSION\"]" ansible/vars.yml)
                echo "postgres-version = \"$PG_VERSION\"" > common-nix.vars.pkr.hcl

                # Build AMI Stage 1
                packer init amazon-arm64-nix.pkr.hcl
                packer build \
                  -var "git-head-version=$GIT_SHA" \
                  -var "packer-execution-id=$RUN_ID" \
                  -var-file="development-arm.vars.pkr.hcl" \
                  -var-file="common-nix.vars.pkr.hcl" \
                  -var "ansible_arguments=" \
                  -var "postgres-version=$RANDOM_STRING" \
                  -var "region=$REGION" \
                  -var 'ami_regions=["'"$REGION"'"]' \
                  -var "force-deregister=true" \
                  -var "ansible_arguments=-e postgresql_major=$POSTGRES_VERSION" \
                  amazon-arm64-nix.pkr.hcl

                # Build AMI Stage 2
                packer init stage2-nix-psql.pkr.hcl
                packer build \
                  -var "git-head-version=$GIT_SHA" \
                  -var "packer-execution-id=$RUN_ID" \
                  -var "postgres_major_version=$POSTGRES_VERSION" \
                  -var-file="development-arm.vars.pkr.hcl" \
                  -var-file="common-nix.vars.pkr.hcl" \
                  -var "postgres-version=$RANDOM_STRING" \
                  -var "region=$REGION" \
                  -var 'ami_regions=["'"$REGION"'"]' \
                  -var "force-deregister=true" \
                  -var "git_sha=$GIT_SHA" \
                  stage2-nix-psql.pkr.hcl

                # Cleanup instances from AMI builds
                cleanup_instances() {
                  echo "Terminating EC2 instances with tag testinfra-run-id=$RUN_ID..."
                  aws ec2 --region $REGION describe-instances \
                    --filters "Name=tag:testinfra-run-id,Values=$RUN_ID" \
                    --query "Reservations[].Instances[].InstanceId" \
                    --output text | xargs -r aws ec2 terminate-instances \
                    --region $REGION --instance-ids || true
                }

                # Set up traps for various signals to ensure cleanup
                trap cleanup_instances EXIT HUP INT QUIT TERM

                # Create and activate virtual environment
                VENV_DIR=$(mktemp -d)
                trap 'rm -rf "$VENV_DIR"' EXIT HUP INT QUIT TERM
                python3 -m venv "$VENV_DIR"
                source "$VENV_DIR/bin/activate"

                # Install required Python packages
                echo "Installing required Python packages..."
                pip install boto3 boto3-stubs[essential] docker ec2instanceconnectcli pytest paramiko requests

                # Run the tests with aws-vault
                echo "Running tests for AMI: $RANDOM_STRING using AWS Vault profile: $AWS_VAULT_PROFILE"
                aws-vault exec $AWS_VAULT_PROFILE -- pytest -vv -s testinfra/test_ami_nix.py

                # Deactivate virtual environment (cleanup is handled by trap)
                deactivate
                EOL
                chmod +x $out/bin/build-test-ami
              '';

            run-testinfra = pkgs.runCommand "run-testinfra"
              {
                buildInputs = with pkgs; [
                  aws-vault
                  python3
                  python3Packages.pip
                  coreutils
                ];
              } ''
                mkdir -p $out/bin
                cat > $out/bin/run-testinfra << 'EOL'
                #!/usr/bin/env bash
                set -euo pipefail

                show_help() {
                  cat << EOF
                Usage: run-testinfra --ami-name NAME [--aws-vault-profile PROFILE]

                Run the testinfra tests locally against a specific AMI.

                This script will:
                1. Check if aws-vault is installed and configured
                2. Set up the required environment variables
                3. Create and activate a virtual environment
                4. Install required Python packages from pip
                5. Run the tests with aws-vault credentials
                6. Clean up the virtual environment

                Required flags:
                  --ami-name NAME              The name of the AMI to test

                Optional flags:
                  --aws-vault-profile PROFILE  AWS Vault profile to use (default: staging)
                  --help                       Show this help message and exit

                Requirements:
                  - aws-vault installed and configured
                  - Python 3 with pip
                  - Must be run from the repository root

                Examples:
                  run-testinfra --ami-name supabase-postgres-abc123
                  run-testinfra --ami-name supabase-postgres-abc123 --aws-vault-profile production
                EOF
                }

                # Default values
                AWS_VAULT_PROFILE="staging"
                AMI_NAME=""

                # Parse arguments
                while [[ $# -gt 0 ]]; do
                  case $1 in
                    --aws-vault-profile)
                      AWS_VAULT_PROFILE="$2"
                      shift 2
                      ;;
                    --ami-name)
                      AMI_NAME="$2"
                      shift 2
                      ;;
                    --help)
                      show_help
                      exit 0
                      ;;
                    *)
                      echo "Error: Unexpected argument: $1"
                      show_help
                      exit 1
                      ;;
                  esac
                done

                # Check for required tools
                if ! command -v aws-vault &> /dev/null; then
                  echo "Error: aws-vault is required but not found"
                  exit 1
                fi

                # Check for AMI name argument
                if [ -z "$AMI_NAME" ]; then
                  echo "Error: --ami-name is required"
                  show_help
                  exit 1
                fi

                # Set environment variables
                export AWS_REGION="ap-southeast-1"
                export AWS_DEFAULT_REGION="ap-southeast-1"
                export AMI_NAME="$AMI_NAME"  # Export AMI_NAME for pytest
                export RUN_ID="local-$(date +%s)"  # Generate a unique RUN_ID

                # Function to terminate EC2 instances
                terminate_instances() {
                  echo "Terminating EC2 instances with tag testinfra-run-id=$RUN_ID..."
                  aws-vault exec $AWS_VAULT_PROFILE -- aws ec2 --region ap-southeast-1 describe-instances \
                    --filters "Name=tag:testinfra-run-id,Values=$RUN_ID" \
                    --query "Reservations[].Instances[].InstanceId" \
                    --output text | xargs -r aws-vault exec $AWS_VAULT_PROFILE -- aws ec2 terminate-instances \
                    --region ap-southeast-1 --instance-ids || true
                }

                # Set up traps for various signals to ensure cleanup
                trap terminate_instances EXIT HUP INT QUIT TERM

                # Create and activate virtual environment
                VENV_DIR=$(mktemp -d)
                trap 'rm -rf "$VENV_DIR"' EXIT HUP INT QUIT TERM
                python3 -m venv "$VENV_DIR"
                source "$VENV_DIR/bin/activate"

                # Install required Python packages
                echo "Installing required Python packages..."
                pip install boto3 boto3-stubs[essential] docker ec2instanceconnectcli pytest paramiko requests

                # Function to run tests and ensure cleanup
                run_tests() {
                  local exit_code=0
                  echo "Running tests for AMI: $AMI_NAME using AWS Vault profile: $AWS_VAULT_PROFILE"
                  aws-vault exec "$AWS_VAULT_PROFILE" -- pytest -vv -s testinfra/test_ami_nix.py || exit_code=$?
                  return $exit_code
                }

                # Run tests and capture exit code
                run_tests
                test_exit_code=$?

                # Deactivate virtual environment
                deactivate

                # Explicitly call cleanup
                terminate_instances

                # Exit with the test exit code
                exit $test_exit_code
                EOL
                chmod +x $out/bin/run-testinfra
              '';

            cleanup-ami = pkgs.runCommand "cleanup-ami"
              {
                buildInputs = with pkgs; [
                  awscli2
                  aws-vault
                ];
              } ''
                mkdir -p $out/bin
                cat > $out/bin/cleanup-ami << 'EOL'
                #!/usr/bin/env bash
                set -euo pipefail

                export PATH="${pkgs.lib.makeBinPath (with pkgs; [
                  awscli2
                  aws-vault
                ])}:$PATH"

                # Check for required tools
                for cmd in aws-vault; do
                  if ! command -v $cmd &> /dev/null; then
                    echo "Error: $cmd is required but not found"
                    exit 1
                  fi
                done

                # Check AWS Vault profile
                if [ -z "''${AWS_VAULT:-}" ]; then
                  echo "Error: AWS_VAULT environment variable must be set with the profile name"
                  echo "Usage: aws-vault exec <profile-name> -- nix run .#cleanup-ami <ami-name>"
                  exit 1
                fi

                # Check for AMI name argument
                if [ -z "''${1:-}" ]; then
                  echo "Error: AMI name must be provided"
                  echo "Usage: aws-vault exec <profile-name> -- nix run .#cleanup-ami <ami-name>"
                  exit 1
                fi

                AMI_NAME="$1"
                REGION="ap-southeast-1"

                # Deregister AMIs
                for AMI_PATTERN in "supabase-postgres-ci-ami-test-stage-1" "$AMI_NAME"; do
                  aws ec2 describe-images --region $REGION --owners self \
                    --filters "Name=name,Values=$AMI_PATTERN" \
                    --query 'Images[*].ImageId' --output text | while read -r ami_id; do
                      echo "Deregistering AMI: $ami_id"
                      aws ec2 deregister-image --region $REGION --image-id "$ami_id" || true
                    done
                done
                EOL
                chmod +x $out/bin/cleanup-ami
              '';

            trigger-nix-build = pkgs.runCommand "trigger-nix-build"
              {
                buildInputs = with pkgs; [
                  gh
                  git
                  coreutils
                ];
              } ''
                mkdir -p $out/bin
                cat > $out/bin/trigger-nix-build << 'EOL'
                #!/usr/bin/env bash
                set -euo pipefail

                show_help() {
                  cat << EOF
                Usage: trigger-nix-build [--help]

                Trigger the nix-build workflow for the current branch and watch its progress.

                This script will:
                1. Check if you're authenticated with GitHub
                2. Get the current branch and commit
                3. Verify you're on a standard branch (develop or release/*) or prompt for confirmation
                4. Trigger the nix-build workflow
                5. Watch the workflow progress until completion

                Options:
                  --help    Show this help message and exit

                Requirements:
                  - GitHub CLI (gh) installed and authenticated
                  - Git installed
                  - Must be run from a git repository

                Example:
                  trigger-nix-build
                EOF
                }

                # Handle help flag
                if [[ "$#" -gt 0 && "$1" == "--help" ]]; then
                  show_help
                  exit 0
                fi

                export PATH="${pkgs.lib.makeBinPath (with pkgs; [
                  gh
                  git
                  coreutils
                ])}:$PATH"

                # Check for required tools
                for cmd in gh git; do
                  if ! command -v $cmd &> /dev/null; then
                    echo "Error: $cmd is required but not found"
                    exit 1
                  fi
                done

                # Check if user is authenticated with GitHub
                if ! gh auth status &>/dev/null; then
                  echo "Error: Not authenticated with GitHub. Please run 'gh auth login' first."
                  exit 1
                fi

                # Get current branch and commit
                BRANCH=$(git rev-parse --abbrev-ref HEAD)
                COMMIT=$(git rev-parse HEAD)

                # Check if we're on a standard branch
                if [[ "$BRANCH" != "develop" && ! "$BRANCH" =~ ^release/ ]]; then
                  echo "Warning: Running workflow from non-standard branch: $BRANCH"
                  echo "This is supported for testing purposes."
                  read -p "Continue? [y/N] " -n 1 -r
                  echo
                  if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                    echo "Aborted."
                    exit 1
                  fi
                fi

                # Trigger the workflow
                echo "Triggering nix-build workflow for branch $BRANCH (commit: $COMMIT)"
                gh workflow run nix-build.yml --ref "$BRANCH"

                # Wait for workflow to start and get the run ID
                echo "Waiting for workflow to start..."
                sleep 5

                # Get the latest run ID for this workflow
                RUN_ID=$(gh run list --workflow=nix-build.yml --limit 1 --json databaseId --jq '.[0].databaseId')

                if [ -z "$RUN_ID" ]; then
                  echo "Error: Could not find workflow run ID"
                  exit 1
                fi

                echo "Watching workflow run $RUN_ID..."
                echo "The script will automatically exit when the workflow completes."
                echo "Press Ctrl+C to stop watching (workflow will continue running)"
                echo "----------------------------------------"

                # Try to watch the run, but handle network errors gracefully
                while true; do
                  if gh run watch "$RUN_ID" --exit-status; then
                    break
                  else
                    echo "Network error while watching workflow. Retrying in 5 seconds..."
                    echo "You can also check the status manually with: gh run view $RUN_ID"
                    sleep 5
                  fi
                done
                EOL
                chmod +x $out/bin/trigger-nix-build
              '';
          };


        # Create a testing harness for a PostgreSQL package. This is used for
        # 'nix flake check', and works with any PostgreSQL package you hand it.

        makeCheckHarness = pgpkg:
          let
            sqlTests = ./nix/tests/smoke;
            pg_prove = pkgs.perlPackages.TAPParserSourceHandlerpgTAP;
            pg_regress = basePackages.pg_regress;
            getkey-script = pkgs.stdenv.mkDerivation {
              name = "pgsodium-getkey";
              buildCommand = ''
                mkdir -p $out/bin
                cat > $out/bin/pgsodium-getkey << 'EOF'
                #!${pkgs.bash}/bin/bash
                set -euo pipefail

                TMPDIR_BASE=$(mktemp -d)

                KEY_DIR="''${PGSODIUM_KEY_DIR:-$TMPDIR_BASE/pgsodium}"
                KEY_FILE="$KEY_DIR/pgsodium.key"

                if ! mkdir -p "$KEY_DIR" 2>/dev/null; then
                  echo "Error: Could not create key directory $KEY_DIR" >&2
                  exit 1
                fi
                chmod 1777 "$KEY_DIR"

                if [[ ! -f "$KEY_FILE" ]]; then
                  if ! (dd if=/dev/urandom bs=32 count=1 2>/dev/null | od -A n -t x1 | tr -d ' \n' > "$KEY_FILE"); then
                    if ! (openssl rand -hex 32 > "$KEY_FILE"); then
                      echo "00000000000000000000000000000000" > "$KEY_FILE"
                      echo "Warning: Using fallback key" >&2
                    fi
                  fi
                  chmod 644 "$KEY_FILE"
                fi

                if [[ -f "$KEY_FILE" && -r "$KEY_FILE" ]]; then
                  cat "$KEY_FILE"
                else
                  echo "Error: Cannot read key file $KEY_FILE" >&2
                  exit 1
                fi
                EOF
                chmod +x $out/bin/pgsodium-getkey
              '';
            };

            # Use the shared setup but with a test-specific name
            start-postgres-server-bin = makePostgresDevSetup {
              inherit pkgs;
              name = "start-postgres-server-test";
              extraSubstitutions = {
                PGSODIUM_GETKEY = "${getkey-script}/bin/pgsodium-getkey";
                PGSQL_DEFAULT_PORT = pgPort;
              };
            };

            getVersionArg = pkg:
              let
                name = pkg.version;
              in
              if builtins.match "15.*" name != null then "15"
              else if builtins.match "17.*" name != null then "17"
              else if builtins.match "orioledb-17.*" name != null then "orioledb-17"
              else throw "Unsupported PostgreSQL version: ${name}";

            # Helper function to filter SQL files based on version
            filterTestFiles = version: dir:
              let
                files = builtins.readDir dir;
                isValidFile = name:
                  let
                    isVersionSpecific = builtins.match "z_.*" name != null;
                    matchesVersion =
                      if isVersionSpecific
                      then
                        if version == "orioledb-17"
                        then builtins.match "z_orioledb-17_.*" name != null
                        else if version == "17"
                        then builtins.match "z_17_.*" name != null
                        else builtins.match "z_15_.*" name != null
                      else true;
                  in
                  pkgs.lib.hasSuffix ".sql" name && matchesVersion;
              in
              pkgs.lib.filterAttrs (name: _: isValidFile name) files;

            # Get the major version for filtering
            majorVersion =
              let
                version = builtins.trace "pgpkg.version is: ${pgpkg.version}" pgpkg.version;
                _ = builtins.trace "Entering majorVersion logic";
                isOrioledbMatch = builtins.match "^17_[0-9]+$" version != null;
                isSeventeenMatch = builtins.match "^17[.][0-9]+$" version != null;
                result =
                  if isOrioledbMatch
                  then "orioledb-17"
                  else if isSeventeenMatch
                  then "17"
                  else "15";
              in
              builtins.trace "Major version result: ${result}" result; # Trace the result                                             # For "15.8"

            # Filter SQL test files
            filteredSqlTests = filterTestFiles majorVersion ./nix/tests/sql;

            pgPort = if (majorVersion == "17") then
                "5535"
                else if (majorVersion == "15") then
                "5536"
                else "5537";

            # Convert filtered tests to a sorted list of basenames (without extension)
            testList = pkgs.lib.mapAttrsToList
              (name: _:
                builtins.substring 0 (pkgs.lib.stringLength name - 4) name
              )
              filteredSqlTests;
            sortedTestList = builtins.sort (a: b: a < b) testList;

          in
          pkgs.runCommand "postgres-${pgpkg.version}-check-harness"
            {
              nativeBuildInputs = with pkgs; [
                coreutils
                bash
                perl
                pgpkg
                pg_prove
                pg_regress
                procps
                start-postgres-server-bin
                which
                getkey-script
                supabase-groonga
              ];
            } ''
            set -e

            #First we need to create a generic pg cluster for pgtap tests and run those
            export GRN_PLUGINS_DIR=${supabase-groonga}/lib/groonga/plugins
            PGTAP_CLUSTER=$(mktemp -d)
            initdb --locale=C --username=supabase_admin -D "$PGTAP_CLUSTER"
            substitute ${./nix/tests/postgresql.conf.in} "$PGTAP_CLUSTER"/postgresql.conf \
              --subst-var-by PGSODIUM_GETKEY_SCRIPT "${getkey-script}/bin/pgsodium-getkey"
            echo "listen_addresses = '*'" >> "$PGTAP_CLUSTER"/postgresql.conf
            echo "port = ${pgPort}" >> "$PGTAP_CLUSTER"/postgresql.conf
            echo "host all all 127.0.0.1/32 trust" >> $PGTAP_CLUSTER/pg_hba.conf
            echo "Checking shared_preload_libraries setting:"
            grep -rn "shared_preload_libraries" "$PGTAP_CLUSTER"/postgresql.conf
            # Remove timescaledb if running orioledb-17 check
            echo "I AM ${pgpkg.version}===================================================="
            if [[ "${pgpkg.version}" == *"17"* ]]; then
              perl -pi -e 's/ timescaledb,//g' "$PGTAP_CLUSTER/postgresql.conf"
            fi
            #NOTE in the future we may also need to add the orioledb extension to the cluster when cluster is oriole
            echo "PGTAP_CLUSTER directory contents:"
            ls -la "$PGTAP_CLUSTER"

            # Check if postgresql.conf exists
            if [ ! -f "$PGTAP_CLUSTER/postgresql.conf" ]; then
                echo "postgresql.conf is missing!"
                exit 1
            fi

            # PostgreSQL startup
            if [[ "$(uname)" == "Darwin" ]]; then
            pg_ctl -D "$PGTAP_CLUSTER" -l "$PGTAP_CLUSTER"/postgresql.log -o "-k "$PGTAP_CLUSTER" -p ${pgPort} -d 5" start 2>&1
            else
            mkdir -p "$PGTAP_CLUSTER/sockets"
            pg_ctl -D "$PGTAP_CLUSTER" -l "$PGTAP_CLUSTER"/postgresql.log -o "-k $PGTAP_CLUSTER/sockets -p ${pgPort} -d 5" start 2>&1
            fi || {
            echo "pg_ctl failed to start PostgreSQL"
            echo "Contents of postgresql.log:"
            cat "$PGTAP_CLUSTER"/postgresql.log
            exit 1
            }
            for i in {1..60}; do
              if pg_isready -h ${pgsqlDefaultHost} -p ${pgPort}; then
                echo "PostgreSQL is ready"
                break
              fi
              sleep 1
              if [ $i -eq 60 ]; then
                echo "PostgreSQL is not ready after 60 seconds"
                echo "PostgreSQL status:"
                pg_ctl -D "$PGTAP_CLUSTER" status
                echo "PostgreSQL log content:"
                cat "$PGTAP_CLUSTER"/postgresql.log
                exit 1
              fi
            done
            createdb -p ${pgPort} -h ${pgsqlDefaultHost} --username=supabase_admin testing
            if ! psql -p ${pgPort} -h ${pgsqlDefaultHost} --username=supabase_admin -d testing -v ON_ERROR_STOP=1 -Xf ${./nix/tests/prime.sql}; then
              echo "Error executing SQL file. PostgreSQL log content:"
              cat "$PGTAP_CLUSTER"/postgresql.log
              pg_ctl -D "$PGTAP_CLUSTER" stop
              exit 1
            fi
            SORTED_DIR=$(mktemp -d)
            for t in $(printf "%s\n" ${builtins.concatStringsSep " " sortedTestList}); do
              psql -p ${pgPort} -h ${pgsqlDefaultHost} --username=supabase_admin -d testing -f "${./nix/tests/sql}/$t.sql" || true
            done
            rm -rf "$SORTED_DIR"
            pg_ctl -D "$PGTAP_CLUSTER" stop
            rm -rf $PGTAP_CLUSTER

            # End of pgtap tests
            # from here on out we are running pg_regress tests, we use a different cluster for this
            # which is start by the start-postgres-server-bin script
            # start-postgres-server-bin script closely matches our AMI setup, configurations and migrations

            unset GRN_PLUGINS_DIR
            ${start-postgres-server-bin}/bin/start-postgres-server ${getVersionArg pgpkg} --daemonize

            for i in {1..60}; do
                if pg_isready -h ${pgsqlDefaultHost} -p ${pgPort} -U supabase_admin -q; then
                    echo "PostgreSQL is ready"
                    break
                fi
                sleep 1
                if [ $i -eq 60 ]; then
                    echo "PostgreSQL failed to start"
                    exit 1
                fi
            done

            if ! psql -p ${pgPort} -h ${pgsqlDefaultHost} --no-password --username=supabase_admin -d postgres -v ON_ERROR_STOP=1 -Xf ${./nix/tests/prime.sql}; then
              echo "Error executing SQL file"
              exit 1
            fi

            mkdir -p $out/regression_output
            if ! pg_regress \
              --use-existing \
              --dbname=postgres \
              --inputdir=${./nix/tests} \
              --outputdir=$out/regression_output \
              --host=${pgsqlDefaultHost} \
              --port=${pgPort} \
              --user=supabase_admin \
              ${builtins.concatStringsSep " " sortedTestList}; then
              echo "pg_regress tests failed"
              cat $out/regression_output/regression.diffs
              exit 1
            fi

            echo "Running migrations tests"
            pg_prove -p ${pgPort} -U supabase_admin -h ${pgsqlDefaultHost} -d postgres -v ${./migrations/tests}/test.sql

            # Copy logs to output
            for logfile in $(find /tmp -name postgresql.log -type f); do
              cp "$logfile" $out/postgresql.log
            done
            exit 0
          '';
      in
      rec {
        # The list of all packages that can be built with 'nix build'. The list
        # of names that can be used can be shown with 'nix flake show'
        packages = flake-utils.lib.flattenTree basePackages // {
          # Any extra packages we might want to include in our package
          # set can go here.
          inherit (pkgs);
        };

        # The list of exported 'checks' that are run with every run of 'nix
        # flake check'. This is run in the CI system, as well.
        checks = {
          psql_15 = makeCheckHarness basePackages.psql_15.bin;
          psql_17 = makeCheckHarness basePackages.psql_17.bin;
          psql_orioledb-17 = makeCheckHarness basePackages.psql_orioledb-17.bin;
          inherit (basePackages) wal-g-2 wal-g-3 dbmate-tool pg_regress;
        } // pkgs.lib.optionalAttrs (system == "aarch64-linux") {
          inherit (basePackages) postgresql_15_debug postgresql_15_src postgresql_orioledb-17_debug postgresql_orioledb-17_src postgresql_17_debug postgresql_17_src;
        };

        # Apps is a list of names of things that can be executed with 'nix run';
        # these are distinct from the things that can be built with 'nix build',
        # so they need to be listed here too.
        apps =
          let
            mkApp = attrName: binName: {
              type = "app";
              program = "${basePackages."${attrName}"}/bin/${binName}";
            };
          in
          {
            start-server = mkApp "start-server" "start-postgres-server";
            start-client = mkApp "start-client" "start-postgres-client";
            start-replica = mkApp "start-replica" "start-postgres-replica";
            # migrate-postgres = mkApp "migrate-tool" "migrate-postgres";
            # sync-exts-versions = mkApp "sync-exts-versions" "sync-exts-versions";
            pg-restore = mkApp "pg-restore" "pg-restore";
            local-infra-bootstrap = mkApp "local-infra-bootstrap" "local-infra-bootstrap";
            dbmate-tool = mkApp "dbmate-tool" "dbmate-tool";
            update-readme = mkApp "update-readme" "update-readme";
            show-commands = mkApp "show-commands" "show-commands";
            build-test-ami = mkApp "build-test-ami" "build-test-ami";
            run-testinfra = mkApp "run-testinfra" "run-testinfra";
            cleanup-ami = mkApp "cleanup-ami" "cleanup-ami";
            trigger-nix-build = mkApp "trigger-nix-build" "trigger-nix-build";
          };

        # 'devShells.default' lists the set of packages that are included in the
        # ambient $PATH environment when you run 'nix develop'. This is useful
        # for development and puts many convenient devtools instantly within
        # reach.

        devShells =
          let
            mkCargoPgrxDevShell = { pgrxVersion, rustVersion }: pkgs.mkShell {
              packages = with pkgs; [
                basePackages."cargo-pgrx_${pgrxVersion}"
                (rust-bin.stable.${rustVersion}.default.override {
                  extensions = [ "rust-src" ];
                })
              ];
              shellHook = ''
                export HISTFILE=.history
              '';
            };
          in
          {
            default = pkgs.mkShell {
              packages = with pkgs; [
                coreutils
                just
                nix-update
                #pg_prove
                shellcheck
                ansible
                ansible-lint
                (packer.overrideAttrs (oldAttrs: {
                  version = "1.7.8";
                }))

                basePackages.start-server
                basePackages.start-client
                basePackages.start-replica
                basePackages.migrate-tool
                basePackages.sync-exts-versions
                basePackages.build-test-ami
                basePackages.run-testinfra
                basePackages.cleanup-ami
                dbmate
                nushell
                pythonEnv
                ] ++ pkgs.lib.optionals (nixFastBuild != null) [
                nixFastBuild
                ];
              shellHook = ''
                export HISTFILE=.history
              '';
            };
            cargo-pgrx_0_11_3 = mkCargoPgrxDevShell {
              pgrxVersion = "0_11_3";
              rustVersion = "1.80.0";
            };
            cargo-pgrx_0_12_6 = mkCargoPgrxDevShell {
              pgrxVersion = "0_12_6";
              rustVersion = "1.80.0";
            };
          };
      }
    );
}

'''

